{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3024\n"
     ]
    }
   ],
   "source": [
    "csv_fp = \"/maps/rt582/coralshift/data/ground_truth/reef_check/reefcheck_data.csv\"\n",
    "data = pd.read_csv(csv_fp)\n",
    "N = data.shape[0]\n",
    "\n",
    "# convert days_since_19811231 to datetime\n",
    "data['date'] = pd.to_datetime('19811231', format='%Y%m%d') + pd.to_timedelta(data['days_since_19811231'], unit='D')\n",
    "data = data.sort_values('date', ascending=False)\n",
    "data['coral_cover_Beta'] = (data['Average_coral_cover'] * (N - 1) + 0.5) / N\n",
    "\n",
    "\n",
    "\n",
    "X_cols = [\"lat\", \"Depth\", \"Human_pop\", \"Cyclone\", \"SST_mean\", \"SSTA_Mean\", \"SSTA_min\", \"SSTA_freqstdev\", \"SSTA_dhwmax\", \"TSA_max\", \"TSA_freqstdev\", \"Turbidity_mean\", \"Historical_SST_max\"]\n",
    "# discard any row which contains one or more nan values in any of the rows, X_cols.\n",
    "data = data.dropna(subset=X_cols)\n",
    "# keeping only most recent survey for each reef. This eliminates a bit of dirty data for which the ecoregion as assigned as changing over time (South China Sea!)\n",
    "data = data.drop_duplicates('Reef_ID', keep=\"first\")\n",
    "\n",
    "predictor_df = data[X_cols]\n",
    "\n",
    "# normalise (min-max) # TODO: try standard scaling\n",
    "predictor_df = (predictor_df - predictor_df.min()) / (predictor_df.max() - predictor_df.min())\n",
    "\n",
    "# site and ecoregion info\n",
    "sites_and_region_df = pd.DataFrame({\n",
    "    'Reef_ID': data.Reef_ID.values,\n",
    "    'Ecoregion': data['Ecoregion'].loc[predictor_df.index.values]\n",
    "})\n",
    "\n",
    "\n",
    "# Create dummy data for \"diversity\" dataframe. # TODO: replace with actual diversity data\n",
    "num_ecoregions_in_sample = sites_and_region_df.Ecoregion.nunique()\n",
    "\n",
    "diversity_df = pd.DataFrame({\n",
    "    'Ecoregion': sites_and_region_df.Ecoregion.unique(),\n",
    "    'Ecoregion_no': range(0, len(data.Ecoregion.unique())),\n",
    "    'diversity.standardized': np.random.uniform(0, 1, num_ecoregions_in_sample)\n",
    "})\n",
    "\n",
    "# join sites_and_region_df and diversity_df\n",
    "sites_and_region_df = sites_and_region_df.merge(diversity_df, on='Ecoregion')\n",
    "print(len(predictor_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 16 jobs)\n",
      "NUTS: [beta, num-site, denom-site, num-ecoregion, denom-ecoregion, beta-diversity, g-temp, mu-global, ecoregion, num-theta-site, denom-theta-site, a, Y-new]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='36' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.60% [36/6000 00:03&lt;08:25 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Sampling\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m coral_cover_model:\n\u001b[0;32m--> 118\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# init='advi+adapt_diag', \u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnuts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_accept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Print summary of the trace\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(pm\u001b[38;5;241m.\u001b[39msummary(trace))\n",
      "File \u001b[0;32m~/miniforge3/envs/coralshift/lib/python3.11/site-packages/pymc/sampling/mcmc.py:820\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/coralshift/lib/python3.11/site-packages/pymc/sampling/mcmc.py:851\u001b[0m, in \u001b[0;36m_sample_return\u001b[0;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[0;32m--> 851\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    853\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/coralshift/lib/python3.11/site-packages/pymc/backends/base.py:594\u001b[0m, in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m    592\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    596\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[1;32m    597\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "# number of samples (rows in dataframe)\n",
    "N = predictor_df.shape[0]\n",
    "# number of covariants (environmental variables)\n",
    "K = predictor_df.shape[1]\n",
    "# Nre: number of sites\n",
    "# Nre = N # TODO: fix hierarchical component. Currently need to have number of sites the same as the number of samples...\n",
    "Nre = len(data['Reef_ID'].unique())\n",
    "# re: site indices\n",
    "re = data.reset_index().index.values[:Nre]\n",
    "# Ner: number of ecoregions\n",
    "Ner = len(data['Ecoregion'].unique())\n",
    "\n",
    "\n",
    "# Preparing data for beta model\n",
    "model_Data = {\n",
    "    'Y': data['coral_cover_Beta'].values,\n",
    "    'N': N,\n",
    "    'predictor_df': predictor_df,\n",
    "    'K': K,\n",
    "    're': re,\n",
    "    'Ner': Ner,\n",
    "    'Nre': Nre,\n",
    "    # 'region_for_each_site': data['Ecoregion'],  # len: Nre\n",
    "    'region_for_each_site': sites_and_region_df['Ecoregion_no'],  # len: Nre\n",
    "    'diversity': diversity_df['diversity.standardized'].values  # len: Ner\n",
    "}\n",
    "\n",
    "# PyMC model\n",
    "with pm.Model(coords={\"predictors\": predictor_df.columns.values}) as coral_cover_model:\n",
    "    # 1. SETTING PRIORS\n",
    "    # 1A. Priors for fixed effects\n",
    "    # setting Normal beta prior for each predictor\n",
    "    beta = pm.Normal('beta', mu=0, sigma=0.0001, shape=K, dims=\"predictors\")\n",
    "    \n",
    "    # 1B. Global and ecoregion priors\n",
    "    num_site = pm.Normal('num-site', mu=0, sigma=0.0016)\n",
    "    denom_site = pm.Normal('denom-site', mu=0, sigma=1)\n",
    "    sigma_site = pm.Deterministic('sigma-site', abs(num_site / denom_site))\n",
    "    \n",
    "    # setting precision parameter (sigma) for ecoregion\n",
    "    num_ecoregion = pm.Normal('num-ecoregion', mu=0, sigma=0.0016)\n",
    "    denom_ecoregion = pm.Normal('denom-ecoregion', mu=0, sigma=1)\n",
    "    sigma_ecoregion = pm.Deterministic('sigma-ecoregion', abs(num_ecoregion / denom_ecoregion))\n",
    "    # variance across ecoregion\n",
    "    tau_ecoregion = pm.Deterministic('tau-ecoregion', 1 / (sigma_ecoregion * sigma_ecoregion))\n",
    "\n",
    "    # 'slope' effect in model\n",
    "    beta_diversity = pm.Normal('beta-diversity', mu=0, sigma=0.0001)\n",
    "    \n",
    "    # g represents hierarchical effect for each ecoregion. Prior depends on global mean, a slope \n",
    "    # component 'beta_diversity' and the ecoregion diversity\n",
    "    g_temp = pm.Normal('g-temp', mu=0, sigma=1, shape=Ner)\n",
    "    mu_global = pm.Normal('mu-global', mu=0, sigma=0.0001)\n",
    "    g = pm.Deterministic('g', mu_global + g_temp + beta_diversity*model_Data['diversity'])\n",
    "\n",
    "    ecoregion = pm.Normal('ecoregion', mu=g, sigma=tau_ecoregion, shape=Ner)\n",
    "    # 1C. Priors for sites: half-Cauchi(25) for each\n",
    "    # variance across globe\n",
    "\n",
    "    tau_site = pm.Deterministic('tau-site', 1 / (sigma_site * sigma_site))\n",
    "    num_theta_site = pm.Normal('num-theta-site', mu=0, sigma=0.0016)\n",
    "    denom_theta_site = pm.Normal('denom-theta-site', mu=0, sigma=1)\n",
    "\n",
    "    # theta is latent model variable used in calculation of beta model\n",
    "    theta = pm.Deterministic('theta', abs(num_theta_site / denom_theta_site))\n",
    "    \n",
    "    # setting Normal prior for each site. Tau is variance for each site\n",
    "    # Indexing 'ecoregion' based on the corresponding ecoregion index for each site\n",
    "    mu_a = ecoregion[model_Data['region_for_each_site']]\n",
    "    a = pm.Normal('a', mu=mu_a, sigma=tau_site, shape=Nre)    # and this\n",
    "\n",
    "    # 2. LIKELIHOOD\n",
    "    # beta distribution: latent variable theta, linear predictor pi, and site effect a\n",
    "\n",
    "    eta = pm.math.dot(predictor_df.values, beta) + a[re.astype(int)]  # I think this is the problem\n",
    "    pi = pm.Deterministic('pi', pm.math.invlogit(eta))  # shape 101 for some reason. Expect shape ?\n",
    "\n",
    "\n",
    "    shape1 = theta * pi\n",
    "    shape2 = theta * (1 - pi)\n",
    "\n",
    "    Y = pm.Beta('Y-obs', alpha=shape1, beta=shape2, observed=model_Data['Y'])\n",
    "\n",
    "    ExpY = pi\n",
    "    VarY = pi * (1 - pi) / (theta + 1)\n",
    "\n",
    "    # Pearson residuals between observed and new data\n",
    "    PRes = (Y - ExpY) / pm.math.sqrt(VarY)\n",
    "\n",
    "    # Discrepancy measures (used for checking overdispersion: https://online.stat.psu.edu/stat504/book/export/html/779#:~:text=Overdispersion%20means%20that%20the%20variance,because%20the%20latter%20is%20greater.)\n",
    "    YNew = pm.Beta('Y-new', alpha=shape1, beta=shape2, shape=N)\n",
    "    # Pearson residuals between observed and new data\n",
    "    PRes_new = (YNew - ExpY) / pm.math.sqrt(VarY)\n",
    "    \n",
    "    # square residuals\n",
    "    D = pm.Deterministic('D', np.power(PRes, 2))\n",
    "    D_new = pm.Deterministic('D-new', np.power(PRes_new, 2))\n",
    "    \n",
    "    # # Fit statistics\n",
    "    # # Fit = pm.Deterministic('Fit', np.sum(D))\n",
    "    # # Fit_new = pm.Deterministic('Fit_new', np.sum(D_new))\n",
    "\n",
    "# Initial values to sample for the distributions (should encapsulate the true values)\n",
    "inits = {\n",
    "    'beta': np.random.normal(0, 0.1, K),\n",
    "    'a': np.random.normal(0, 0.1, Nre),\n",
    "    'beta_diversity': np.random.normal(0, 0.1),\n",
    "    'num-site': np.random.normal(0, 25),\n",
    "    'denom-site': np.random.normal(0, 1),\n",
    "    'numtheta-site': np.random.normal(0, 25),\n",
    "    'denom-theta-site': np.random.normal(0, 1),\n",
    "    'num-ecoregion': np.random.normal(0, 25),\n",
    "    'denom-ecoregion': np.random.normal(0, 1)\n",
    "}\n",
    "\n",
    "# Sampling\n",
    "with coral_cover_model:\n",
    "    trace = pm.sample(500, \n",
    "    tune=1000,\n",
    "    # init='advi+adapt_diag', \n",
    "    chains=4,\n",
    "    cores=16,\n",
    "    random_seed=42,\n",
    "    nuts={\"target_accept\": 0.95},\n",
    "    )\n",
    "\n",
    "# Print summary of the trace\n",
    "print(pm.summary(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(coral_cover_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"sigma-site\",\"sigma-ecoregion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"beta\"], combined=True, hdi_prob=0.95, r_hat=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralshift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

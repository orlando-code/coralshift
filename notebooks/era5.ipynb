{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from h3.dataloading import general_df_utils\n",
    "from h3.utils import directories\n",
    "from h3.utils.simple_functions import pad_number_with_zeros\n",
    "import os\n",
    "import re\n",
    "import xarray as xa\n",
    "import geopy\n",
    "import glob\n",
    "from shapely.geometry.point import Point\n",
    "from h3.utils.file_ops import guarantee_existence\n",
    "from h3.dataprocessing import extract_metadata\n",
    "import cdsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_api_dict(\n",
    "    weather_params: list[str],\n",
    "    time_info_dict: dict,\n",
    "    area: list[float],\n",
    "    format: str\n",
    ") -> dict:\n",
    "    \"\"\"Generate api dictionary format for single month of event\"\"\"\n",
    "\n",
    "    api_call_dict = {\n",
    "        \"variable\": weather_params,\n",
    "        \"area\": area,\n",
    "        \"format\": format\n",
    "    } | time_info_dict\n",
    "\n",
    "    return api_call_dict\n",
    "\n",
    "\n",
    "def return_full_weather_param_strings(\n",
    "    dict_keys: list[str]\n",
    "):\n",
    "    \"\"\"Look up weather parameters in a dictionary so they can be entered as short strings rather than typed out in full.\n",
    "    Key:value pairs ordered in expected importance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_keys : list[str]\n",
    "        list of shorthand keys for longhand weather parameters. See accompanying documentation on GitHub\n",
    "    \"\"\"\n",
    "\n",
    "    weather_dict = {\n",
    "        'd2m': '2m_dewpoint_temperature', 't2m': '2m_temperature', 'skt': 'skin_temperature',\n",
    "        'tp': 'total_precipitation',\n",
    "        'sp': 'surface_pressure',\n",
    "        'src': 'skin_reservoir_content', 'swvl1': 'volumetric_soil_water_layer_1',\n",
    "        'swvl2': 'volumetric_soil_water_layer_2', 'swvl3': 'volumetric_soil_water_layer_3',\n",
    "        'swvl4': 'volumetric_soil_water_layer_4',\n",
    "        'slhf': 'surface_latent_heat_flux', 'sshf': 'surface_sensible_heat_flux',\n",
    "        'ssr': 'surface_net_solar_radiation', 'str': 'surface_net_thermal_radiation',\n",
    "        'ssrd': 'surface_solar_radiation_downwards', 'strd': 'surface_thermal_radiation_downwards',\n",
    "        'e': 'total_evaporation', 'pev': 'potential_evaporation',\n",
    "        'ro': 'runoff', 'ssro': 'sub-surface_runoff', 'sro': 'surface_runoff',\n",
    "        'u10': '10m_u_component_of_wind', 'v10': '10m_v_component_of_wind',\n",
    "    }\n",
    "\n",
    "    weather_params = []\n",
    "    for key in dict_keys:\n",
    "        weather_params.append(weather_dict.get(key))\n",
    "\n",
    "    return weather_params\n",
    "\n",
    "\n",
    "def generate_times_from_start_end(\n",
    "    start_end_dates: list[tuple[pd.Timestamp]]\n",
    ") -> dict:\n",
    "    \"\"\"Generate dictionary containing ecmwf time values from list of start and end dates.\n",
    "\n",
    "    TODO: update so can span multiple months accurately (will involve several api calls)\n",
    "    \"\"\"\n",
    "\n",
    "    # padding dates of interest + 1 day on either side to deal with later nans\n",
    "    dates = pd.date_range(start_end_dates[0]-pd.Timedelta(1, 'd'), start_end_dates[1]+pd.Timedelta(1, 'd'))\n",
    "    years, months, days, hours = set(), set(), set(), []\n",
    "    # extract years from time\n",
    "    for date in dates:\n",
    "        years.add(str(date.year))\n",
    "        months.add(pad_number_with_zeros(date.month))\n",
    "        days.add(pad_number_with_zeros(date.day))\n",
    "\n",
    "    for i in range(24):\n",
    "        hours.append(f'{i:02d}:00')\n",
    "\n",
    "    years, months, days = list(years), list(months), list(days)\n",
    "\n",
    "    time_info = {\"year\": years, \"month\": months[0], \"day\": days, \"time\": hours}\n",
    "\n",
    "    return time_info\n",
    "\n",
    "\n",
    "def fetch_era5_data(\n",
    "    weather_params: list[str],\n",
    "    start_end_dates: list[tuple[pd.Timestamp]],\n",
    "    areas: list[tuple[float]],\n",
    "    download_dest_dir: str | Path,\n",
    "    format: str = 'grib'\n",
    ") -> None:\n",
    "    \"\"\"Generate API call, download files, merge xarrays, save as new pkl file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weather_keys : list[str]\n",
    "        list of weather parameter short names to be included in the call\n",
    "    start_end_dates : list[tuple[pd.Timestamp]]\n",
    "        list of start and end date/times for each event\n",
    "    area : list[tuple[float]]\n",
    "        list of max/min lat/lon values in format [north, west, south, east]\n",
    "    download_dest_dir : str | Path\n",
    "        path to download destination\n",
    "    format : str = 'grib'\n",
    "        format of data file to be downloaded\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # initialise client\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    for i, dates in enumerate(start_end_dates):\n",
    "        # create new folder for downloads - TODO: FUNCTION\n",
    "        dir_name = '_'.join((\n",
    "            dates[0].strftime(\"%d-%m-%Y\"), dates[1].strftime(\"%d-%m-%Y\")\n",
    "            ))\n",
    "        dir_path = guarantee_existence(os.path.join(download_dest_dir, dir_name))\n",
    "\n",
    "        time_info_dict = generate_times_from_start_end(dates)\n",
    "\n",
    "        for param in weather_params:\n",
    "            # generate api call info TODO: FUNCTION\n",
    "            api_call_dict = generate_api_dict(param, time_info_dict, areas[i], format)\n",
    "            file_name = f'{param}.{format}'\n",
    "            dest = '/'.join((dir_path, file_name))\n",
    "            # make api call\n",
    "            try:\n",
    "                c.retrieve(\n",
    "                    'reanalysis-era5-land',\n",
    "                    api_call_dict,\n",
    "                    dest\n",
    "                )\n",
    "            # if error in fetching, limit the parameter\n",
    "            except TypeError():\n",
    "                print(f'{param} not found in {dates}. Skipping fetching, moving on.')\n",
    "\n",
    "        # TODO: FUNCTION\n",
    "        # load in all files in folder\n",
    "        file_paths = '/'.join((dir_path, f'*.{format}'))\n",
    "\n",
    "        xa_dict = {}\n",
    "        for file_path in tqdm(glob.glob(file_paths)):\n",
    "            # get name of file\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            # read into xarray\n",
    "            xa_dict[file_name] = xr.load_dataset(file_path, engine=\"cfgrib\")\n",
    "\n",
    "        # merge TODO: apparently conflicting values of 'step'. Unsure why.\n",
    "        out = xr.merge([array for array in xa_dict.values()], compat='override')\n",
    "        # save as new file\n",
    "        nc_file_name = '.'.join((dir_name, 'nc'))\n",
    "        save_file_path = '/'.join((download_dest_dir, nc_file_name))\n",
    "        out.to_netcdf(path=save_file_path)\n",
    "        print(f'{nc_file_name} saved successfully')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

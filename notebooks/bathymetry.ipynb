{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/coralshift/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nc files based on URL and patch together\n",
    "# https://www.ngdc.noaa.gov/thredds/catalog/global/ETOPO2022/15s/15s_geoid_netcdf/catalog.html?dataset=globalDatasetScan/ETOPO2022/15s/15s_geoid_netcdf/ETOPO_2022_v1_15s_N00E000_geoid.nc#:~:text=the%20DAP2%20protcol.-,HTTPServer,-Data%20Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url: str, output_path: Path | str, loading_bar: bool = True) -> None:\n",
    "    print('\\n')\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=Path(output_path), reporthook=t.update_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_last_subparts_path(dir_path: Path | str, n: int) -> Path:\n",
    "    \"\"\"Returns 'n'  last parts of a path. E.g. /first/second/third/fourth with n = 3 will return second/third/fourth\"\"\"\n",
    "    return Path(*Path(dir_path).parts[-n:])\n",
    "\n",
    "\n",
    "def check_path_suffix(path: Path | str, comparison: str) -> bool:\n",
    "    \"\"\"Checks whether path provided ends in a particular suffix e.g. \"nc\". Since usually forget to specify \".\", first\n",
    "    tries to pad \"comparison\" with a period. TODO: docstring\"\"\"\n",
    "\n",
    "    # pad with leading \".\"\n",
    "    if \".\" not in comparison:\n",
    "        comparison = \".\" + comparison\n",
    "\n",
    "    if Path(path).suffix == comparison:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_etopo_data(download_dest_dir: Path | str, resolution: str | int) -> None:\n",
    "    \"\"\"Download data\"\"\"\n",
    "    bathymetry_url_page = f\"https://www.ngdc.noaa.gov/thredds/catalog/global/ETOPO2022/{resolution}s/{resolution}s_geoid_netcdf/catalog.html\"\n",
    "    file_server_url = \"https://www.ngdc.noaa.gov/thredds/fileServer/global/\"\n",
    "\n",
    "    reqs = requests.get(bathymetry_url_page)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    # traverse paragraphs from soup TODO: add progress bar\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        # if ends with nc\n",
    "        if check_path_suffix(link.get(\"href\"), \"nc\"):\n",
    "            file_specifier = get_n_last_subparts_path(Path(link.get(\"href\")), 4)\n",
    "            file_name = get_n_last_subparts_path(Path(link.get(\"href\")), 1)\n",
    "\n",
    "            # download file TODO: put this into a function (including checking if already exists)\n",
    "            r = requests.get(file_server_url + str(file_specifier))\n",
    "            # with open(Path(download_dest_dir, file_name), 'wb') as f:\n",
    "                # copypasted something over and lost this line...\n",
    "\n",
    "\n",
    "def check_path_suffix(path: Path | str, comparison: str) -> bool:\n",
    "    \"\"\"Checks whether path provided ends in a particular suffix e.g. \"nc\". Since usually forget to specify \".\", first\n",
    "    tries to pad \"comparison\" with a period. TODO: docstring\"\"\"\n",
    "\n",
    "    # pad with leading \".\"\n",
    "    if \".\" not in comparison:\n",
    "        comparison = \".\" + comparison\n",
    "\n",
    "    if Path(path).suffix == comparison:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_etopo_data(download_dest_dir: Path | str, resolution: str | int) -> None:\n",
    "    \"\"\"Download data\"\"\"\n",
    "    bathymetry_url_page = f\"https://www.ngdc.noaa.gov/thredds/catalog/global/ETOPO2022/{resolution}s/{resolution}s_geoid_netcdf/catalog.html\"\n",
    "    file_server_url = \"https://www.ngdc.noaa.gov/thredds/fileServer/global/\"\n",
    "\n",
    "    reqs = requests.get(bathymetry_url_page)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    # traverse paragraphs from soup TODO: add progress bar\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        # if ends with nc\n",
    "        if check_path_suffix(link.get(\"href\"), \"nc\"):\n",
    "            file_specifier = get_n_last_subparts_path(Path(link.get(\"href\")), 4)\n",
    "            file_name = get_n_last_subparts_path(Path(link.get(\"href\")), 1)\n",
    "\n",
    "            # download file TODO: put this into a function (including checking if already exists)\n",
    "            r = requests.get(file_server_url + str(file_specifier))\n",
    "            with open(Path(download_dest_dir, file_name), 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            # print(file_specifier)\n",
    "\n",
    "\n",
    "def load_merge_nc_files(nc_dir: Path | str):\n",
    "    files = Path(nc_dir).glob(\"*.nc\")\n",
    "    # combine nc files by coordinates\n",
    "    merged_ncs = xa.open_mfdataset(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is VERY hacky: but couldn't figure out how to scrape links from buttons: there seems to be no hmtl which\n",
    "# differentiates one button from another...\n",
    "\n",
    "def check_exists_download_url(filepath: Path | str, url : str, loading_bar: bool = True) -> None:\n",
    "\t\"\"\"Download file from URL, with choice of loading bar to show download progress\"\"\"\n",
    "\n",
    "\t# if not downloaded\n",
    "\tif not Path(filepath).is_file():\n",
    "\t\t# download with loading bar\n",
    "\t\tif loading_bar:\n",
    "\t\t\tdownload_url(url, str(filepath))\n",
    "\t\t# download without loading bar for some reason...\n",
    "\t\telse:\n",
    "\t\t\turllib.request.urlretrieve(url, filename=output_path)\n",
    "\t# if already downloaded\n",
    "\telse:\n",
    "\t\tprint(f'Already exists: {filepath}')\n",
    "\t\n",
    "    \n",
    "\n",
    "def download_30m_gbr_bathymetry(download_dest_dir: Path | str, areas: list[str]) -> None:\n",
    "    # start_data_url = \"https://ausseabed-public-warehouse-bathymetry.s3.ap-southeast-2.amazonaws.com/L3/0b9ad3f3-7ade-40a7-ae70-f7c6c0f3ae2e/\"\n",
    "\n",
    "    data_urls = [\"https://ausseabed-public-warehouse-bathymetry.s3.ap-southeast-2.amazonaws.com/L3/0b9ad3f3-7ade-40a7-ae70-f7c6c0f3ae2e/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif\",\n",
    "        \"https://ausseabed-public-warehouse-bathymetry.s3.ap-southeast-2.amazonaws.com/L3/4a6e7365-d7b1-45f9-a576-2be8ff8cd755/Great_Barrier_Reef_B_2020_30m_MSL_cog.tif\",\n",
    "        \"https://ausseabed-public-warehouse-bathymetry.s3.ap-southeast-2.amazonaws.com/L3/3b171f8d-9248-4aeb-8b32-0737babba3c2/Great_Barrier_Reef_C_2020_30m_MSL_cog.tif\",\n",
    "        \"https://ausseabed-public-warehouse-bathymetry.s3.ap-southeast-2.amazonaws.com/L3/7168f130-f903-4f2b-948b-78508aad8020/Great_Barrier_Reef_D_2020_30m_MSL_cog.tif\"\n",
    "        ]\n",
    "\n",
    "    for area_url in list(data_urls):\n",
    "        area_filename = get_n_last_subparts_path(area_url, 1)\n",
    "        # area_filename = f\"Great_Barrier_Reef_{alpha.upper()}_2020_30m_MSL_cog.tif\"\n",
    "        # area_url = '/'.join((start_data_url, area_filename))\n",
    "\n",
    "        filepath = Path(download_dest_dir, area_filename)\n",
    "        # check whether file already downloaded\n",
    "        check_exists_download_url(filepath, area_url)\n",
    "\t\t\n",
    "\t\t# if not filepath.is_file():\n",
    "        #     # if not downloaded, downloaded\n",
    "        #     download_url(area_url, str(filepath))\n",
    "        # else:\n",
    "        #     print(f'{filepath} already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# from rich.progress import Progress, BarColumn, DownloadColumn, TransferSpeedColumn, TimeRemainingColumn\n",
    "\n",
    "\n",
    "# class DownloadProgressBar:\n",
    "#     def __init__(self, unit='B'):\n",
    "#         self.progress = Progress(\n",
    "#             \"{task.description}\",\n",
    "#             BarColumn(),\n",
    "#             DownloadColumn(),\n",
    "#             TransferSpeedColumn(),\n",
    "#             TimeRemainingColumn(),\n",
    "#         )\n",
    "#         self.unit = unit\n",
    "\n",
    "#     def __enter__(self):\n",
    "#         self.task_id = self.progress.add_task(\"\", start=False)\n",
    "#         self.progress.start()\n",
    "#         return self\n",
    "\n",
    "#     def __exit__(self, *exc_info):\n",
    "#         self.progress.stop()\n",
    "\n",
    "#     def update_to(self, b=1, bsize=1, tsize=None):\n",
    "#         if tsize is not None:\n",
    "#             # Convert the total size to the specified unit\n",
    "#             total_size = tsize / self.unit_size\n",
    "#             self.progress.update(self.task_id, total=total_size)\n",
    "#         self.progress.update(self.task_id, advance=b * bsize / self.unit_size)\n",
    "\n",
    "#     @property\n",
    "#     def unit_size(self):\n",
    "#         # Return the size of one unit in bytes\n",
    "#         if self.unit == 'B':\n",
    "#             return 1\n",
    "#         elif self.unit == 'KB':\n",
    "#             return 1024\n",
    "#         elif self.unit == 'MB':\n",
    "#             return 1024 * 1024\n",
    "#         elif self.unit == 'GB':\n",
    "#             return 1024 * 1024 * 1024\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid unit: {self.unit}\")\n",
    "\n",
    "# def download_url(url, output_path, progress_units: str = 'MB'):\n",
    "#     print(\"\\n\")\n",
    "#     with DownloadProgressBar(progress_units) as t:\n",
    "#         urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_30m_gbr_bathymetry(\n",
    "\t'/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/GBR_30m', \n",
    "\tareas = ['a','b','c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tif\n",
    "import rasterio\n",
    "\n",
    "src = rasterio.open('/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/GBR_30m/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif')\n",
    "gbr_a_data = src.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_tifs_to_dict(tif_paths: list[Path] | list[str]) -> dict:\n",
    "\t\"\"\"TODO: docstring\"\"\"\n",
    "\n",
    "\ttifs_dict = {}\n",
    "\tfor tif in tif_paths:\n",
    "\t\tfilename = get_n_last_subparts_path(tif, 1)\n",
    "\t\ttif_src = rasterio.open(tif)\n",
    "\t\ttif_array = tif_src.read(1)\n",
    "\n",
    "\t\ttifs_dict[filename] = tif_array\n",
    "\n",
    "\treturn tifs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(gbr_a_data[0:10000, 0:10000])\n",
    "plt.imshow(gbr_a_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = np.isclose(data_array[0], 0, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "# Perform a morphological dilation operation\n",
    "buffer_size = 10  # Define the buffer size\n",
    "struct_elem = np.ones((buffer_size, buffer_size))  # Define the structuring element\n",
    "dilated = binary_dilation(binary, iterations=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.imshow(dilated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.imshow(shoreline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return pixel values closest to the shoreline\n",
    "def return_pixels_closest_to_value(\n",
    "\tarray: np.ndarray, \n",
    "\tcentral_value: float, \n",
    "\ttolerance: float = .5, \n",
    "\tbuffer_pixels: int = 10,\n",
    "\tbathymetry_only: bool = True\n",
    "\t) -> np.ndarray:\n",
    "\t\"\"\"Returns a 1D array of all the pixels in the input array that are closest to a specified central value within a \n",
    "\tgiven tolerance and within a pixel buffer zone.\n",
    "\n",
    "   \tParameters\n",
    "\t----------\n",
    "\tarray (np.ndarray): The input array of pixel values.\n",
    "\tcentral_value (float): The central value to which the pixels should be compared.\n",
    "\ttolerance (float, optional): The tolerance within which the pixels are considered to be \"close\" to the central \n",
    "\t\tvalue. Defaults to 0.5.\n",
    "\tbuffer_pixels (int, optional): The size of the buffer zone around the pixels. Defaults to 10.\n",
    "\tbathymetry_only (bool, optional): Whether to only consider bathymetric data, i.e., values less than zero. \n",
    "\t\tDefaults to True.\n",
    "\n",
    "    Returns\n",
    "\t-------\n",
    "\tnp.ndarray: A 1D array of all the pixels in the input array that are closest to the specified central value within \n",
    "\t\tthe given tolerance and within the pixel buffer zone.\n",
    "    \"\"\"\n",
    "\tbinary = np.isclose(array, central_value, atol=0.5)\n",
    "\t# morphological dilation operation\n",
    "\tdilated = binary_dilation(binary, iterations=buffer_pixels)\n",
    "\n",
    "\tarray_vals = array[dilated]\n",
    "\t# if specifying only bathymetric data\n",
    "\tif bathymetry_only:\n",
    "\t\tarray_vals = array_vals[array_vals < 0]\n",
    "\t\n",
    "\t# return only non-zero values as 1d array\n",
    "\treturn array_vals[np.nonzero(array_vals)]\n",
    "\n",
    "\n",
    "def return_distance_closest_to_value(\n",
    "\tarray: np.ndarray, \n",
    "\tcentral_value: float, \n",
    "\ttolerance: float = .5, \n",
    "\tbuffer_distance: float = 300,\n",
    "\tdistance_per_pixel: float = 30,\n",
    "\tbathymetry_only: bool = True,\n",
    ") -> np.ndarray:\n",
    "\t\"\"\"Wrapper for return_pixels_closest_to_value() allowing specification by distance from thresholded values rather \n",
    "\tthan number of pixels\n",
    "\t\n",
    "\tReturns a 1D array of all the pixels in the input array that are closest to a specified central value within a \n",
    "\tgiven tolerance and within a distance buffer zone.\n",
    "\n",
    "   \tParameters\n",
    "\t----------\n",
    "\tarray (np.ndarray): The input array of pixel values.\n",
    "\tcentral_value (float): The central value to which the pixels should be compared.\n",
    "\ttolerance (float, optional): The tolerance within which the pixels are considered to be \"close\" to the central \n",
    "\t\tvalue. Defaults to 0.5.\n",
    "\tbuffer_distance (float, optional): The size of the buffer zone around the pixels. Defaults to 300.\n",
    "\tbathymetry_only (bool, optional): Whether to only consider bathymetric data, i.e., values less than zero. \n",
    "\t\tDefaults to True.\n",
    "\n",
    "    Returns\n",
    "\t-------\n",
    "\tnp.ndarray: A 1D array of all the pixels in the input array that are closest to the specified central value within \n",
    "\t\tthe given tolerance and within the distance buffer zone.\n",
    "\t\"\"\"\n",
    "\tbuffer_pixels = buffer_distance / distance_per_pixel\n",
    "\treturn return_pixels_closest_to_value(array, central_value, tolerance, buffer_pixels, bathymetry_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = return_pixels_closest_to_value(data_array[0].values, 0, buffer_pixels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "shallow_out = out[out > -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out,100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot histogram of values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xa.plot.hist(data_array, ax=ax, bins=100)\n",
    "ax.set_xlabel(\"depth\")\n",
    "ax.set_ylabel(\"counts\")\n",
    "ax.set_title(\"Histogram of DEM counts for selected area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = xa.open_rasterio(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array[0]\n",
    "# rename coordinate and value fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name_dict = {'y': 'latitude', 'x': 'longitude'}\n",
    "\n",
    "data_array = data_array.rename(new_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array[0, 0:5000, 0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## module import error\n",
    "import gdal\n",
    "# ds = gdal.Open('/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/GBR_30m/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif')\n",
    "# channel = np.array(ds.GetRasterBand(1).ReadAsArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array[0, :1000, :1000].plot(x='longitude', y='latitude', figsize=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_array[0].to_dataframe(name='asdf').reset_index()\n",
    "# gdf = gpd.GeoDataFrame(df.value_column, geometry=gpd.points_from_xy(df.y,df.x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tifs_to_worldmap(tifs_dict: dict) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"TODO: docstring\"\"\"\n",
    "\t\n",
    "\tgdf_list = []\n",
    "\tfor tif_name, tif_array in tifs_dict:\n",
    "\t\tbbox_gdf = align_tifs_to_worldmap(tif_array)\n",
    "\t\tgdf_list.append(bbox_gdf)\n",
    "\n",
    "\tall_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\t\n",
    "\treturn all_gdf\n",
    "\n",
    "\n",
    "def align_tifs_to_worldmap(tif_object) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"TODO: function to line tif files up with world map\"\"\"\n",
    "\n",
    "\t# Create GeoDataFrame with extent of the raster\n",
    "\txmin, ymin, xmax, ymax = tif_object.bounds\n",
    "\tbbox_gdf = gpd.GeoDataFrame({'geometry': gpd.box(xmin, ymin, xmax, ymax)}, index=[0], crs=tif_object.crs)\n",
    "\n",
    "\t# Reproject the GeoDataFrame to Web Mercator\n",
    "\tbbox_gdf = bbox_gdf.to_crs(epsg=3857)\n",
    "\n",
    "\treturn bbox_gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_tifs_to_worldmap(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.bbox_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot – TODO: update with custom bounds\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the raster on the GeoDataFrame extent\n",
    "rasterio.plot.show(bbox_gdf, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to line tif files up with world map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dir = '/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/ETOPO22'\n",
    "name = 'ETOPO_2022_v1_15s_N00E000_geoid.nc'\n",
    "\n",
    "Path(nc_dir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nc_files(nc_dir: Path | str, file_names: list[str]) -> xa.Dataset:\n",
    "\tfiles = [Path(nc_dir, file_name) for file_name in file_names]\n",
    "\tmerged_ncs = xa.open_mfdataset(files)\n",
    "\treturn merged_ncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"ETOPO_2022_v1_15s_N00E000_geoid.nc\", \"ETOPO_2022_v1_15s_N00E015_geoid.nc\", \"ETOPO_2022_v1_15s_N00E030_geoid.nc\"]\n",
    "# bathy_xa = xa.open_dataset('/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/ETOPO22/ETOPO_2022_v1_15s_N00E000_geoid.nc')\n",
    "out = merge_nc_files(nc_dir, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6), dpi=300)\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "# Add a global map background\n",
    "ax.stock_img()\n",
    "\n",
    "out['z'].plot(ax=ax, x='lon', y='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = load_merge_nc_files('/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/ETOPO22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_etopo_data('/Users/orlandotimmerman/Library/CloudStorage/OneDrive-UniversityofCambridge/cambridge/mres/mres_project/coralshift/datasets/bathymetry/ETOPO22', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ngdc.noaa.gov/thredds/fileServer/global/ETOPO2022/15s/15s_geoid_netcdf/ETOPO_2022_v1_15s_N00E000_geoid.nc\"\n",
    "\n",
    "# download file\n",
    "r = requests.get(file_server_url, file_specifier)\n",
    "with open(Path(download_dest_dir, file_name), 'wb') as f:\n",
    "\tf.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathymetry_url_page = \"https://www.ngdc.noaa.gov/thredds/catalog/global/ETOPO2022/15s/15s_geoid_netcdf/catalog.html\"\n",
    "reqs = requests.get(bathymetry_url_page)\n",
    "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "\n",
    "url_start = \"https://www.ngdc.noaa.gov/thredds/catalog/global/ETOPO2022/15s/15s_geoid_netcdf/\"\n",
    "access_str = \"#:~:text=the%20DAP2%20protcol.-,HTTPServer,-Data%20Access\"\n",
    "\n",
    "# traverse paragraphs from soup\n",
    "for link in soup.find_all(\"a\"):\n",
    "    # if ends with nc\n",
    "    if check_path_suffix(link.get(\"href\"), \"nc\"):\n",
    "        file_specifier = get_n_last_subparts_path(Path(link.get(\"href\")), 3)\n",
    "        # download file\n",
    "        print(file_specifier)\n",
    "\n",
    "#https://www.ngdc.noaa.gov/thredds/fileServer/global/ETOPO2022/15s/15s_geoid_netcdf/ETOPO_2022_v1_15s_N00E000_geoid.nc\n",
    "   # https://www.ngdc.noaa.gov/thredds/catalog/global/ETOPO2022/15s/15s_geoid_netcdf/catalog.html?dataset=globalDatasetScan/ETOPO2022/15s/15s_geoid_netcdf/ETOPO_2022_v1_15s_N00E000_geoid.nc#:~:text=the%20DAP2%20protcol.-,HTTPServer,-Data%20Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.geeksforgeeks.org/'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "\tprint(link.get('href'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralshift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f0322cbbfe2118df7ac5a169fc6f9a126c1c0fca1798dd82a93c390122bed8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "location = \"remote\"\n",
    "if location == \"remote\":\n",
    "    # TODO: hacky, shouldn't be necessary\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"lustre_scratch/coralshift/notebooks/gru.ipynb\"\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "# import math as m\n",
    "# import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "# from sklearn.preprocessing import normalize\n",
    "from scipy.interpolate import interp2d\n",
    "from sklearn.utils import class_weight\n",
    "# from scipy.ndimage import gaussian_gradient_magnitude\n",
    "import xbatcher\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "# import rasterio\n",
    "# from rasterio.plot import show\n",
    "# import rioxarray as rio\n",
    "\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories\n",
    "from coralshift.plotting import spatial_plots, model_results\n",
    "from coralshift.dataloading import data_structure, climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n",
      "[PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        print(tf.config.experimental.get_visible_devices('GPU'))\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 25 01:02:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    46W / 163W |  31460MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    47W / 163W |   3077MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu_options.allow_growth = True\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (12800, 10000, 15)\n",
      "Label shape: (12800,)\n",
      "Class weights: [0.55507372 5.03937008]\n"
     ]
    }
   ],
   "source": [
    "cells = 12800\n",
    "seq_len = 10000\n",
    "num_fs = 15\n",
    "target_frac = 0.1\n",
    "\n",
    "# Creating the features array\n",
    "features = np.random.random((cells, seq_len, num_fs)).astype(np.float32)\n",
    "\n",
    "# Creating the label array\n",
    "labels = np.random.choice((2), size=(cells,), p=[1-target_frac,target_frac])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "\n",
    "# Verifying the shape of the arrays\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Label shape:\", labels.shape)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 400\n"
     ]
    }
   ],
   "source": [
    "# send numpy features/label array to tf.Data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 10\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(\"Number of batches:\", len(train_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing toys with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs shape:  (25619, 336, 4)\n",
      "ys shape:  (25619,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: replace with all variables\n",
    "dir = directories.get_datasets_dir() / \"test\"\n",
    "Xs, ys = np.load(dir / \"Xs_lstm.npy\"), np.load(dir / \"ys_lstm.npy\")\n",
    "\n",
    "Xs = np.moveaxis(Xs,1,2)\n",
    "\n",
    "print(\"Xs shape: \", Xs.shape)\n",
    "print(\"ys shape: \", ys.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights:  [0.54231583 6.40795398]\n"
     ]
    }
   ],
   "source": [
    "# X_train = Xs[:1000, :, :20]\n",
    "# y_train = ys[:1000]\n",
    "X_train = Xs\n",
    "y_train = ys\n",
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "print(\"class weights: \", class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 401\n"
     ]
    }
   ],
   "source": [
    "# send numpy features/label array to tf.Data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 10\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(\"Number of batches:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"coralshift\",\n",
    "#     entity=\"orlando-code\",\n",
    "#     settings=wandb.Settings(start_method=\"fork\")\n",
    "#     # config={    }\n",
    "#     )\n",
    "\n",
    "# # initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "# optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "# # X = ds_man.get_dataset(\"monthly_climate_1_12_X_np\")\n",
    "# # y = ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    "# # # check that untrained model runs (should output array of non-nan values)\n",
    "# # # why values change?\n",
    "# # # g_model(X[:32])\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "# #     X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "# #     sub_X, sub_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define Gated Recurrent Unit model class in TensorFlow\n",
    "# class gru_model(tf.keras.Model):\n",
    "#     # initialise class instance to define layers of the model\n",
    "#     def __init__(self, rnn_units: list[int], num_layers: int, \n",
    "#         # dff: int\n",
    "#         ):\n",
    "#         \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "#         layers to a desired output shape\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "#         num_layers (int): number of layers in GRU model\n",
    "#         \"\"\"\n",
    "#         super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "#         # store values for later use\n",
    "#         self.num_layers = num_layers    # number of layers in GRU model\n",
    "#         self.rnn_units = rnn_units\n",
    "#         # self.dff = dff\n",
    "#         # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "#         self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "#                                    return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "#                                    return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "#         # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "#         # self.w1 = tf.keras.layers.Dense(dff) # 10 units\n",
    "#         self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "\n",
    "#         self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "#         # self.A = tf.keras.layers.Dense(30)\n",
    "#         # self.B = tf.keras.layers.Dense(dff)\n",
    "\n",
    "\n",
    "\n",
    "#     def call(self, inputs: np.ndarray, training: bool=False):\n",
    "#         \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "#         fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "#             batch_size - defines the size of the sample drawn from datapoints\n",
    "#             seq_length - number of timesteps in sequence\n",
    "#             features - number of features associated with each datapoint\n",
    "#         training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         target: probability of an event occuring, with shape (batch_size, 1)\n",
    "#         \"\"\"\n",
    "#         # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "#         assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "#         # check that input tensor has correct shape\n",
    "#         if (len(inputs.shape) != 3):\n",
    "#             print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "#         # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "#         # whole_seq, static_input = inputs\n",
    "#         whole_seq = inputs\n",
    "\n",
    "\n",
    "#         # iteratively passes input tensor to GRU layers, overwriting preceding sequence 'whole_seq'\n",
    "#         for layer_num in range(self.num_layers):\n",
    "#             whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "#         # adding extra layers\n",
    "#         # static = self.B(tf.nn.gelu(self.A(static_input)))\n",
    "#         # target = self.w1(final_s)  + static # final hidden state of last layer used as input to fully connected dense layers...\n",
    "#         target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "\n",
    "#         target = tf.nn.relu(target) # via ReLU activation function\n",
    "#         target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "#         # obtain a probability value between 0 and 1\n",
    "#         target = tf.nn.sigmoid(target)\n",
    "        \n",
    "#         return target\n",
    "\n",
    "\n",
    "# # initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "# g_model = gru_model([100], 1) # N.B. [x] is number of hidden layers in GRU network\n",
    "\n",
    "\n",
    "# def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "#     \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "#     incorporating class weights.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "#     y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "#     class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "#     incorporating class weights if provided\n",
    "#     \"\"\"\n",
    "#     bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "#     if class_weights is not None:\n",
    "#         sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "#         # reshape to match size of y and y_pred\n",
    "#         return bce(y, y_pred, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "#     return bce(y, y_pred)\n",
    "\n",
    "\n",
    "# def training_batches(X: np.ndarray, y: np.ndarray, batch_num: int, batch_size: int=32):\n",
    "#     start_idx = batch_num * batch_size\n",
    "#     end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "#     X_batch = X[start_idx:end_idx]\n",
    "#     y_batch = y[start_idx:end_idx]\n",
    "    \n",
    "#     return X_batch, y_batch\n",
    "\n",
    "# # https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "# # should aim to delete the following to speed up training: but can't figure out a way to make wandb reporting work\n",
    "# # without it\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# def build_graph():\n",
    "    \n",
    "#     # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "#     # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "#     @tf.function(experimental_relax_shapes=True)\n",
    "#     def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: tf.Tensor, y: tf.Tensor, \n",
    "#                num_batches: int, batch_num: int, training: bool = True, class_weights=class_weights\n",
    "#                ) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "#         \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "#         negative_log_likelihood)\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "#         y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "#         \"\"\"\n",
    "#         if training:\n",
    "\n",
    "#             num_batches = len(X)\n",
    "#             # num_samples = X.shape[0]\n",
    "#             # num_batches = num_samples // batch_size\n",
    "#             # num_batches = batch_num\n",
    "#             # total_epoch_loss = 0.0\n",
    "#             # for batch_num in tqdm(range(num_batches), desc=\"batches\", position=0, leave=True):\n",
    "#             # for batch_num, batch in tqdm(enumerate(ds), desc=\"batches\", position=0, leave=True):\n",
    "#             total_batch_loss = 0\n",
    "\n",
    "#             y_pred = gru(X, training)\n",
    "#             xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "\n",
    "#                 # X_batch, y_batch = training_batches(X, y, batch_num=batch_num, batch_size=batch_size)\n",
    "\n",
    "#             with tf.GradientTape(persistent=True) as tape:\n",
    "#                 y_pred = gru(X, training) \n",
    "#                 xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "#                 # print(xent)\n",
    "#                 # print(y_pred)\n",
    "            \n",
    "#             gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "#             # print(\"xent\", xent.numpy())\n",
    "#             # print(\"total_epoch_loss\", total_epoch_loss)\n",
    "#             total_batch_loss += xent\n",
    "#                 # learning rate?\n",
    "#             wandb.log({\"batch\": batch_num, \"loss\": xent, \"total_epoch_loss\": total_batch_loss})\n",
    "\n",
    "#             # average_loss = total_batch_loss / num_batches\n",
    "#             # return predicted output values and total loss value\n",
    "#             return y_pred, xent, total_batch_loss\n",
    "\n",
    "#     # set default float type\n",
    "#     tf.keras.backend.set_floatx('float32')\n",
    "#     return train_step\n",
    "\n",
    "\n",
    "# with tf.device(\"/GPU:1\"):\n",
    "#     num_epochs = 20\n",
    "#     # will update so that subsamples are fed in from which batches are taken: will require recomputation\n",
    "#     # of class_weight for each subsample\n",
    "\n",
    "#     tr_step = build_graph()\n",
    "#     num_batches = len(train_dataset)\n",
    "\n",
    "#     for epoch in tqdm(range(num_epochs), desc= \" epochs\", position=1, leave=True):\n",
    "#         ongoing_epoch_loss = 0.0\n",
    "#         for i, (X_batch, y_batch) in tqdm(enumerate(train_dataset), position=0, total=len(train_dataset), desc=\" training on batches\", leave=True):\n",
    "#             y_pred, xent, batch_loss = tr_step(\n",
    "#                 g_model, optimizer, X_batch, y_batch, class_weights=class_weights, \n",
    "#                 num_batches = num_batches, batch_num=i,\n",
    "#                 training=True)\n",
    "\n",
    "#             ongoing_epoch_loss += batch_loss\n",
    "#             # print(ongoing_epoch_loss)\n",
    "#             wandb.log({\"ongoing_epoch_loss\": ongoing_epoch_loss})\n",
    "#         average_epoch_loss = ongoing_epoch_loss / len(train_dataset)\n",
    "#         # print(average_epoch_loss)\n",
    "#         wandb.log({\"epoch\": epoch, \"tota_epoch_loss\": average_epoch_loss})\n",
    "\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout=0,\n",
    "        mlp_dropout=0,\n",
    "        n_classes=2,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.mlp_units = mlp_units\n",
    "        self.dropout = dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def transformer_encoder(self, inputs):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(\n",
    "            key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout\n",
    "        )(x, x)\n",
    "        x = layers.Dropout(self.dropout)(x)\n",
    "        res = x + inputs\n",
    "\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(self.dropout)(x)\n",
    "        x = layers.Conv1D(filters=self.model_input_shape[-1], kernel_size=1)(x)\n",
    "        return x + res\n",
    "\n",
    "    def build_model(self):\n",
    "        inputs = keras.Input(shape=self.model_input_shape)\n",
    "        x = inputs\n",
    "\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_encoder(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in self.mlp_units:\n",
    "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(self.mlp_dropout)(x)\n",
    "        outputs = layers.Dense(self.n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        self.model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# configure model\n",
    "LEARNING_RATE = 1e-3    # TODO: adjustable (not particularly preessing though)\n",
    "# INPUT_SHAPE = (None, samples, seq_len, num_features)\n",
    "INPUT_SHAPE =  Xs.shape[1:]\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "EPOCHS = 4\n",
    "HEAD_SIZE = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 4\n",
    "NUM_TRANSFORMER_BLOCKS = 4\n",
    "MLP_UNITS = 128\n",
    "MLP_DROPOUT = 0.4\n",
    "DROPOUT = 0.25\n",
    "N_CLASSES = 2\n",
    "\n",
    "# Create an instance of the TransformerModel\n",
    "transformer_model = TransformerModel(\n",
    "    model_input_shape=INPUT_SHAPE,\n",
    "    head_size=HEAD_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    mlp_units=[MLP_UNITS],\n",
    "    dropout=DROPOUT,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    "    n_classes=N_CLASSES,\n",
    ")\n",
    "\n",
    "# Access the underlying model\n",
    "transformer_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "transformer_model.model.fit(\n",
    "    Xs[:1000],\n",
    "    ys[:1000],\n",
    "    validation_split=0.2,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# DEFINE TRANSFORMER MODEL\n",
    "####################################\n",
    "\n",
    "\n",
    "class CoralTransformer(tf.keras.Model):\n",
    "    \"\"\"Transformer-based model for binary classification of (spatio)temporal data\"\"\"\n",
    "\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, inputs, head_size, num_heads, ff_dim, dropout=0, epsilon=1e-6, activation=\"Relu\", kernel_size=1):\n",
    "\n",
    "        # initialise CoralTransformer model as subclass of tf.keras.Model\n",
    "        super(CoralTransformer, self).__init__()\n",
    "\n",
    "        # Normalization and Attention layers\n",
    "        self.layer_norm = layers.LayerNormalization(\n",
    "            epsilon=epsilon\n",
    "            )\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "            )\n",
    "        self.dropout = layers.Dropout(\n",
    "            dropout\n",
    "            )\n",
    "\n",
    "        # Feed Forward layers\n",
    "        self.conv1D_0 = layers.Conv1D(\n",
    "            filters=ff_dim, kernel_size=kernel_size, activation=activation\n",
    "            )\n",
    "        self.conv1D_1 = layers.Conv1D(\n",
    "            filters=inputs.shape[-1], kernel_size=kernel_size\n",
    "            )\n",
    "\n",
    "\n",
    "    def transformer_encoder(self, inputs, training=False):\n",
    "        # Normalizationi and Attention\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        result = x + inputs\n",
    "\n",
    "        # Feed Forward\n",
    "        x = self.layer_norm(result)\n",
    "        x = self.conv1D_0(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv1D_1(x)\n",
    "        return x + result\n",
    "\n",
    "        # x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        # x = layers.MultiHeadAttention(\n",
    "        #     key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        # )(x, x)\n",
    "        # x = layers.Dropout(dropout)(x)\n",
    "        # res = x + inputs\n",
    "\n",
    "        # # Feed Forward Part\n",
    "        # x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        # x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        # x = layers.Dropout(dropout)(x)\n",
    "        # x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "        # return x + res\n",
    "\n",
    "    def build_model(\n",
    "        input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout,\n",
    "        mlp_dropout,\n",
    "    ):\n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in mlp_units:\n",
    "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(mlp_dropout)(x)\n",
    "        outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "        return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "###########################\n",
    "# INITIALISE PARAMETERS\n",
    "###########################\n",
    "\n",
    "\n",
    "# configure model\n",
    "LEARNING_RATE = 1e-3    # TODO: adjustable (not particularly preessing though)\n",
    "# INPUT_SHAPE = (None, samples, seq_len, num_features)\n",
    "INPUT_SHAPE =  Xs.shape[1:]\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "EPOCHS = 4\n",
    "HEAD_SIZE = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 4\n",
    "NUM_TRANSFORMER_BLOCKS = 4\n",
    "MLP_UNITS = 128\n",
    "MLP_DROPOUT = 0.4\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model(build_model)\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    activation=ACTIVATION,\n",
    "    head_size=HEAD_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    mlp_units=[MLP_UNITS],\n",
    "    dropout=DROPOUT,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout=0,\n",
    "        mlp_dropout=0,\n",
    "        n_classes=2,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.mlp_units = mlp_units\n",
    "        self.dropout = dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def transformer_encoder(self, inputs, training=False):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(\n",
    "            key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout\n",
    "        )(x, x)\n",
    "        x = layers.Dropout(self.dropout)(x)\n",
    "        res = x + inputs\n",
    "\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(self.dropout)(x)\n",
    "        x = layers.Conv1D(filters=self.model_input_shape[-1], kernel_size=1)(x)\n",
    "        return x + res\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inputs = keras.Input(shape=self.model_input_shape)\n",
    "        x = inputs\n",
    "\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_encoder(x, training=training)\n",
    "\n",
    "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in self.mlp_units:\n",
    "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(self.mlp_dropout)(x, training=training)\n",
    "        outputs = layers.Dense(self.n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        self.model = keras.Model(inputs, outputs)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout=0,\n",
    "        mlp_dropout=0,\n",
    "        n_classes=2,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.mlp_units = mlp_units\n",
    "        self.dropout = dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.transformer_layers = []\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            self.transformer_layers.append(self.transformer_encoder())\n",
    "\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D(\n",
    "            data_format=\"channels_first\"\n",
    "        )\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for units in self.mlp_units:\n",
    "            self.dense_layers.append(layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "        self.dropout_layers = []\n",
    "        for _ in range(len(self.mlp_units)):\n",
    "            self.dropout_layers.append(layers.Dropout(self.mlp_dropout))\n",
    "\n",
    "        self.output_layer = layers.Dense(self.n_classes, activation=\"softmax\")\n",
    "\n",
    "    def transformer_encoder(self):\n",
    "        def encoder(inputs, training=False):\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            x = layers.MultiHeadAttention(\n",
    "                key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout\n",
    "            )(x, x, training=training)\n",
    "            x = layers.Dropout(self.dropout)(x, training=training)\n",
    "            res = x + inputs\n",
    "\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "            x = layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(self.dropout)(x, training=training)\n",
    "            x = layers.Conv1D(filters=self.model_input_shape[-1], kernel_size=1)(x)\n",
    "            return x + res\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "\n",
    "        x = inputs\n",
    "        for i in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_layers[i](x, training=training)\n",
    "\n",
    "        x = self.global_average_pooling(x)\n",
    "\n",
    "        for i in range(len(self.dense_layers)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "\n",
    "        outputs = self.output_layer(x)\n",
    "        # self.model = keras.Model(inputs, outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"transformer_model_25\" (type TransformerModel).\n\nin user code:\n\n    File \"/tmp/ipykernel_28444/452054373.py\", line 81, in call  *\n        self.model = keras.Model(inputs, outputs)\n    File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/engine/functional.py\", line 158, in __init__\n        [\n    File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/engine/functional.py\", line 159, in <listcomp>\n        functional_utils.is_input_keras_tensor(t)\n    File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/engine/functional_utils.py\", line 48, in is_input_keras_tensor\n        raise ValueError(_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG.format(tensor))\n\n    ValueError: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: Tensor(\"Placeholder:0\", shape=(None, 336, 4), dtype=float32)\n\n\nCall arguments received by layer \"transformer_model_25\" (type TransformerModel):\n  • inputs=tf.Tensor(shape=(None, 336, 4), dtype=float32)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 200\u001b[0m\n\u001b[1;32m    196\u001b[0m     model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs)\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m--> 200\u001b[0m model \u001b[39m=\u001b[39m create_model()\n\u001b[1;32m    203\u001b[0m \u001b[39m# for epoch in tqdm(range(EPOCHS), desc=\"epochs\", position=0):\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m#     # Just total loss\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m#     epoch_loss = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     distributed_test_step(conv_lstm, features, labels)\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m#     wandb.log({\"test accuracy\": test_accuracy.result()})\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[67], line 194\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model\u001b[39m():\n\u001b[1;32m    193\u001b[0m     inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39mINPUT_SHAPE)\n\u001b[0;32m--> 194\u001b[0m     transformer_output \u001b[39m=\u001b[39m transformer_model(inputs)  \u001b[39m# Wrap transformer_model with InputLayer\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(transformer_output)\n\u001b[1;32m    196\u001b[0m     model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs)\n",
      "File \u001b[0;32m~/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filex7dxjt69.py:42\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     40\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mrange\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlen\u001b[39m), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdense_layers,), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope), \u001b[39mNone\u001b[39;00m, loop_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m,), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     41\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39moutput_layer, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 42\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(keras)\u001b[39m.\u001b[39;49mModel, (ag__\u001b[39m.\u001b[39;49mld(inputs), ag__\u001b[39m.\u001b[39;49mld(outputs)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"transformer_model_25\" (type TransformerModel).\n\nin user code:\n\n    File \"/tmp/ipykernel_28444/452054373.py\", line 81, in call  *\n        self.model = keras.Model(inputs, outputs)\n    File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/engine/functional.py\", line 158, in __init__\n        [\n    File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/engine/functional.py\", line 159, in <listcomp>\n        functional_utils.is_input_keras_tensor(t)\n    File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/keras/engine/functional_utils.py\", line 48, in is_input_keras_tensor\n        raise ValueError(_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG.format(tensor))\n\n    ValueError: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: Tensor(\"Placeholder:0\", shape=(None, 336, 4), dtype=float32)\n\n\nCall arguments received by layer \"transformer_model_25\" (type TransformerModel):\n  • inputs=tf.Tensor(shape=(None, 336, 4), dtype=float32)\n  • training=False"
     ]
    }
   ],
   "source": [
    "# # set up distributed processing\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# GLOBAL_BATCH_SIZE = (BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync)\n",
    "\n",
    "# with mirrored_strategy.scope():\n",
    "# transformer_model = TransformerModel(activation=wandb.config[\"activation\"])\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout=0,\n",
    "        mlp_dropout=0,\n",
    "        n_classes=2,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.mlp_units = mlp_units\n",
    "        self.dropout = dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.transformer_layers = []\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            self.transformer_layers.append(self.transformer_encoder())\n",
    "\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D(\n",
    "            data_format=\"channels_first\"\n",
    "        )\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for units in self.mlp_units:\n",
    "            self.dense_layers.append(layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "        self.dropout_layers = []\n",
    "        for _ in range(len(self.mlp_units)):\n",
    "            self.dropout_layers.append(layers.Dropout(self.mlp_dropout))\n",
    "\n",
    "        self.output_layer = layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def transformer_encoder(self):\n",
    "        def encoder(inputs, training=False):\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            x = layers.MultiHeadAttention(\n",
    "                key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout\n",
    "            )(x, x, training=training)\n",
    "            x = layers.Dropout(self.dropout)(x, training=training)\n",
    "            res = x + inputs\n",
    "\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "            x = layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(self.dropout)(x, training=training)\n",
    "            x = layers.Conv1D(filters=self.model_input_shape[-1], kernel_size=1)(x)\n",
    "            return x + res\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "\n",
    "        x = inputs\n",
    "        for i in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_layers[i](x, training=training)\n",
    "\n",
    "        x = self.global_average_pooling(x)\n",
    "\n",
    "        for i in range(len(self.dense_layers)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "\n",
    "        outputs = self.output_layer(x)\n",
    "        self.model = keras.Model(inputs, outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the TransformerModel\n",
    "transformer_model = TransformerModel(\n",
    "    model_input_shape=INPUT_SHAPE,\n",
    "    head_size=HEAD_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    mlp_units=[MLP_UNITS],\n",
    "    dropout=DROPOUT,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    "    n_classes=N_CLASSES,\n",
    ")\n",
    "DIST_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "def negative_log_likelihood(\n",
    "    labels: np.ndarray, predictions: np.ndarray, class_weights: np.ndarray = None, distributed: bool=False) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if distributed:\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)  \n",
    "\n",
    "    print(\"labels shape:\", tf.shape(labels))\n",
    "    print(\"predictions shape:\", tf.shape(predictions))\n",
    "\n",
    "    # TODO: this may be broken, throwing error associated with numpy/tensor incompatability\n",
    "    if class_weights is not None:\n",
    "        # sample_weights = tf.gather(class_weights, np.asarray(tf.concat(labels.values, axis=0), dtype=np.int32))\n",
    "        sample_weights = tf.gather(class_weights, tf.cast(labels, dtype=tf.int32))\n",
    "        sample_weights = tf.expand_dims(sample_weights, axis=1)  # Add a new dimension\n",
    "\n",
    "        # reshape to match size of y and y_pred\n",
    "        return bce(labels, predictions, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "    return bce(labels, predictions)\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "    labels: tf.Tensor, predictions: tf.Tensor, class_weights: tuple[float] = None, distributed: bool = False):\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        labels (tf.Tensor): True binary labels, where 0 represents the negative class.\n",
    "        predictions (tf.Tensor): Predicted labels as probability values between 0 and 1.\n",
    "        class_weights (np.ndarray): Weights for each class. If None, no class weights will be applied.\n",
    "        distributed (bool): Flag indicating if the computation is distributed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        float: Negative log likelihood loss computed using binary cross-entropy loss between 'labels' and \n",
    "            'predictions', incorporating class weights if provided.\n",
    "    \"\"\"\n",
    "    per_example_loss = negative_log_likelihood(labels, predictions, class_weights, distributed)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)\n",
    "\n",
    "train_accuracy = tf.keras.metrics.BinaryCrossentropy(name=\"train_accuracy\")\n",
    "test_accuracy = tf.keras.metrics.BinaryCrossentropy(name=\"test_accuracy\")\n",
    "# would use BinaryAccuracy if predicting ones/zeros\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor, \n",
    "    optimizer: tf.keras.optimizers.Optimizer = OPTIMIZER, learn_rate: float = LEARNING_RATE, \n",
    "    class_weights: tuple = None, distributed: bool=False):\n",
    "\n",
    "    # features = tf.keras.Input(tensor=features)\n",
    "\n",
    "    with tf.GradientTape() as lstm_tape:\n",
    "        predictions = model(features, training=True)\n",
    "        # Calculate loss\n",
    "        loss = compute_loss(labels, predictions, class_weights, distributed)\n",
    "    \n",
    "    # get trainable variables\n",
    "    trainable_vars = model.trainable_variables\n",
    "    # Calculate gradient          `\n",
    "    lstm_grads = lstm_tape.gradient(loss, trainable_vars)\n",
    "    # And then apply the gradient to change the weights\n",
    "    optimizer.apply_gradients(zip(lstm_grads, trainable_vars))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_step(model: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor):\n",
    "\n",
    "    predictions = model(features, training=False)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "\n",
    "def create_model():\n",
    "    inputs = tf.keras.Input(shape=INPUT_SHAPE)\n",
    "    transformer_output = transformer_model(inputs)  # Wrap transformer_model with InputLayer\n",
    "    outputs = tf.keras.layers.Dense(1, activation='softmax')(transformer_output)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(EPOCHS), desc=\"epochs\", position=0):\n",
    "#     # Just total loss\n",
    "#     epoch_loss = 0\n",
    "#     # Iterate through the train dataset\n",
    "#     for features, labels in tqdm(train_dataset, desc=\" train batches\", position=1):\n",
    "#         loss = train_step(transformer_model, features, labels, DIST_OPTIMIZER, LEARNING_RATE,\n",
    "#             class_weights\n",
    "#             )\n",
    "#         epoch_loss += loss\n",
    "#         print(epoch_loss)\n",
    "#         print(train_accuracy.result())\n",
    "        # wandb.log({\"epoch loss\": epoch_loss, \"train accuracy\": train_accuracy.result()})\n",
    "\n",
    "    # # Iterate through the test dataset\n",
    "    # for features, labels in tqdm(dist_train_dataset, desc=\" test batches\", position=1):\n",
    "    #     distributed_test_step(conv_lstm, features, labels)\n",
    "    #     wandb.log({\"test accuracy\": test_accuracy.result()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerModel' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer_model\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39msummary()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerModel' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "transformer_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.metrics.metrics.BinaryCrossentropy at 0x7fa0af97f250>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# INITIALISE PARAMETERS\n",
    "###########################\n",
    "\n",
    "\n",
    "# configure model\n",
    "LEARNING_RATE = 1e-3    # TODO: adjustable (not particularly preessing though)\n",
    "BATCH_SIZE = 32\n",
    "# INPUT_SHAPE = (None, samples, seq_len, num_features)\n",
    "MODEL_INPUT_SHAPE = Xs.shape[1:]\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "EPOCHS = 4\n",
    "HEAD_SIZE = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 4\n",
    "NUM_TRANSFORMER_BLOCKS = 4\n",
    "MLP_UNITS = 128\n",
    "MLP_DROPOUT = 0.4\n",
    "DROPOUT = 0.25\n",
    "\n",
    "# Create an instance of the TransformerModel\n",
    "transformer_model = TransformerModel(\n",
    "    model_input_shape=MODEL_INPUT_SHAPE,\n",
    "    head_size=HEAD_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    mlp_units=[MLP_UNITS],\n",
    "    dropout=DROPOUT,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    "    n_classes=2,\n",
    ")\n",
    "\n",
    "# transformer_model = transformer_model(Xs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure model\n",
    "LEARNING_RATE = 1e-3    # TODO: adjustable (not particularly preessing though)\n",
    "# INPUT_SHAPE = (None, samples, seq_len, num_features)\n",
    "INPUT_SHAPE =  Xs.shape[1:]\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "EPOCHS = 4\n",
    "HEAD_SIZE = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 4\n",
    "NUM_TRANSFORMER_BLOCKS = 4\n",
    "MLP_UNITS = 128\n",
    "MLP_DROPOUT = 0.4\n",
    "DROPOUT = 0.25\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_size = int(0.8 * len(Xs[:1000]))\n",
    "train_X, val_X = Xs[:train_size], Xs[train_size:]\n",
    "train_y, val_y = ys[:train_size], ys[train_size:]\n",
    "\n",
    "# Create a dataset from the training data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerModel.transformer_encoder() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create an instance of the TransformerModel\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m transformer_model \u001b[39m=\u001b[39m TransformerModel(\n\u001b[1;32m      5\u001b[0m     model_input_shape\u001b[39m=\u001b[39;49mINPUT_SHAPE,\n\u001b[1;32m      6\u001b[0m     head_size\u001b[39m=\u001b[39;49mHEAD_SIZE,\n\u001b[1;32m      7\u001b[0m     num_heads\u001b[39m=\u001b[39;49mNUM_HEADS,\n\u001b[1;32m      8\u001b[0m     ff_dim\u001b[39m=\u001b[39;49mFF_DIM,\n\u001b[1;32m      9\u001b[0m     num_transformer_blocks\u001b[39m=\u001b[39;49mNUM_TRANSFORMER_BLOCKS,\n\u001b[1;32m     10\u001b[0m     mlp_units\u001b[39m=\u001b[39;49m[MLP_UNITS],\n\u001b[1;32m     11\u001b[0m     dropout\u001b[39m=\u001b[39;49mDROPOUT,\n\u001b[1;32m     12\u001b[0m     mlp_dropout\u001b[39m=\u001b[39;49mMLP_DROPOUT,\n\u001b[1;32m     13\u001b[0m     n_classes\u001b[39m=\u001b[39;49mN_CLASSES,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m transformer_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     17\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m     19\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_accuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(model: tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel, optimizer: tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mOptimizer, \n\u001b[1;32m     23\u001b[0m         X: np\u001b[39m.\u001b[39mndarray, y: np\u001b[39m.\u001b[39mndarray, \n\u001b[1;32m     24\u001b[0m         training: \u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m     25\u001b[0m         \u001b[39m# class_weights=class_weights, \u001b[39;00m\n\u001b[1;32m     26\u001b[0m         batch_num:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch_size: \u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[0;34m(self, model_input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout, n_classes)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers \u001b[39m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_transformer_blocks):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder())\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_average_pooling \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mGlobalAveragePooling1D(\n\u001b[1;32m     31\u001b[0m     data_format\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_layers \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerModel.transformer_encoder() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Create an instance of the TransformerModel\n",
    "transformer_model = TransformerModel(\n",
    "    model_input_shape=INPUT_SHAPE,\n",
    "    head_size=HEAD_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    mlp_units=[MLP_UNITS],\n",
    "    dropout=DROPOUT,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    "    n_classes=N_CLASSES,\n",
    ")\n",
    "\n",
    "transformer_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "def train_step(model: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, \n",
    "        X: np.ndarray, y: np.ndarray, \n",
    "        training: bool=True, \n",
    "        # class_weights=class_weights, \n",
    "        batch_num:int=None, batch_size: int=None):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = transformer_model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, transformer_model.trainable_weights)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(grads, transformer_model.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss_metric.update_state(loss_value)\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Print training progress every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{len(train_X)//batch_size}, Loss: {train_loss_metric.result()}, Accuracy: {train_acc_metric.result()}\")\n",
    "\n",
    "\n",
    "        # inputs = keras.Input(shape=self.model_input_shape)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define metrics\n",
    "train_loss_metric = tf.keras.metrics.Mean()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_loss_metric = tf.keras.metrics.Mean()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset metrics at the start of each epoch\n",
    "    train_loss_metric.reset_states()\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Iterate over batches of the training dataset\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "       \n",
    "        train_step(transformer_model, optimizer, \n",
    "            x_batch_train, y_batch_train, batch_num = step, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # Perform validation at the end of each epoch\n",
    "    val_logits = transformer_model(val_X, training=False)\n",
    "    val_loss_value = loss_fn(val_y, val_logits)\n",
    "\n",
    "    # Update validation metrics\n",
    "    val_loss_metric.update_state(val_loss_value)\n",
    "    val_acc_metric.update_state(val_y, val_logits)\n",
    "\n",
    "    # Print validation results\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss_metric.result()}, Validation Accuracy: {val_acc_metric.result()}\")\n",
    "\n",
    "    # Reset validation metrics for the next epoch\n",
    "    val_loss_metric.reset_states()\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    Xs,\n",
    "    ys,\n",
    "    validation_split=0.2,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.reshape(model.predict(Xs), (187,137,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Xs.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "# model.fit(\n",
    "#     Xs,\n",
    "#     ys,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=2,\n",
    "#     batch_size=64,\n",
    "#     callbacks=callbacks,\n",
    "# )\n",
    "\n",
    "model.evaluate(Xs, ys, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=[14,7])\n",
    "\n",
    "im = ax[0].imshow(prediction[:,:,1])\n",
    "im = ax[1].imshow(ys.reshape(187,137))\n",
    "\n",
    "f.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"coralshift\",\n",
    "    entity=\"orlando-code\",\n",
    "    settings=wandb.Settings(start_method=\"fork\")\n",
    "    # config={    }\n",
    "    )\n",
    "\n",
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "# X = ds_man.get_dataset(\"monthly_climate_1_12_X_np\")\n",
    "# y = ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    "# # check that untrained model runs (should output array of non-nan values)\n",
    "# # why values change?\n",
    "# # g_model(X[:32])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     sub_X, sub_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class transformer_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    # def __init__(self, rnn_units: list[int], num_layers: int):\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        # whole_seq, static_input = inputs\n",
    "        whole_seq = inputs\n",
    "\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwriting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        # static = self.B(tf.nn.gelu(self.A(static_input)))\n",
    "        # target = self.w1(final_s)  + static # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "\n",
    "# initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([100], 1) # N.B. [x] is number of hidden layers in GRU network\n",
    "\n",
    "\n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if class_weights is not None:\n",
    "        sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "        # reshape to match size of y and y_pred\n",
    "        return bce(y, y_pred, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def training_batches(X: np.ndarray, y: np.ndarray, batch_num: int, batch_size: int=32):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "    X_batch = X[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "# https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "# should aim to delete the following to speed up training: but can't figure out a way to make wandb reporting work\n",
    "# without it\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: tf.Tensor, y: tf.Tensor, \n",
    "               num_batches: int, batch_num: int, training: bool = True, class_weights=class_weights\n",
    "               ) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        if training:\n",
    "\n",
    "            num_batches = len(X)\n",
    "            # num_samples = X.shape[0]\n",
    "            # num_batches = num_samples // batch_size\n",
    "            # num_batches = batch_num\n",
    "            # total_epoch_loss = 0.0\n",
    "            # for batch_num in tqdm(range(num_batches), desc=\"batches\", position=0, leave=True):\n",
    "            # for batch_num, batch in tqdm(enumerate(ds), desc=\"batches\", position=0, leave=True):\n",
    "            total_batch_loss = 0\n",
    "\n",
    "            y_pred = gru(X, training)\n",
    "            xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "\n",
    "                # X_batch, y_batch = training_batches(X, y, batch_num=batch_num, batch_size=batch_size)\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                y_pred = gru(X, training) \n",
    "                xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "                # print(xent)\n",
    "                # print(y_pred)\n",
    "            \n",
    "            gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "            # print(\"xent\", xent.numpy())\n",
    "            # print(\"total_epoch_loss\", total_epoch_loss)\n",
    "            total_batch_loss += xent\n",
    "                # learning rate?\n",
    "            wandb.log({\"batch\": batch_num, \"loss\": xent, \"total_epoch_loss\": total_batch_loss})\n",
    "\n",
    "            # average_loss = total_batch_loss / num_batches\n",
    "            # return predicted output values and total loss value\n",
    "            return y_pred, xent, total_batch_loss\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    return train_step\n",
    "\n",
    "\n",
    "with tf.device(\"/GPU:1\"):\n",
    "    num_epochs = 20\n",
    "    # will update so that subsamples are fed in from which batches are taken: will require recomputation\n",
    "    # of class_weight for each subsample\n",
    "\n",
    "    tr_step = build_graph()\n",
    "    num_batches = len(train_dataset)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position=1, leave=True):\n",
    "        ongoing_epoch_loss = 0.0\n",
    "        for i, (X_batch, y_batch) in tqdm(enumerate(train_dataset), position=0, total=len(train_dataset), desc=\" training on batches\", leave=True):\n",
    "            y_pred, xent, batch_loss = tr_step(\n",
    "                g_model, optimizer, X_batch, y_batch, class_weights=class_weights, \n",
    "                num_batches = num_batches, batch_num=i,\n",
    "                training=True)\n",
    "\n",
    "            ongoing_epoch_loss += batch_loss\n",
    "            # print(ongoing_epoch_loss)\n",
    "            wandb.log({\"ongoing_epoch_loss\": ongoing_epoch_loss})\n",
    "        average_epoch_loss = ongoing_epoch_loss / len(train_dataset)\n",
    "        # print(average_epoch_loss)\n",
    "        wandb.log({\"epoch\": epoch, \"tota_epoch_loss\": average_epoch_loss})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = g_model(Xs, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape\n",
    "\n",
    "f,a = plt.subplots(figsize=[7,7])\n",
    "im = plt.imshow(pred.numpy().reshape((187,137)))\n",
    "f.colorbar(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "location = \"remote\"\n",
    "if location == \"remote\":\n",
    "    # TODO: hacky, shouldn't be necessary\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"lustre_scratch/coralshift/notebooks/rnn.ipynb\"\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "# import math as m\n",
    "# import pandas as pd\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "# from sklearn.preprocessing import normalize\n",
    "from scipy.interpolate import interp2d\n",
    "from sklearn.utils import class_weight\n",
    "# from scipy.ndimage import gaussian_gradient_magnitude\n",
    "import xbatcher\n",
    "\n",
    "# import rasterio\n",
    "# from rasterio.plot import show\n",
    "# import rioxarray as rio\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories\n",
    "from coralshift.plotting import spatial_plots, model_results\n",
    "from coralshift.dataloading import data_structure, climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (12800, 10000, 15)\n",
      "Label shape: (12800,)\n",
      "Class weights: [0.55483312 5.05928854]\n"
     ]
    }
   ],
   "source": [
    "cells = 12800\n",
    "seq_len = 10000\n",
    "num_fs = 15\n",
    "target_frac = 0.1\n",
    "\n",
    "# Creating the features array\n",
    "features = np.random.random((cells, seq_len, num_fs)).astype(np.float32)\n",
    "\n",
    "# Creating the label array\n",
    "labels = np.random.choice((2), size=(cells,), p=[1-target_frac,target_frac])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "\n",
    "# Verifying the shape of the arrays\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Label shape:\", labels.shape)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 200\n"
     ]
    }
   ],
   "source": [
    "# send numpy features/label array to tf.Data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 10\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(\"Number of batches:\", len(train_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing toys with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " training on batches:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 22:44:52.763747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      " training on batches: 100%|██████████| 200/200 [01:47<00:00,  1.87it/s]\n",
      " training on batches: 100%|██████████| 200/200 [01:39<00:00,  2.01it/s]\n",
      " epochs: 100%|██████████| 2/2 [03:26<00:00, 103.34s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train = features\n",
    "y_train = labels\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"coralshift\",\n",
    "#     entity=\"orlando-code\",\n",
    "#     settings=wandb.Settings(start_method=\"fork\")\n",
    "#     # config={    }\n",
    "#     )\n",
    "\n",
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "# X = ds_man.get_dataset(\"monthly_climate_1_12_X_np\")\n",
    "# y = ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    "# # check that untrained model runs (should output array of non-nan values)\n",
    "# # why values change?\n",
    "# # g_model(X[:32])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     sub_X, sub_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class gru_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, rnn_units: list[int], num_layers: int, \n",
    "        # dff: int\n",
    "        ):\n",
    "        \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "        layers to a desired output shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "        num_layers (int): number of layers in GRU model\n",
    "        \"\"\"\n",
    "        super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "        # store values for later use\n",
    "        self.num_layers = num_layers    # number of layers in GRU model\n",
    "        self.rnn_units = rnn_units\n",
    "        # self.dff = dff\n",
    "        # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "        self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "                                   return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "                                   return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "        # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "        # self.w1 = tf.keras.layers.Dense(dff) # 10 units\n",
    "        self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "\n",
    "        self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "        # self.A = tf.keras.layers.Dense(30)\n",
    "        # self.B = tf.keras.layers.Dense(dff)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        # whole_seq, static_input = inputs\n",
    "        whole_seq = inputs\n",
    "\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwriting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        # static = self.B(tf.nn.gelu(self.A(static_input)))\n",
    "        # target = self.w1(final_s)  + static # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "\n",
    "# initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([100], 1) # N.B. [x] is number of hidden layers in GRU network\n",
    "\n",
    "\n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if class_weights is not None:\n",
    "        sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "        # reshape to match size of y and y_pred\n",
    "        return bce(y, y_pred, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def training_batches(X: np.ndarray, y: np.ndarray, batch_num: int, batch_size: int=32):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "    X_batch = X[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "# https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "# should aim to delete the following to speed up training: but can't figure out a way to make wandb reporting work\n",
    "# without it\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: tf.Tensor, y: tf.Tensor, \n",
    "               training: bool = True, class_weights=class_weights) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # num_samples = X.shape[0]\n",
    "            # num_batches = num_samples // batch_size\n",
    "            # num_batches = batch_num\n",
    "            # total_epoch_loss = 0.0\n",
    "            # for batch_num in tqdm(range(num_batches), desc=\"batches\", position=0, leave=True):\n",
    "            # for batch_num, batch in tqdm(enumerate(ds), desc=\"batches\", position=0, leave=True):\n",
    "            total_batch_loss = 0\n",
    "\n",
    "            y_pred = gru(X, training)\n",
    "            xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "\n",
    "                # X_batch, y_batch = training_batches(X, y, batch_num=batch_num, batch_size=batch_size)\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                y_pred = gru(X, training) \n",
    "                xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "                # y_pred = gru(X, training) # TO DELETE\n",
    "                # xent = negative_log_likelihood(y, y_pred)\n",
    "            \n",
    "            gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "            # print(\"xent\", xent.numpy())\n",
    "            # print(\"total_epoch_loss\", total_epoch_loss)\n",
    "            total_batch_loss += xent\n",
    "                # learning rate?\n",
    "                # wandb.log({\"batch\": batch_num, \"loss\": xent, \"total_epoch_loss\": total_epoch_loss})\n",
    "\n",
    "            average_loss = total_batch_loss / num_batches\n",
    "            # return predicted output values and total loss value\n",
    "            return y_pred, xent, total_batch_loss\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    # TODO: this isn't assigned... What should it return otherwise? OOH yeas it is!\n",
    "    return train_step\n",
    "\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    num_epochs = 2\n",
    "    # will update so that subsamples are fed in from which batches are taken: will require recomputation\n",
    "    # of class_weight for each subsample\n",
    "\n",
    "    tr_step = build_graph()\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position=1, leave=True):\n",
    "        total_epoch_loss = 0.0\n",
    "        for X_batch, y_batch in tqdm(train_dataset, position=0, desc=\" training on batches\", leave=True):\n",
    "            y_pred, xent, batch_loss = tr_step(\n",
    "                g_model, optimizer, X_batch, y_batch, class_weights=class_weights, training=True)\n",
    "\n",
    "            total_epoch_loss += batch_loss\n",
    "\n",
    "        average_loss = total_epoch_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gru_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 multiple                  35100     \n",
      "                                                                 \n",
      " dense_20 (Dense)            multiple                  1010      \n",
      "                                                                 \n",
      " dense_21 (Dense)            multiple                  11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,121\n",
      "Trainable params: 36,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

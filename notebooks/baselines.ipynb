{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# choose whether to work on a remote machine\n",
    "location = \"remote\"\n",
    "if location == \"remote\":\n",
    "    # change this line to the where the GitHub repository is located\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/jovyan/lustre_scratch/conda-envs/coralshift/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import math as m\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn import datasets, ensemble\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# from sklearn.metrics import mean_squared_error, log_loss\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import sklearn.metrics as sklmetrics\n",
    "# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.enums import Resampling\n",
    "import rioxarray as rio\n",
    "import pickle\n",
    "\n",
    "from coralshift.utils import directories, file_ops, utils\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.plotting import spatial_plots, model_results\n",
    "from coralshift.machine_learning import baselines\n",
    "from coralshift.dataloading import bathymetry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline statistics\n",
    "\n",
    "This notebook automates the recreation of the simple parameterisation scheme utilised in previous literature, for example Couce et al. (2012, 2023)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reproducing_metrics_for_regions(\n",
    "    regions_list: list = [\"A\", \"B\", \"C\", \"D\"], target_resolution_d: float = 1 / 27\n",
    ") -> None:\n",
    "    for region in tqdm(\n",
    "        regions_list,\n",
    "        total=len(regions_list),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc=\" Processing regions\",\n",
    "    ):\n",
    "        lat_lims = bathymetry.ReefAreas().get_lat_lon_limits(region)[0]\n",
    "        lon_lims = bathymetry.ReefAreas().get_lat_lon_limits(region)[1]\n",
    "\n",
    "        # create list of xarray dataarrays\n",
    "        reproduction_xa_list = baselines.load_and_process_reproducing_xa_das(region)\n",
    "        # create dictionary of xa arrays, resampled to correct resolution\n",
    "        resampled_xa_das_dict = resample_list_xa_ds_into_dict(\n",
    "            reproduction_xa_list,\n",
    "            target_resolution=target_resolution_d,\n",
    "            unit=\"d\",\n",
    "            lat_lims=lat_lims,\n",
    "            lon_lims=lon_lims,\n",
    "        )\n",
    "        # generate and save reproducing metrics from merged dict\n",
    "        generate_reproducing_metrics(resampled_xa_das_dict, region=region)\n",
    "\n",
    "\n",
    "def resample_list_xa_ds_into_dict(\n",
    "    xa_das: list[xa.DataArray],\n",
    "    target_resolution: float,\n",
    "    unit: str = \"m\",\n",
    "    lat_lims: tuple[float] = (-10, -17),\n",
    "    lon_lims: tuple[float] = (142, 147),\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Resample a list of xarray DataArrays to the target resolution and merge them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        xa_das (list[xa.DataArray]): A list of xarray DataArrays to be resampled and merged.\n",
    "        target_resolution (float): The target resolution for resampling.\n",
    "        unit (str, defaults to \"m\"): The unit of the target resolution.\n",
    "        interp_method: (str, defaults to \"linear\") The interpolation method for resampling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A dictionary containing the resampled xarray DataArrays merged by their names.\n",
    "    \"\"\"\n",
    "    # TODO: will probably need to save to individual files/folders and combine at test/train time\n",
    "    # may need to go to target array here\n",
    "    target_resolution_d = spatial_data.choose_resolution(target_resolution, unit)[1]\n",
    "\n",
    "    target_xa_d = spatial_data.generate_dummy_xa(\n",
    "        target_resolution_d, lat_lims, lon_lims\n",
    "    )\n",
    "\n",
    "    resampled_xa_das_dict = {}\n",
    "    for xa_da in tqdm(xa_das, desc=\"Resampling xarray DataArrays\"):\n",
    "        # xa_resampled = resample_xa_d_to_other(xa_da, dummy_xa, name=xa_da.name)\n",
    "        xa_resampled = spatial_data.resample_xa_d_to_other(xa_da, target_xa_d)\n",
    "        resampled_xa_das_dict[xa_da.name] = xa_resampled\n",
    "\n",
    "    return resampled_xa_das_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your spatial region(s) and your target resolution (data will be resampled to this value)\n",
    "regions_list = [\"A\",\"B\",\"C\",\"D\"]\n",
    "target_resolution_d = spatial_data.choose_resolution(resolution=0.25, unit=\"d\")[1]\n",
    "\n",
    "# generate all necessary metrics to reproduce literature\n",
    "baselines.generate_reproducing_metrics_for_regions(regions_list=regions_list, target_resolution_d=target_resolution_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and process variables for reproducing Couce et al. (2013)\n",
    "\n",
    "# reproduction_xa_list = load_and_process_reproducing_xa_das(\"A\")\n",
    "# resampled_xa_das_dict = spatial_data.resample_list_xa_ds_to_target_resolution_and_merge(reproduction_xa_list, target_resolution=4000, unit=\"m\")\n",
    "# all_4km = generate_reproducing_metrics(resampled_xa_das_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xa.open_dataset(directories.get_comparison_dir() / \"Great_Barrier_Reef_C/0-0367d_arrays/all_0-0367d_comparative.nc\")\n",
    "test[\"gt\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef_areas = bathymetry.ReefAreas()\n",
    "comparison_dir = directories.get_comparison_dir()\n",
    "region_list = [\"A\",\"B\",\"C\",\"D\"]\n",
    "resolution = 0.0368\n",
    "\n",
    "\n",
    "das = []\n",
    "paths = []\n",
    "for i, region in enumerate(region_list):\n",
    "    name = reef_areas.get_short_filename(region)\n",
    "    path = comparison_dir / name / f\"{utils.replace_dot_with_dash(str(resolution))}d_arrays/all_0-0368d_comparative.nc\"\n",
    "    paths.append(path)\n",
    "    da = (xa.open_dataset(path, decode_coords=\"all\"))\n",
    "    da.attrs[\"region\"] = region_list[i]\n",
    "    das.append(da)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = das[0].sel({\"longitude\": slice(142, 147), \"latitude\": slice(-17,-10)})\n",
    "test1 = das[1].sel({\"longitude\": slice(147, 148), \"latitude\": slice(-18,-17)})\n",
    "test2 = das[2].sel({\"longitude\": slice(148, 154), \"latitude\": slice(-24,-18)})\n",
    "test3 = das[3].sel({\"longitude\": slice(154, 156), \"latitude\": slice(-29,-24)})\n",
    "\n",
    "das_test = [test0,test1,test2,test3]\n",
    "merged = xa.merge(das_test)\n",
    "\n",
    "\n",
    "merged[\"gt\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [region[\"short_name\"] for region in bathymetry.ReefAreas().datasets]\n",
    "# X_train, X_test, y_train, y_test, train_coordinates, test_coordinates = spatial_split_train_test(das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune_across_models(\n",
    "    model_types: list[str],\n",
    "    d_resolution: float = 0.03691,\n",
    "    split_type: str = \"pixel\",\n",
    "    test_lats: tuple[float] = None,\n",
    "    test_lons: tuple[float] = None,\n",
    "    test_fraction: float = 0.25,\n",
    "    cv: int = 3,\n",
    "    n_iter: int = 10,\n",
    "):\n",
    "    model_comp_dir = file_ops.guarantee_existence(\n",
    "        directories.get_datasets_dir() / \"model_params/best_models\"\n",
    "    )\n",
    "\n",
    "    all_data = get_comparison_xa_ds(d_resolution=d_resolution)\n",
    "    res_string = utils.replace_dot_with_dash(f\"{d_resolution:.04f}d\")\n",
    "\n",
    "    # define train/test split so it's the same for all models\n",
    "    (X_trains, X_tests, y_trains, y_tests, _, _) = spatial_split_train_test(\n",
    "        all_data,\n",
    "        \"gt\",\n",
    "        split_type=split_type,\n",
    "        test_fraction=test_fraction,\n",
    "    )\n",
    "\n",
    "    for model in tqdm(\n",
    "        model_types, total=len(model_types), desc=\"Fitting each model via random search\"\n",
    "    ):\n",
    "        train_tune(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model_type=model,\n",
    "            resolution=d_resolution,\n",
    "            save_dir=model_comp_dir,\n",
    "            name=f\"{model}_{res_string}_tuned\",\n",
    "            test_fraction=test_fraction,\n",
    "            cv=cv,\n",
    "            n_iter=n_iter,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_resolution=1/27\n",
    "all_data = baselines.get_comparison_xa_ds(d_resolution=d_resolution)\n",
    "all_data = spatial_data.combine_ds_tiles(all_data, d_resolution)\n",
    "\n",
    "flattened_data_dfs = baselines.xa_dss_to_df([all_data], bath_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = generate_test_train_coords_from_dfs(flattened_data_dfs, split_type=\"spatial\", train_test_lat_divide=-18, train_direction=\"S\")\n",
    "\n",
    "print(\"len_train:\", len(train[0]))\n",
    "print(\"len_test:\", len(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing xarray objects: 4it [00:01,  3.25it/s]                       \n",
      " Combining data: 100%|██████████| 3/3 [00:00<00:00, 95.51it/s]\n",
      "Fitting each model via random search:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with a randomized hyperparameter search...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "Fitting each model via random search:  50%|█████     | 1/2 [00:22<00:22, 22.08s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /lustre_scratch/orlando-code/datasets/model_params/best_models/rf_cla_0-2500d_tuned.pickle.\n",
      "rf_cla_0-2500d_tuned metadata saved to /lustre_scratch/orlando-code/datasets/model_params/best_models/rf_cla_0-2500d_tuned_metadata.json\n",
      "Fitting model with a randomized hyperparameter search...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.21731840\n",
      "Epoch 3, change: 0.11800570\n",
      "Epoch 4, change: 0.06352854\n",
      "Epoch 5, change: 0.03911703\n",
      "Epoch 6, change: 0.03428087\n",
      "Epoch 7, change: 0.03235445\n",
      "Epoch 8, change: 0.02541371\n",
      "Epoch 9, change: 0.02332593\n",
      "Epoch 10, change: 0.02118587\n",
      "Epoch 11, change: 0.01956268\n",
      "Epoch 12, change: 0.01847859\n",
      "Epoch 13, change: 0.01724091\n",
      "Epoch 14, change: 0.01662098\n",
      "Epoch 15, change: 0.01614477\n",
      "Epoch 16, change: 0.01594507\n",
      "Epoch 17, change: 0.01577717\n",
      "Epoch 18, change: 0.01490939\n",
      "Epoch 19, change: 0.01491718\n",
      "Epoch 20, change: 0.01503384\n",
      "Epoch 21, change: 0.01395988\n",
      "Epoch 22, change: 0.01388471\n",
      "Epoch 23, change: 0.01347614\n",
      "Epoch 24, change: 0.01351024\n",
      "Epoch 25, change: 0.01281877\n",
      "Epoch 26, change: 0.01268641\n",
      "Epoch 27, change: 0.01296436\n",
      "Epoch 28, change: 0.01238627\n",
      "Epoch 29, change: 0.01237874\n",
      "Epoch 30, change: 0.01226970\n",
      "Epoch 31, change: 0.01187212\n",
      "Epoch 32, change: 0.01179698\n",
      "Epoch 33, change: 0.01164661\n",
      "Epoch 34, change: 0.01122312\n",
      "Epoch 35, change: 0.01118124\n",
      "Epoch 36, change: 0.01049625\n",
      "Epoch 37, change: 0.00949480\n",
      "Epoch 38, change: 0.00886915\n",
      "Epoch 39, change: 0.00880077\n",
      "Epoch 40, change: 0.00872619\n",
      "Epoch 41, change: 0.00839748\n",
      "Epoch 42, change: 0.00792435\n",
      "Epoch 43, change: 0.00796530\n",
      "Epoch 44, change: 0.00775599\n",
      "Epoch 45, change: 0.00753550\n",
      "Epoch 46, change: 0.00725375\n",
      "Epoch 47, change: 0.00702777\n",
      "Epoch 48, change: 0.00708890\n",
      "Epoch 49, change: 0.00707628\n",
      "Epoch 50, change: 0.00691540\n",
      "Epoch 51, change: 0.00698540\n",
      "Epoch 52, change: 0.00702781\n",
      "Epoch 53, change: 0.00693946\n",
      "Epoch 54, change: 0.00685603\n",
      "Epoch 55, change: 0.00689821\n",
      "Epoch 56, change: 0.00686856\n",
      "Epoch 57, change: 0.00670883\n",
      "Epoch 58, change: 0.00677247\n",
      "Epoch 59, change: 0.00680893\n",
      "Epoch 60, change: 0.00551744\n",
      "Epoch 61, change: 0.00534574\n",
      "Epoch 62, change: 0.00524902\n",
      "Epoch 63, change: 0.00503380\n",
      "Epoch 64, change: 0.00506231\n",
      "Epoch 65, change: 0.00484985\n",
      "Epoch 66, change: 0.00488397\n",
      "Epoch 67, change: 0.00459476\n",
      "Epoch 68, change: 0.00460616\n",
      "Epoch 69, change: 0.00457219\n",
      "Epoch 70, change: 0.00430854\n",
      "Epoch 71, change: 0.00426450\n",
      "Epoch 72, change: 0.00420092\n",
      "Epoch 73, change: 0.00403620\n",
      "Epoch 74, change: 0.00394722\n",
      "Epoch 75, change: 0.00385596\n",
      "Epoch 76, change: 0.00373300\n",
      "Epoch 77, change: 0.00363320\n",
      "Epoch 78, change: 0.00364120\n",
      "Epoch 79, change: 0.00347991\n",
      "Epoch 80, change: 0.00353540\n",
      "Epoch 81, change: 0.00342318\n",
      "Epoch 82, change: 0.00336450\n",
      "Epoch 83, change: 0.00332728\n",
      "Epoch 84, change: 0.00332775\n",
      "Epoch 85, change: 0.00335146\n",
      "Epoch 86, change: 0.00329659\n",
      "Epoch 87, change: 0.00331303\n",
      "Epoch 88, change: 0.00334651\n",
      "Epoch 89, change: 0.00343047\n",
      "Epoch 90, change: 0.00358685\n",
      "Epoch 91, change: 0.00361910\n",
      "Epoch 92, change: 0.00361902\n",
      "Epoch 93, change: 0.00361612\n",
      "Epoch 94, change: 0.00359906\n",
      "Epoch 95, change: 0.00361596\n",
      "Epoch 96, change: 0.00362271\n",
      "Epoch 97, change: 0.00359541\n",
      "Epoch 98, change: 0.00359212\n",
      "Epoch 99, change: 0.00358655\n",
      "Epoch 100, change: 0.00361735\n",
      "Epoch 101, change: 0.00357668\n",
      "Epoch 102, change: 0.00359118\n",
      "Epoch 103, change: 0.00358134\n",
      "Epoch 104, change: 0.00356616\n",
      "Epoch 105, change: 0.00359796\n",
      "Epoch 106, change: 0.00352764\n",
      "Epoch 107, change: 0.00356038\n",
      "Epoch 108, change: 0.00355906\n",
      "Epoch 109, change: 0.00353422\n",
      "Epoch 110, change: 0.00355243\n",
      "Epoch 111, change: 0.00352572\n",
      "Epoch 112, change: 0.00352835\n",
      "Epoch 113, change: 0.00352368\n",
      "Epoch 114, change: 0.00352584\n",
      "Epoch 115, change: 0.00348605\n",
      "Epoch 116, change: 0.00348368\n",
      "Epoch 117, change: 0.00349348\n",
      "Epoch 118, change: 0.00347606\n",
      "Epoch 119, change: 0.00347687\n",
      "Epoch 120, change: 0.00346976\n",
      "Epoch 121, change: 0.00345854\n",
      "Epoch 122, change: 0.00345131\n",
      "Epoch 123, change: 0.00342423\n",
      "Epoch 124, change: 0.00347379\n",
      "Epoch 125, change: 0.00340611\n",
      "Epoch 126, change: 0.00343201\n",
      "Epoch 127, change: 0.00342356\n",
      "Epoch 128, change: 0.00340568\n",
      "Epoch 129, change: 0.00339619\n",
      "Epoch 130, change: 0.00339305\n",
      "Epoch 131, change: 0.00338687\n",
      "Epoch 132, change: 0.00337492\n",
      "Epoch 133, change: 0.00338102\n",
      "Epoch 134, change: 0.00335180\n",
      "Epoch 135, change: 0.00335513\n",
      "Epoch 136, change: 0.00336177\n",
      "Epoch 137, change: 0.00332538\n",
      "Epoch 138, change: 0.00333529\n",
      "Epoch 139, change: 0.00332307\n",
      "Epoch 140, change: 0.00332485\n",
      "EpocEpoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20365285\n",
      "Epoch 3, change: 0.09552528\n",
      "Epoch 4, change: 0.07444116\n",
      "Epoch 5, change: 0.05740001\n",
      "Epoch 6, change: 0.05456409\n",
      "Epoch 7, change: 0.05746532\n",
      "Epoch 8, change: 0.04584695\n",
      "Epoch 9, change: 0.04006681\n",
      "Epoch 10, change: 0.03518050\n",
      "Epoch 11, change: 0.03147023\n",
      "Epoch 12, change: 0.03022484\n",
      "Epoch 13, change: 0.02933645\n",
      "Epoch 14, change: 0.02708913\n",
      "Epoch 15, change: 0.02628453\n",
      "Epoch 16, change: 0.02526637\n",
      "Epoch 17, change: 0.02394350\n",
      "Epoch 18, change: 0.02335689\n",
      "Epoch 19, change: 0.02236431\n",
      "Epoch 20, change: 0.02170367\n",
      "Epoch 21, change: 0.02069388\n",
      "Epoch 22, change: 0.02018226\n",
      "Epoch 23, change: 0.01961416\n",
      "Epoch 24, change: 0.01908306\n",
      "Epoch 25, change: 0.01848799\n",
      "Epoch 26, change: 0.01810367\n",
      "Epoch 27, change: 0.01763984\n",
      "Epoch 28, change: 0.01728477\n",
      "Epoch 29, change: 0.01703474\n",
      "Epoch 30, change: 0.01650441\n",
      "Epoch 31, change: 0.01623092\n",
      "Epoch 32, change: 0.01590169\n",
      "Epoch 33, change: 0.01553122\n",
      "Epoch 34, change: 0.01532398\n",
      "Epoch 35, change: 0.01499889\n",
      "Epoch 36, change: 0.01481419\n",
      "Epoch 37, change: 0.01461867\n",
      "Epoch 38, change: 0.01437686\n",
      "Epoch 39, change: 0.01418084\n",
      "Epoch 40, change: 0.01399371\n",
      "Epoch 41, change: 0.01387508\n",
      "Epoch 42, change: 0.01367850\n",
      "Epoch 43, change: 0.01352901\n",
      "Epoch 44, change: 0.01334300\n",
      "Epoch 45, change: 0.01305475\n",
      "Epoch 46, change: 0.01271935\n",
      "Epoch 47, change: 0.01250189\n",
      "Epoch 48, change: 0.01224091\n",
      "Epoch 49, change: 0.01203010\n",
      "Epoch 50, change: 0.01173312\n",
      "Epoch 51, change: 0.01159064\n",
      "Epoch 52, change: 0.01132984\n",
      "Epoch 53, change: 0.01111139\n",
      "Epoch 54, change: 0.01092664\n",
      "Epoch 55, change: 0.01074237\n",
      "Epoch 56, change: 0.01060935\n",
      "Epoch 57, change: 0.01039531\n",
      "Epoch 58, change: 0.01019973\n",
      "Epoch 59, change: 0.01008616\n",
      "Epoch 60, change: 0.00988711\n",
      "Epoch 61, change: 0.00975969\n",
      "Epoch 62, change: 0.00960637\n",
      "Epoch 63, change: 0.00950221\n",
      "Epoch 64, change: 0.00933249\n",
      "Epoch 65, change: 0.00916501\n",
      "Epoch 66, change: 0.00910228\n",
      "Epoch 67, change: 0.00890438\n",
      "Epoch 68, change: 0.00881934\n",
      "Epoch 69, change: 0.00869137\n",
      "Epoch 70, change: 0.00858229\n",
      "Epoch 71, change: 0.00847564\n",
      "Epoch 72, change: 0.00836690\n",
      "Epoch 73, change: 0.00830348\n",
      "Epoch 74, change: 0.00809788\n",
      "Epoch 75, change: 0.00805964\n",
      "Epoch 76, change: 0.00797996\n",
      "Epoch 77, change: 0.00782640\n",
      "Epoch 78, change: 0.00776348\n",
      "Epoch 79, change: 0.00765825\n",
      "Epoch 80, change: 0.00759182\n",
      "Epoch 81, change: 0.00746110\n",
      "Epoch 82, change: 0.00738563\n",
      "Epoch 83, change: 0.00734424\n",
      "Epoch 84, change: 0.00725898\n",
      "Epoch 85, change: 0.00718239\n",
      "Epoch 86, change: 0.00707969\n",
      "Epoch 87, change: 0.00702713\n",
      "Epoch 88, change: 0.00693091\n",
      "Epoch 89, change: 0.00685116\n",
      "Epoch 90, change: 0.00682976\n",
      "Epoch 91, change: 0.00674031\n",
      "Epoch 92, change: 0.00665689\n",
      "Epoch 93, change: 0.00660937\n",
      "Epoch 94, change: 0.00652894\n",
      "Epoch 95, change: 0.00647869\n",
      "Epoch 96, change: 0.00642876\n",
      "Epoch 97, change: 0.00635246\n",
      "Epoch 98, change: 0.00629723\n",
      "Epoch 99, change: 0.00622836\n",
      "Epoch 100, change: 0.00617561\n",
      "Epoch 101, change: 0.00613555\n",
      "Epoch 102, change: 0.00604706\n",
      "Epoch 103, change: 0.00602025\n",
      "Epoch 104, change: 0.00594177\n",
      "Epoch 105, change: 0.00588814\n",
      "Epoch 106, change: 0.00586575\n",
      "Epoch 107, change: 0.00578870\n",
      "Epoch 108, change: 0.00573199\n",
      "Epoch 109, change: 0.00568948\n",
      "Epoch 110, change: 0.00564573\n",
      "Epoch 111, change: 0.00559637\n",
      "Epoch 112, change: 0.00554069\n",
      "Epoch 113, change: 0.00551327\n",
      "Epoch 114, change: 0.00545547\n",
      "Epoch 115, change: 0.00540772\n",
      "Epoch 116, change: 0.00537573\n",
      "Epoch 117, change: 0.00531762\n",
      "Epoch 118, change: 0.00527844\n",
      "Epoch 119, change: 0.00523656\n",
      "Epoch 120, change: 0.00519380\n",
      "Epoch 121, change: 0.00514539\n",
      "Epoch 122, change: 0.00511475\n",
      "Epoch 123, change: 0.00506616\n",
      "Epoch 124, change: 0.00503784\n",
      "Epoch 125, change: 0.00499282\n",
      "Epoch 126, change: 0.00496055\n",
      "Epoch 127, change: 0.00491036\n",
      "Epoch 128, change: 0.00488193\n",
      "Epoch 129, change: 0.00484161\n",
      "Epoch 130, change: 0.00481265\n",
      "Epoch 131, change: 0.00476245\n",
      "Epoch 132, change: 0.00474452\n",
      "Epoch 133, change: 0.00470421\n",
      "Epoch 134, change: 0.00467727\n",
      "Epoch 135, change: 0.00463087\n",
      "Epoch 136, change: 0.00460514\n",
      "Epoch 137, change: 0.00457000\n",
      "Epoch 138, change: 0.00453522\n",
      "Epoch 139, change: 0.00450273\n",
      "Epoch 140, change: 0.00448061\n",
      "EpocEpoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27718786\n",
      "Epoch 3, change: 0.14656590\n",
      "Epoch 4, change: 0.07207597\n",
      "Epoch 5, change: 0.05265621\n",
      "Epoch 6, change: 0.04214151\n",
      "Epoch 7, change: 0.03976963\n",
      "Epoch 8, change: 0.03174468\n",
      "Epoch 9, change: 0.02829627\n",
      "Epoch 10, change: 0.02616986\n",
      "Epoch 11, change: 0.02387893\n",
      "Epoch 12, change: 0.02302345\n",
      "Epoch 13, change: 0.02057629\n",
      "Epoch 14, change: 0.02032532\n",
      "Epoch 15, change: 0.01913500\n",
      "Epoch 16, change: 0.01865255\n",
      "Epoch 17, change: 0.01800386\n",
      "Epoch 18, change: 0.01721420\n",
      "Epoch 19, change: 0.01733551\n",
      "Epoch 20, change: 0.01665957\n",
      "Epoch 21, change: 0.01571471\n",
      "Epoch 22, change: 0.01583214\n",
      "Epoch 23, change: 0.01576730\n",
      "Epoch 24, change: 0.01552569\n",
      "Epoch 25, change: 0.01542985\n",
      "Epoch 26, change: 0.01546466\n",
      "Epoch 27, change: 0.01526793\n",
      "Epoch 28, change: 0.01499952\n",
      "Epoch 29, change: 0.01470841\n",
      "Epoch 30, change: 0.01433558\n",
      "Epoch 31, change: 0.01434289\n",
      "Epoch 32, change: 0.01410470\n",
      "Epoch 33, change: 0.01362819\n",
      "Epoch 34, change: 0.01347771\n",
      "Epoch 35, change: 0.01325856\n",
      "Epoch 36, change: 0.01311978\n",
      "Epoch 37, change: 0.01271821\n",
      "Epoch 38, change: 0.01264519\n",
      "Epoch 39, change: 0.01244432\n",
      "Epoch 40, change: 0.01138560\n",
      "Epoch 41, change: 0.01107257\n",
      "Epoch 42, change: 0.01057502\n",
      "Epoch 43, change: 0.01038813\n",
      "Epoch 44, change: 0.01003583\n",
      "Epoch 45, change: 0.00992628\n",
      "Epoch 46, change: 0.00970102\n",
      "Epoch 47, change: 0.00940788\n",
      "Epoch 48, change: 0.00931304\n",
      "Epoch 49, change: 0.00903521\n",
      "Epoch 50, change: 0.00896588\n",
      "Epoch 51, change: 0.00889402\n",
      "Epoch 52, change: 0.00874640\n",
      "Epoch 53, change: 0.00854939\n",
      "Epoch 54, change: 0.00852114\n",
      "Epoch 55, change: 0.00831279\n",
      "Epoch 56, change: 0.00821412\n",
      "Epoch 57, change: 0.00808297\n",
      "Epoch 58, change: 0.00786808\n",
      "Epoch 59, change: 0.00776463\n",
      "Epoch 60, change: 0.00767909\n",
      "Epoch 61, change: 0.00751398\n",
      "Epoch 62, change: 0.00740456\n",
      "Epoch 63, change: 0.00722931\n",
      "Epoch 64, change: 0.00725061\n",
      "Epoch 65, change: 0.00701775\n",
      "Epoch 66, change: 0.00697373\n",
      "Epoch 67, change: 0.00679385\n",
      "Epoch 68, change: 0.00672648\n",
      "Epoch 69, change: 0.00660924\n",
      "Epoch 70, change: 0.00647856\n",
      "Epoch 71, change: 0.00639706\n",
      "Epoch 72, change: 0.00637745\n",
      "Epoch 73, change: 0.00615727\n",
      "Epoch 74, change: 0.00609355\n",
      "Epoch 75, change: 0.00608960\n",
      "Epoch 76, change: 0.00594403\n",
      "Epoch 77, change: 0.00588097\n",
      "Epoch 78, change: 0.00571145\n",
      "Epoch 79, change: 0.00568758\n",
      "Epoch 80, change: 0.00556508\n",
      "Epoch 81, change: 0.00550212\n",
      "Epoch 82, change: 0.00548381\n",
      "Epoch 83, change: 0.00532598\n",
      "Epoch 84, change: 0.00535474\n",
      "Epoch 85, change: 0.00517531\n",
      "Epoch 86, change: 0.00510754\n",
      "Epoch 87, change: 0.00508053\n",
      "Epoch 88, change: 0.00498252\n",
      "Epoch 89, change: 0.00492941\n",
      "Epoch 90, change: 0.00483719\n",
      "Epoch 91, change: 0.00479797\n",
      "Epoch 92, change: 0.00476790\n",
      "Epoch 93, change: 0.00467833\n",
      "Epoch 94, change: 0.00460796\n",
      "Epoch 95, change: 0.00453978\n",
      "Epoch 96, change: 0.00450394\n",
      "Epoch 97, change: 0.00445606\n",
      "Epoch 98, change: 0.00434092\n",
      "Epoch 99, change: 0.00435248\n",
      "Epoch 100, change: 0.00428887\n",
      "Epoch 101, change: 0.00423007\n",
      "Epoch 102, change: 0.00415683\n",
      "Epoch 103, change: 0.00411636\n",
      "Epoch 104, change: 0.00406694\n",
      "Epoch 105, change: 0.00401842\n",
      "Epoch 106, change: 0.00398289\n",
      "Epoch 107, change: 0.00389093\n",
      "Epoch 108, change: 0.00388134\n",
      "Epoch 109, change: 0.00380664\n",
      "Epoch 110, change: 0.00377970\n",
      "Epoch 111, change: 0.00372217\n",
      "Epoch 112, change: 0.00370453\n",
      "Epoch 113, change: 0.00362710\n",
      "Epoch 114, change: 0.00363201\n",
      "Epoch 115, change: 0.00357513\n",
      "Epoch 116, change: 0.00352839\n",
      "Epoch 117, change: 0.00348032\n",
      "Epoch 118, change: 0.00341043\n",
      "Epoch 119, change: 0.00340455\n",
      "Epoch 120, change: 0.00335271\n",
      "Epoch 121, change: 0.00334547\n",
      "Epoch 122, change: 0.00328216\n",
      "Epoch 123, change: 0.00326011\n",
      "Epoch 124, change: 0.00320532\n",
      "Epoch 125, change: 0.00319610\n",
      "Epoch 126, change: 0.00313396\n",
      "Epoch 127, change: 0.00312489\n",
      "Epoch 128, change: 0.00307928\n",
      "Epoch 129, change: 0.00303695\n",
      "Epoch 130, change: 0.00301608\n",
      "Epoch 131, change: 0.00297239\n",
      "Epoch 132, change: 0.00294390\n",
      "Epoch 133, change: 0.00293670\n",
      "Epoch 134, change: 0.00290361\n",
      "Epoch 135, change: 0.00285502\n",
      "Epoch 136, change: 0.00283330\n",
      "Epoch 137, change: 0.00281488\n",
      "Epoch 138, change: 0.00275076\n",
      "Epoch 139, change: 0.00274847\n",
      "Epoch 140, change: 0.00272764\n",
      "Epoc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "111 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "24 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1207, in fit\n",
      "    multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 90, in _check_multi_class\n",
      "    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n",
      "ValueError: Solver newton-cholesky does not support a multinomial backend.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/jovyan/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.74220033        nan        nan        nan        nan 0.80952381\n",
      "        nan        nan        nan        nan 0.727422          nan\n",
      "        nan 0.73891626        nan        nan        nan        nan\n",
      "        nan        nan 0.8045977         nan 0.70279146 0.7454844\n",
      "        nan        nan        nan 0.74712644        nan        nan\n",
      " 0.71756979        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.80952381 0.80788177\n",
      "        nan        nan        nan 0.72577997        nan        nan\n",
      "        nan 0.71756979]\n",
      "  warnings.warn(\n",
      "Fitting each model via random search: 100%|██████████| 2/2 [00:22<00:00, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20172704\n",
      "Epoch 3, change: 0.08758974\n",
      "Epoch 4, change: 0.07420365\n",
      "Epoch 5, change: 0.05661182\n",
      "Epoch 6, change: 0.05309954\n",
      "Epoch 7, change: 0.05549590\n",
      "Epoch 8, change: 0.04604416\n",
      "Epoch 9, change: 0.04009206\n",
      "Epoch 10, change: 0.03654678\n",
      "Epoch 11, change: 0.03315043\n",
      "Epoch 12, change: 0.03202393\n",
      "Epoch 13, change: 0.03100169\n",
      "Epoch 14, change: 0.02878355\n",
      "Epoch 15, change: 0.02780334\n",
      "Epoch 16, change: 0.02679746\n",
      "Epoch 17, change: 0.02549594\n",
      "Epoch 18, change: 0.02493760\n",
      "Epoch 19, change: 0.02393005\n",
      "Epoch 20, change: 0.02325494\n",
      "Epoch 21, change: 0.02223613\n",
      "Epoch 22, change: 0.02166513\n",
      "Epoch 23, change: 0.02102393\n",
      "Epoch 24, change: 0.02048272\n",
      "Epoch 25, change: 0.01988937\n",
      "Epoch 26, change: 0.01946064\n",
      "Epoch 27, change: 0.01895964\n",
      "Epoch 28, change: 0.01858105\n",
      "Epoch 29, change: 0.01831829\n",
      "Epoch 30, change: 0.01774763\n",
      "Epoch 31, change: 0.01752755\n",
      "Epoch 32, change: 0.01723657\n",
      "Epoch 33, change: 0.01683477\n",
      "Epoch 34, change: 0.01661577\n",
      "Epoch 35, change: 0.01627011\n",
      "Epoch 36, change: 0.01609435\n",
      "Epoch 37, change: 0.01587719\n",
      "Epoch 38, change: 0.01561824\n",
      "Epoch 39, change: 0.01541713\n",
      "Epoch 40, change: 0.01522640\n",
      "Epoch 41, change: 0.01508187\n",
      "Epoch 42, change: 0.01470541\n",
      "Epoch 43, change: 0.01434526\n",
      "Epoch 44, change: 0.01403859\n",
      "Epoch 45, change: 0.01373104\n",
      "Epoch 46, change: 0.01339220\n",
      "Epoch 47, change: 0.01315616\n",
      "Epoch 48, change: 0.01290197\n",
      "Epoch 49, change: 0.01266770\n",
      "Epoch 50, change: 0.01235143\n",
      "Epoch 51, change: 0.01220412\n",
      "Epoch 52, change: 0.01192184\n",
      "Epoch 53, change: 0.01168797\n",
      "Epoch 54, change: 0.01149293\n",
      "Epoch 55, change: 0.01129909\n",
      "Epoch 56, change: 0.01115400\n",
      "Epoch 57, change: 0.01092986\n",
      "Epoch 58, change: 0.01070910\n",
      "Epoch 59, change: 0.01059595\n",
      "Epoch 60, change: 0.01037565\n",
      "Epoch 61, change: 0.01024148\n",
      "Epoch 62, change: 0.01007480\n",
      "Epoch 63, change: 0.00996265\n",
      "Epoch 64, change: 0.00978353\n",
      "Epoch 65, change: 0.00960632\n",
      "Epoch 66, change: 0.00953743\n",
      "Epoch 67, change: 0.00932833\n",
      "Epoch 68, change: 0.00923767\n",
      "Epoch 69, change: 0.00908659\n",
      "Epoch 70, change: 0.00898296\n",
      "Epoch 71, change: 0.00885799\n",
      "Epoch 72, change: 0.00873955\n",
      "Epoch 73, change: 0.00867420\n",
      "Epoch 74, change: 0.00845564\n",
      "Epoch 75, change: 0.00840853\n",
      "Epoch 76, change: 0.00832534\n",
      "Epoch 77, change: 0.00817565\n",
      "Epoch 78, change: 0.00809292\n",
      "Epoch 79, change: 0.00799753\n",
      "Epoch 80, change: 0.00791269\n",
      "Epoch 81, change: 0.00776728\n",
      "Epoch 82, change: 0.00769372\n",
      "Epoch 83, change: 0.00764339\n",
      "Epoch 84, change: 0.00754881\n",
      "Epoch 85, change: 0.00746058\n",
      "Epoch 86, change: 0.00735727\n",
      "Epoch 87, change: 0.00729572\n",
      "Epoch 88, change: 0.00719309\n",
      "Epoch 89, change: 0.00710848\n",
      "Epoch 90, change: 0.00707923\n",
      "Epoch 91, change: 0.00698637\n",
      "Epoch 92, change: 0.00689330\n",
      "Epoch 93, change: 0.00684334\n",
      "Epoch 94, change: 0.00675384\n",
      "Epoch 95, change: 0.00670233\n",
      "Epoch 96, change: 0.00664226\n",
      "Epoch 97, change: 0.00656736\n",
      "Epoch 98, change: 0.00650120\n",
      "Epoch 99, change: 0.00643575\n",
      "Epoch 100, change: 0.00637719\n",
      "Epoch 101, change: 0.00632997\n",
      "Epoch 102, change: 0.00623668\n",
      "Epoch 103, change: 0.00620453\n",
      "Epoch 104, change: 0.00612728\n",
      "Epoch 105, change: 0.00606676\n",
      "Epoch 106, change: 0.00603632\n",
      "Epoch 107, change: 0.00596612\n",
      "Epoch 108, change: 0.00589863\n",
      "Epoch 109, change: 0.00585567\n",
      "Epoch 110, change: 0.00580445\n",
      "Epoch 111, change: 0.00575855\n",
      "Epoch 112, change: 0.00569983\n",
      "Epoch 113, change: 0.00566346\n",
      "Epoch 114, change: 0.00560606\n",
      "Epoch 115, change: 0.00555575\n",
      "Epoch 116, change: 0.00552461\n",
      "Epoch 117, change: 0.00546299\n",
      "Epoch 118, change: 0.00541432\n",
      "Epoch 119, change: 0.00537717\n",
      "Epoch 120, change: 0.00532826\n",
      "Epoch 121, change: 0.00528012\n",
      "Epoch 122, change: 0.00525041\n",
      "Epoch 123, change: 0.00519932\n",
      "Epoch 124, change: 0.00516085\n",
      "Epoch 125, change: 0.00511966\n",
      "Epoch 126, change: 0.00508254\n",
      "Epoch 127, change: 0.00503219\n",
      "Epoch 128, change: 0.00500366\n",
      "Epoch 129, change: 0.00495930\n",
      "Epoch 130, change: 0.00492708\n",
      "Epoch 131, change: 0.00487928\n",
      "Epoch 132, change: 0.00485698\n",
      "Epoch 133, change: 0.00481709\n",
      "Epoch 134, change: 0.00478837\n",
      "Epoch 135, change: 0.00473784\n",
      "Epoch 136, change: 0.00471132\n",
      "Epoch 137, change: 0.00467660\n",
      "Epoch 138, change: 0.00464126\n",
      "Epoch 139, change: 0.00460735\n",
      "Epoch 140, change: 0.00458197\n",
      "EpocEpoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.19450554\n",
      "Epoch 3, change: 0.13261253\n",
      "Epoch 4, change: 0.03520305\n",
      "Epoch 5, change: 0.02907380\n",
      "Epoch 6, change: 0.02094695\n",
      "Epoch 7, change: 0.00986766\n",
      "Epoch 8, change: 0.00604378\n",
      "Epoch 9, change: 0.00141837\n",
      "Epoch 10, change: 0.00108846\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27107536\n",
      "Epoch 3, change: 0.14886784\n",
      "Epoch 4, change: 0.07315406\n",
      "Epoch 5, change: 0.05374740\n",
      "Epoch 6, change: 0.04308148\n",
      "Epoch 7, change: 0.04012807\n",
      "Epoch 8, change: 0.03262398\n",
      "Epoch 9, change: 0.03023212\n",
      "Epoch 10, change: 0.02692720\n",
      "Epoch 11, change: 0.02469203\n",
      "Epoch 12, change: 0.02399847\n",
      "Epoch 13, change: 0.02211510\n",
      "Epoch 14, change: 0.02130961\n",
      "Epoch 15, change: 0.02034436\n",
      "Epoch 16, change: 0.01977494\n",
      "Epoch 17, change: 0.01937096\n",
      "Epoch 18, change: 0.01852320\n",
      "Epoch 19, change: 0.01822844\n",
      "Epoch 20, change: 0.01759447\n",
      "Epoch 21, change: 0.01696158\n",
      "Epoch 22, change: 0.01706071\n",
      "Epoch 23, change: 0.01687939\n",
      "Epoch 24, change: 0.01575379\n",
      "Epoch 25, change: 0.01520769\n",
      "Epoch 26, change: 0.01449188\n",
      "Epoch 27, change: 0.01417682\n",
      "Epoch 28, change: 0.01351814\n",
      "Epoch 29, change: 0.01359750\n",
      "Epoch 30, change: 0.01272068\n",
      "Epoch 31, change: 0.01221383\n",
      "Epoch 32, change: 0.01237667\n",
      "Epoch 33, change: 0.01159874\n",
      "Epoch 34, change: 0.01143440\n",
      "Epoch 35, change: 0.01104312\n",
      "Epoch 36, change: 0.01074248\n",
      "Epoch 37, change: 0.01021056\n",
      "Epoch 38, change: 0.01018917\n",
      "Epoch 39, change: 0.00999937\n",
      "Epoch 40, change: 0.00953631\n",
      "Epoch 41, change: 0.00944627\n",
      "Epoch 42, change: 0.00913631\n",
      "Epoch 43, change: 0.00883273\n",
      "Epoch 44, change: 0.00882177\n",
      "Epoch 45, change: 0.00851582\n",
      "Epoch 46, change: 0.00827704\n",
      "Epoch 47, change: 0.00802273\n",
      "Epoch 48, change: 0.00809341\n",
      "Epoch 49, change: 0.00756566\n",
      "Epoch 50, change: 0.00747596\n",
      "Epoch 51, change: 0.00744521\n",
      "Epoch 52, change: 0.00731335\n",
      "Epoch 53, change: 0.00683826\n",
      "Epoch 54, change: 0.00706027\n",
      "Epoch 55, change: 0.00676578\n",
      "Epoch 56, change: 0.00647593\n",
      "Epoch 57, change: 0.00654772\n",
      "Epoch 58, change: 0.00626437\n",
      "Epoch 59, change: 0.00621949\n",
      "Epoch 60, change: 0.00612801\n",
      "Epoch 61, change: 0.00590280\n",
      "Epoch 62, change: 0.00580072\n",
      "Epoch 63, change: 0.00564470\n",
      "Epoch 64, change: 0.00561650\n",
      "Epoch 65, change: 0.00535624\n",
      "Epoch 66, change: 0.00553120\n",
      "Epoch 67, change: 0.00517202\n",
      "Epoch 68, change: 0.00515802\n",
      "Epoch 69, change: 0.00506021\n",
      "Epoch 70, change: 0.00489267\n",
      "Epoch 71, change: 0.00483255\n",
      "Epoch 72, change: 0.00495550\n",
      "Epoch 73, change: 0.00467506\n",
      "Epoch 74, change: 0.00466248\n",
      "Epoch 75, change: 0.00464795\n",
      "Epoch 76, change: 0.00446885\n",
      "Epoch 77, change: 0.00435503\n",
      "Epoch 78, change: 0.00430943\n",
      "Epoch 79, change: 0.00419083\n",
      "Epoch 80, change: 0.00418716\n",
      "Epoch 81, change: 0.00411108\n",
      "Epoch 82, change: 0.00403582\n",
      "Epoch 83, change: 0.00401335\n",
      "Epoch 84, change: 0.00407700\n",
      "Epoch 85, change: 0.00379600\n",
      "Epoch 86, change: 0.00374254\n",
      "Epoch 87, change: 0.00369725\n",
      "Epoch 88, change: 0.00366729\n",
      "Epoch 89, change: 0.00357399\n",
      "Epoch 90, change: 0.00358960\n",
      "Epoch 91, change: 0.00346229\n",
      "Epoch 92, change: 0.00342713\n",
      "Epoch 93, change: 0.00340316\n",
      "Epoch 94, change: 0.00334516\n",
      "Epoch 95, change: 0.00324371\n",
      "Epoch 96, change: 0.00319718\n",
      "Epoch 97, change: 0.00314916\n",
      "Epoch 98, change: 0.00309855\n",
      "Epoch 99, change: 0.00311971\n",
      "Epoch 100, change: 0.00310512\n",
      "Epoch 101, change: 0.00303039\n",
      "Epoch 102, change: 0.00306592\n",
      "Epoch 103, change: 0.00299095\n",
      "Epoch 104, change: 0.00306398\n",
      "Epoch 105, change: 0.00305076\n",
      "Epoch 106, change: 0.00298636\n",
      "Epoch 107, change: 0.00298072\n",
      "Epoch 108, change: 0.00298687\n",
      "Epoch 109, change: 0.00293208\n",
      "Epoch 110, change: 0.00294510\n",
      "Epoch 111, change: 0.00298010\n",
      "Epoch 112, change: 0.00295800\n",
      "Epoch 113, change: 0.00292408\n",
      "Epoch 114, change: 0.00294574\n",
      "Epoch 115, change: 0.00291414\n",
      "Epoch 116, change: 0.00289759\n",
      "Epoch 117, change: 0.00290330\n",
      "Epoch 118, change: 0.00287925\n",
      "Epoch 119, change: 0.00288348\n",
      "Epoch 120, change: 0.00286592\n",
      "Epoch 121, change: 0.00286111\n",
      "Epoch 122, change: 0.00284222\n",
      "Epoch 123, change: 0.00285459\n",
      "Epoch 124, change: 0.00283892\n",
      "Epoch 125, change: 0.00282696\n",
      "Epoch 126, change: 0.00278064\n",
      "Epoch 127, change: 0.00283993\n",
      "Epoch 128, change: 0.00280356\n",
      "Epoch 129, change: 0.00279791\n",
      "Epoch 130, change: 0.00277362\n",
      "Epoch 131, change: 0.00h 141, change: 0.00331033\n",
      "Epoch 142, change: 0.00331651\n",
      "Epoch 143, change: 0.00328820\n",
      "Epoch 144, change: 0.00327950\n",
      "Epoch 145, change: 0.00326854\n",
      "Epoch 146, change: 0.00328252\n",
      "Epoch 147, change: 0.00326847\n",
      "Epoch 148, change: 0.00326182\n",
      "Epoch 149, change: 0.00323979\n",
      "Epoch 150, change: 0.00325015\n",
      "Epoch 151, change: 0.00323503\n",
      "Epoch 152, change: 0.00324079\n",
      "Epoch 153, change: 0.00321517\n",
      "Epoch 154, change: 0.00321132\n",
      "Epoch 155, change: 0.00320117\n",
      "Epoch 156, change: 0.00320630\n",
      "Epoch 157, change: 0.00320924\n",
      "Epoch 158, change: 0.00316722\n",
      "Epoch 159, change: 0.00318683\n",
      "Epoch 160, change: 0.00317492\n",
      "Epoch 161, change: 0.00316784\n",
      "Epoch 162, change: 0.00315749\n",
      "Epoch 163, change: 0.00314610\n",
      "Epoch 164, change: 0.00314377\n",
      "Epoch 165, change: 0.00314651\n",
      "Epoch 166, change: 0.00313042\n",
      "Epoch 167, change: 0.00312494\n",
      "Epoch 168, change: 0.00311722\n",
      "Epoch 169, change: 0.00310770\n",
      "Epoch 170, change: 0.00309347\n",
      "Epoch 171, change: 0.00309348\n",
      "Epoch 172, change: 0.00309859\n",
      "Epoch 173, change: 0.00307898\n",
      "Epoch 174, change: 0.00307328\n",
      "Epoch 175, change: 0.00306939\n",
      "Epoch 176, change: 0.00306308\n",
      "Epoch 177, change: 0.00306667\n",
      "Epoch 178, change: 0.00304677\n",
      "Epoch 179, change: 0.00304373\n",
      "Epoch 180, change: 0.00304648\n",
      "Epoch 181, change: 0.00301634\n",
      "Epoch 182, change: 0.00302659\n",
      "Epoch 183, change: 0.00300939\n",
      "Epoch 184, change: 0.00301626\n",
      "Epoch 185, change: 0.00300423\n",
      "Epoch 186, change: 0.00299363\n",
      "Epoch 187, change: 0.00299600\n",
      "Epoch 188, change: 0.00299455\n",
      "Epoch 189, change: 0.00297895\n",
      "Epoch 190, change: 0.00297281\n",
      "Epoch 191, change: 0.00295678\n",
      "Epoch 192, change: 0.00297080\n",
      "Epoch 193, change: 0.00294758\n",
      "Epoch 194, change: 0.00294928\n",
      "Epoch 195, change: 0.00293977\n",
      "Epoch 196, change: 0.00293655\n",
      "Epoch 197, change: 0.00293273\n",
      "Epoch 198, change: 0.00292744\n",
      "Epoch 199, change: 0.00291327\n",
      "Epoch 200, change: 0.00291506\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20295687\n",
      "Epoch 3, change: 0.09264634\n",
      "Epoch 4, change: 0.07380557\n",
      "Epoch 5, change: 0.05665352\n",
      "Epoch 6, change: 0.05338349\n",
      "Epoch 7, change: 0.05614548\n",
      "Epoch 8, change: 0.04484489\n",
      "Epoch 9, change: 0.03943951\n",
      "Epoch 10, change: 0.03767334\n",
      "Epoch 11, change: 0.03421809\n",
      "Epoch 12, change: 0.03303615\n",
      "Epoch 13, change: 0.03204106\n",
      "Epoch 14, change: 0.02969738\n",
      "Epoch 15, change: 0.02873853\n",
      "Epoch 16, change: 0.02768540\n",
      "Epoch 17, change: 0.02635140\n",
      "Epoch 18, change: 0.02576330\n",
      "Epoch 19, change: 0.02473032\n",
      "Epoch 20, change: 0.02405104\n",
      "Epoch 21, change: 0.02300967\n",
      "Epoch 22, change: 0.02241350\n",
      "Epoch 23, change: 0.02176457\n",
      "Epoch 24, change: 0.02120698\n",
      "Epoch 25, change: 0.02062708\n",
      "Epoch 26, change: 0.02018086\n",
      "Epoch 27, change: 0.01968843\n",
      "Epoch 28, change: 0.01932085\n",
      "Epoch 29, change: 0.01905815\n",
      "Epoch 30, change: 0.01849845\n",
      "Epoch 31, change: 0.01826524\n",
      "Epoch 32, change: 0.01795850\n",
      "Epoch 33, change: 0.01755703\n",
      "Epoch 34, change: 0.01735033\n",
      "Epoch 35, change: 0.01700493\n",
      "Epoch 36, change: 0.01681203\n",
      "Epoch 37, change: 0.01660684\n",
      "Epoch 38, change: 0.01620382\n",
      "Epoch 39, change: 0.01577093\n",
      "Epoch 40, change: 0.01537994\n",
      "Epoch 41, change: 0.01504393\n",
      "Epoch 42, change: 0.01465939\n",
      "Epoch 43, change: 0.01430982\n",
      "Epoch 44, change: 0.01401203\n",
      "Epoch 45, change: 0.01370159\n",
      "Epoch 46, change: 0.01335524\n",
      "Epoch 47, change: 0.01312786\n",
      "Epoch 48, change: 0.01286974\n",
      "Epoch 49, change: 0.01263401\n",
      "Epoch 50, change: 0.01232462\n",
      "Epoch 51, change: 0.01217579\n",
      "Epoch 52, change: 0.01189816\n",
      "Epoch 53, change: 0.01166442\n",
      "Epoch 54, change: 0.01147348\n",
      "Epoch 55, change: 0.01128238\n",
      "Epoch 56, change: 0.01113475\n",
      "Epoch 57, change: 0.01092087\n",
      "Epoch 58, change: 0.01069563\n",
      "Epoch 59, change: 0.01058525\n",
      "Epoch 60, change: 0.01036802\n",
      "Epoch 61, change: 0.01022970\n",
      "Epoch 62, change: 0.01007485\n",
      "Epoch 63, change: 0.00996383\n",
      "Epoch 64, change: 0.00978307\n",
      "Epoch 65, change: 0.00960402\n",
      "Epoch 66, change: 0.00954268\n",
      "Epoch 67, change: 0.00933238\n",
      "Epoch 68, change: 0.00924473\n",
      "Epoch 69, change: 0.00909739\n",
      "Epoch 70, change: 0.00899284\n",
      "Epoch 71, change: 0.00887024\n",
      "Epoch 72, change: 0.00875418\n",
      "Epoch 73, change: 0.00868548\n",
      "Epoch 74, change: 0.00847562\n",
      "Epoch 75, change: 0.00842734\n",
      "Epoch 76, change: 0.00834389\n",
      "Epoch 77, change: 0.00819276\n",
      "Epoch 78, change: 0.00811328\n",
      "Epoch 79, change: 0.00802053\n",
      "Epoch 80, change: h 141, change: 0.00266759\n",
      "Epoch 142, change: 0.00266727\n",
      "Epoch 143, change: 0.00263997\n",
      "Epoch 144, change: 0.00258078\n",
      "Epoch 145, change: 0.00257337\n",
      "Epoch 146, change: 0.00256664\n",
      "Epoch 147, change: 0.00252681\n",
      "Epoch 148, change: 0.00249921\n",
      "Epoch 149, change: 0.00248050\n",
      "Epoch 150, change: 0.00245047\n",
      "Epoch 151, change: 0.00242772\n",
      "Epoch 152, change: 0.00241156\n",
      "Epoch 153, change: 0.00238773\n",
      "Epoch 154, change: 0.00234984\n",
      "Epoch 155, change: 0.00234322\n",
      "Epoch 156, change: 0.00230510\n",
      "Epoch 157, change: 0.00227923\n",
      "Epoch 158, change: 0.00227528\n",
      "Epoch 159, change: 0.00224645\n",
      "Epoch 160, change: 0.00224020\n",
      "Epoch 161, change: 0.00220763\n",
      "Epoch 162, change: 0.00218508\n",
      "Epoch 163, change: 0.00215385\n",
      "Epoch 164, change: 0.00215188\n",
      "Epoch 165, change: 0.00212185\n",
      "Epoch 166, change: 0.00211041\n",
      "Epoch 167, change: 0.00207805\n",
      "Epoch 168, change: 0.00207094\n",
      "Epoch 169, change: 0.00204014\n",
      "Epoch 170, change: 0.00203119\n",
      "Epoch 171, change: 0.00200846\n",
      "Epoch 172, change: 0.00199240\n",
      "Epoch 173, change: 0.00196715\n",
      "Epoch 174, change: 0.00194609\n",
      "Epoch 175, change: 0.00193571\n",
      "Epoch 176, change: 0.00191231\n",
      "Epoch 177, change: 0.00190102\n",
      "Epoch 178, change: 0.00194929\n",
      "Epoch 179, change: 0.00193188\n",
      "Epoch 180, change: 0.00190839\n",
      "Epoch 181, change: 0.00186899\n",
      "Epoch 182, change: 0.00186157\n",
      "Epoch 183, change: 0.00183922\n",
      "Epoch 184, change: 0.00181391\n",
      "Epoch 185, change: 0.00180349\n",
      "Epoch 186, change: 0.00179478\n",
      "Epoch 187, change: 0.00175696\n",
      "Epoch 188, change: 0.00175104\n",
      "Epoch 189, change: 0.00172783\n",
      "Epoch 190, change: 0.00170909\n",
      "Epoch 191, change: 0.00169271\n",
      "Epoch 192, change: 0.00166798\n",
      "Epoch 193, change: 0.00165812\n",
      "Epoch 194, change: 0.00164468\n",
      "Epoch 195, change: 0.00162082\n",
      "Epoch 196, change: 0.00160615\n",
      "Epoch 197, change: 0.00159784\n",
      "Epoch 198, change: 0.00156896\n",
      "Epoch 199, change: 0.00158280\n",
      "Epoch 200, change: 0.00154771\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27517742\n",
      "Epoch 3, change: 0.14767862\n",
      "Epoch 4, change: 0.07043110\n",
      "Epoch 5, change: 0.05149986\n",
      "Epoch 6, change: 0.04508114\n",
      "Epoch 7, change: 0.04037601\n",
      "Epoch 8, change: 0.03416220\n",
      "Epoch 9, change: 0.02940053\n",
      "Epoch 10, change: 0.02808252\n",
      "Epoch 11, change: 0.02583519\n",
      "Epoch 12, change: 0.02502242\n",
      "Epoch 13, change: 0.02240895\n",
      "Epoch 14, change: 0.02225729\n",
      "Epoch 15, change: 0.02106882\n",
      "Epoch 16, change: 0.02056849\n",
      "Epoch 17, change: 0.01996087\n",
      "Epoch 18, change: 0.01915652\n",
      "Epoch 19, change: 0.01880030\n",
      "Epoch 20, change: 0.01812712\n",
      "Epoch 21, change: 0.01720976\n",
      "Epoch 22, change: 0.01731401\n",
      "Epoch 23, change: 0.01669386\n",
      "Epoch 24, change: 0.01628648\n",
      "Epoch 25, change: 0.01566274\n",
      "Epoch 26, change: 0.01562426\n",
      "Epoch 27, change: 0.01542762\n",
      "Epoch 28, change: 0.01511022\n",
      "Epoch 29, change: 0.01487745\n",
      "Epoch 30, change: 0.01442925\n",
      "Epoch 31, change: 0.01453712\n",
      "Epoch 32, change: 0.01431708\n",
      "Epoch 33, change: 0.01379923\n",
      "Epoch 34, change: 0.01367109\n",
      "Epoch 35, change: 0.01343692\n",
      "Epoch 36, change: 0.01332153\n",
      "Epoch 37, change: 0.01287991\n",
      "Epoch 38, change: 0.01283145\n",
      "Epoch 39, change: 0.01278524\n",
      "Epoch 40, change: 0.01236676\n",
      "Epoch 41, change: 0.01246652\n",
      "Epoch 42, change: 0.01217262\n",
      "Epoch 43, change: 0.01189978\n",
      "Epoch 44, change: 0.01173311\n",
      "Epoch 45, change: 0.01174459\n",
      "Epoch 46, change: 0.01148600\n",
      "Epoch 47, change: 0.01126665\n",
      "Epoch 48, change: 0.01116111\n",
      "Epoch 49, change: 0.01097656\n",
      "Epoch 50, change: 0.01080792\n",
      "Epoch 51, change: 0.01078481\n",
      "Epoch 52, change: 0.01066580\n",
      "Epoch 53, change: 0.01032447\n",
      "Epoch 54, change: 0.01034736\n",
      "Epoch 55, change: 0.01007021\n",
      "Epoch 56, change: 0.00992475\n",
      "Epoch 57, change: 0.00981119\n",
      "Epoch 58, change: 0.00952775\n",
      "Epoch 59, change: 0.00938657\n",
      "Epoch 60, change: 0.00925769\n",
      "Epoch 61, change: 0.00904349\n",
      "Epoch 62, change: 0.00890657\n",
      "Epoch 63, change: 0.00868808\n",
      "Epoch 64, change: 0.00868251\n",
      "Epoch 65, change: 0.00839723\n",
      "Epoch 66, change: 0.00835304\n",
      "Epoch 67, change: 0.00812795\n",
      "Epoch 68, change: 0.00802869\n",
      "Epoch 69, change: 0.00788943\n",
      "Epoch 70, change: 0.00775101\n",
      "Epoch 71, change: 0.00760885\n",
      "Epoch 72, change: 0.00761669\n",
      "Epoch 73, change: 0.00735215\n",
      "Epoch 74, change: 0.00726688\n",
      "Epoch 75, change: 0.00724655\n",
      "Epoch 76, change: 0.00707486\n",
      "Epoch 77, change: 0.00699980\n",
      "Epoch 78, change: 0.00679925\n",
      "Epoch 79, change: 0.00677072\n",
      "Epoch 80, change: Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27323332\n",
      "Epoch 3, change: 0.12129696\n",
      "Epoch 4, change: 0.12429481\n",
      "Epoch 5, change: 0.11270082\n",
      "Epoch 6, change: 0.08606712\n",
      "Epoch 7, change: 0.17479503\n",
      "Epoch 8, change: 0.13470462\n",
      "Epoch 9, change: 0.07609602\n",
      "Epoch 10, change: 0.05130412\n",
      "Epoch 11, change: 0.03535887\n",
      "Epoch 12, change: 0.02369614\n",
      "Epoch 13, change: 0.02175894\n",
      "Epoch 14, change: 0.02012837\n",
      "Epoch 15, change: 0.01768388\n",
      "Epoch 16, change: 0.01721821\n",
      "Epoch 17, change: 0.01629065\n",
      "Epoch 18, change: 0.01649536\n",
      "Epoch 19, change: 0.01646173\n",
      "Epoch 20, change: 0.01568853\n",
      "Epoch 21, change: 0.01437785\n",
      "Epoch 22, change: 0.01408121\n",
      "Epoch 23, change: 0.01390811\n",
      "Epoch 24, change: 0.01312945\n",
      "Epoch 25, change: 0.01206616\n",
      "Epoch 26, change: 0.01133484\n",
      "Epoch 27, change: 0.01085035\n",
      "Epoch 28, change: 0.01034712\n",
      "Epoch 29, change: 0.00993768\n",
      "Epoch 30, change: 0.00930577\n",
      "Epoch 31, change: 0.00877604\n",
      "Epoch 32, change: 0.00844510\n",
      "Epoch 33, change: 0.00811955\n",
      "Epoch 34, change: 0.00772886\n",
      "Epoch 35, change: 0.00734965\n",
      "Epoch 36, change: 0.00695761\n",
      "Epoch 37, change: 0.00663902\n",
      "Epoch 38, change: 0.00631895\n",
      "Epoch 39, change: 0.00604555\n",
      "Epoch 40, change: 0.00577564\n",
      "Epoch 41, change: 0.00550871\n",
      "Epoch 42, change: 0.00524039\n",
      "Epoch 43, change: 0.00500005\n",
      "Epoch 44, change: 0.00477248\n",
      "Epoch 45, change: 0.00455025\n",
      "Epoch 46, change: 0.00437390\n",
      "Epoch 47, change: 0.00416592\n",
      "Epoch 48, change: 0.00398095\n",
      "Epoch 49, change: 0.00380869\n",
      "Epoch 50, change: 0.00364159\n",
      "Epoch 51, change: 0.00349320\n",
      "Epoch 52, change: 0.00335710\n",
      "Epoch 53, change: 0.00320274\n",
      "Epoch 54, change: 0.00306745\n",
      "Epoch 55, change: 0.00293902\n",
      "Epoch 56, change: 0.00281986\n",
      "Epoch 57, change: 0.00269739\n",
      "Epoch 58, change: 0.00258128\n",
      "Epoch 59, change: 0.00247361\n",
      "Epoch 60, change: 0.00237126\n",
      "Epoch 61, change: 0.00227426\n",
      "Epoch 62, change: 0.00218689\n",
      "Epoch 63, change: 0.00209996\n",
      "Epoch 64, change: 0.00201975\n",
      "Epoch 65, change: 0.00193330\n",
      "Epoch 66, change: 0.00184807\n",
      "Epoch 67, change: 0.00176987\n",
      "Epoch 68, change: 0.00170113\n",
      "Epoch 69, change: 0.00163742\n",
      "Epoch 70, change: 0.00157390\n",
      "Epoch 71, change: 0.00151191\n",
      "Epoch 72, change: 0.00144694\n",
      "Epoch 73, change: 0.00138456\n",
      "Epoch 74, change: 0.00133463\n",
      "Epoch 75, change: 0.00128837\n",
      "Epoch 76, change: 0.00123777\n",
      "Epoch 77, change: 0.00118631\n",
      "Epoch 78, change: 0.00114120\n",
      "Epoch 79, change: 0.00109620\n",
      "Epoch 80, change: 0.00104997\n",
      "Epoch 81, change: 0.00100826\n",
      "Epoch 82, change: 0.00096907\n",
      "Epoch 83, change: 0.00093398\n",
      "Epoch 84, change: 0.00090113\n",
      "Epoch 85, change: 0.00086847\n",
      "Epoch 86, change: 0.00083259\n",
      "Epoch 87, change: 0.00080143\n",
      "Epoch 88, change: 0.00076858\n",
      "Epoch 89, change: 0.00074071\n",
      "Epoch 90, change: 0.00071458\n",
      "Epoch 91, change: 0.00068508\n",
      "Epoch 92, change: 0.00065739\n",
      "Epoch 93, change: 0.00063403\n",
      "Epoch 94, change: 0.00061230\n",
      "Epoch 95, change: 0.00058982\n",
      "Epoch 96, change: 0.00056855\n",
      "Epoch 97, change: 0.00054435\n",
      "Epoch 98, change: 0.00052393\n",
      "Epoch 99, change: 0.00050226\n",
      "Epoch 100, change: 0.00048565\n",
      "Epoch 101, change: 0.00046948\n",
      "Epoch 102, change: 0.00044883\n",
      "Epoch 103, change: 0.00043406\n",
      "Epoch 104, change: 0.00041670\n",
      "Epoch 105, change: 0.00040115\n",
      "Epoch 106, change: 0.00038681\n",
      "Epoch 107, change: 0.00037272\n",
      "Epoch 108, change: 0.00035832\n",
      "Epoch 109, change: 0.00034476\n",
      "Epoch 110, change: 0.00033241\n",
      "Epoch 111, change: 0.00032004\n",
      "Epoch 112, change: 0.00030856\n",
      "Epoch 113, change: 0.00029676\n",
      "Epoch 114, change: 0.00028696\n",
      "Epoch 115, change: 0.00027562\n",
      "Epoch 116, change: 0.00026595\n",
      "Epoch 117, change: 0.00025628\n",
      "Epoch 118, change: 0.00024727\n",
      "Epoch 119, change: 0.00023765\n",
      "Epoch 120, change: 0.00022836\n",
      "Epoch 121, change: 0.00022098\n",
      "Epoch 122, change: 0.00021185\n",
      "Epoch 123, change: 0.00020492\n",
      "Epoch 124, change: 0.00019782\n",
      "Epoch 125, change: 0.00019026\n",
      "Epoch 126, change: 0.00018342\n",
      "Epoch 127, change: 0.00017705\n",
      "Epoch 128, change: 0.00017001\n",
      "Epoch 129, change: 0.00016379\n",
      "Epoch 130, change: 0.00015795\n",
      "Epoch 131, change: 0.00015225\n",
      "Epoch 132, change: 0.00014695\n",
      "Epoch 133, change: 0.00014188\n",
      "Epoch 134, change: 0.00013666\n",
      "Epoch 135, change: 0.00013183\n",
      "Epoch 136, change: 0.00012704\n",
      "Epoch 137, change: 0.00012256\n",
      "Epoch 138, change: 0.00011781\n",
      "Epoch 139, change: 0.00011366\n",
      "Epoch 140, change: 0.00010984\n",
      "EpocEpoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.39500168\n",
      "Epoch 3, change: 0.24050189\n",
      "Epoch 4, change: 0.06756132\n",
      "Epoch 5, change: 0.12879183\n",
      "Epoch 6, change: 0.13102519\n",
      "Epoch 7, change: 0.07618329\n",
      "Epoch 8, change: 0.07501878\n",
      "Epoch 9, change: 0.04863898\n",
      "Epoch 10, change: 0.04353607\n",
      "Epoch 11, change: 0.03071332\n",
      "Epoch 12, change: 0.02702803\n",
      "Epoch 13, change: 0.02090781\n",
      "Epoch 14, change: 0.02036553\n",
      "Epoch 15, change: 0.01745874\n",
      "Epoch 16, change: 0.01816058\n",
      "Epoch 17, change: 0.01597115\n",
      "Epoch 18, change: 0.01519244\n",
      "Epoch 19, change: 0.01186927\n",
      "Epoch 20, change: 0.01164108\n",
      "Epoch 21, change: 0.01096565\n",
      "Epoch 22, change: 0.01043581\n",
      "Epoch 23, change: 0.00959197\n",
      "Epoch 24, change: 0.00898350\n",
      "Epoch 25, change: 0.00851815\n",
      "Epoch 26, change: 0.00797820\n",
      "Epoch 27, change: 0.00790065\n",
      "Epoch 28, change: 0.00767464\n",
      "Epoch 29, change: 0.00737954\n",
      "Epoch 30, change: 0.00712561\n",
      "Epoch 31, change: 0.00693256\n",
      "Epoch 32, change: 0.00670063\n",
      "Epoch 33, change: 0.00637669\n",
      "Epoch 34, change: 0.00626229\n",
      "Epoch 35, change: 0.00605577\n",
      "Epoch 36, change: 0.00580092\n",
      "Epoch 37, change: 0.00562725\n",
      "Epoch 38, change: 0.00549514\n",
      "Epoch 39, change: 0.00531388\n",
      "Epoch 40, change: 0.00509327\n",
      "Epoch 41, change: 0.00488241\n",
      "Epoch 42, change: 0.00471134\n",
      "Epoch 43, change: 0.00464347\n",
      "Epoch 44, change: 0.00446006\n",
      "Epoch 45, change: 0.00430775\n",
      "Epoch 46, change: 0.00416411\n",
      "Epoch 47, change: 0.00398347\n",
      "Epoch 48, change: 0.00386423\n",
      "Epoch 49, change: 0.00372627\n",
      "Epoch 50, change: 0.00363800\n",
      "Epoch 51, change: 0.00351073\n",
      "Epoch 52, change: 0.00333698\n",
      "Epoch 53, change: 0.00324783\n",
      "Epoch 54, change: 0.00313290\n",
      "Epoch 55, change: 0.00301781\n",
      "Epoch 56, change: 0.00294583\n",
      "Epoch 57, change: 0.00282906\n",
      "Epoch 58, change: 0.00271117\n",
      "Epoch 59, change: 0.00262211\n",
      "Epoch 60, change: 0.00250782\n",
      "Epoch 61, change: 0.00246456\n",
      "Epoch 62, change: 0.00239022\n",
      "Epoch 63, change: 0.00225077\n",
      "Epoch 64, change: 0.00218043\n",
      "Epoch 65, change: 0.00212170\n",
      "Epoch 66, change: 0.00205314\n",
      "Epoch 67, change: 0.00197678\n",
      "Epoch 68, change: 0.00190070\n",
      "Epoch 69, change: 0.00184442\n",
      "Epoch 70, change: 0.00178786\n",
      "Epoch 71, change: 0.00171325\n",
      "Epoch 72, change: 0.00165540\n",
      "Epoch 73, change: 0.00160308\n",
      "Epoch 74, change: 0.00154667\n",
      "Epoch 75, change: 0.00148234\n",
      "Epoch 76, change: 0.00143392\n",
      "Epoch 77, change: 0.00138787\n",
      "Epoch 78, change: 0.00133905\n",
      "Epoch 79, change: 0.00128688\n",
      "Epoch 80, change: 0.00124652\n",
      "Epoch 81, change: 0.00120815\n",
      "Epoch 82, change: 0.00116568\n",
      "Epoch 83, change: 0.00112391\n",
      "Epoch 84, change: 0.00107918\n",
      "Epoch 85, change: 0.00104380\n",
      "Epoch 86, change: 0.00101275\n",
      "Epoch 87, change: 0.00097200\n",
      "Epoch 88, change: 0.00094367\n",
      "Epoch 89, change: 0.00090718\n",
      "Epoch 90, change: 0.00088430\n",
      "Epoch 91, change: 0.00084771\n",
      "Epoch 92, change: 0.00081422\n",
      "Epoch 93, change: 0.00079068\n",
      "Epoch 94, change: 0.00076143\n",
      "Epoch 95, change: 0.00073807\n",
      "Epoch 96, change: 0.00071061\n",
      "Epoch 97, change: 0.00069147\n",
      "Epoch 98, change: 0.00066856\n",
      "Epoch 99, change: 0.00063745\n",
      "Epoch 100, change: 0.00061771\n",
      "Epoch 101, change: 0.00060071\n",
      "Epoch 102, change: 0.00058027\n",
      "Epoch 103, change: 0.00055607\n",
      "Epoch 104, change: 0.00053805\n",
      "Epoch 105, change: 0.00051670\n",
      "Epoch 106, change: 0.00050364\n",
      "Epoch 107, change: 0.00048981\n",
      "Epoch 108, change: 0.00047164\n",
      "Epoch 109, change: 0.00045314\n",
      "Epoch 110, change: 0.00043628\n",
      "Epoch 111, change: 0.00042318\n",
      "Epoch 112, change: 0.00041051\n",
      "Epoch 113, change: 0.00039522\n",
      "Epoch 114, change: 0.00038048\n",
      "Epoch 115, change: 0.00036843\n",
      "Epoch 116, change: 0.00035491\n",
      "Epoch 117, change: 0.00034372\n",
      "Epoch 118, change: 0.00033223\n",
      "Epoch 119, change: 0.00032171\n",
      "Epoch 120, change: 0.00030941\n",
      "Epoch 121, change: 0.00029947\n",
      "Epoch 122, change: 0.00028775\n",
      "Epoch 123, change: 0.00027929\n",
      "Epoch 124, change: 0.00027025\n",
      "Epoch 125, change: 0.00026158\n",
      "Epoch 126, change: 0.00025294\n",
      "Epoch 127, change: 0.00024237\n",
      "Epoch 128, change: 0.00023437\n",
      "Epoch 129, change: 0.00022808\n",
      "Epoch 130, change: 0.00021898\n",
      "Epoch 131, change: 0.00021257\n",
      "Epoch 132, change: 0.00020456\n",
      "Epoch 133, change: 0.00019722\n",
      "Epoch 134, change: 0.00019177\n",
      "Epoch 135, change: 0.00018525\n",
      "Epoch 136, change: 0.00017905\n",
      "Epoch 137, change: 0.00017258\n",
      "Epoch 138, change: 0.00016544\n",
      "Epoch 139, change: 0.00016129\n",
      "Epoch 140, change: 0.00015544\n",
      "Epoch 141, change: 0.00453644\n",
      "Epoch 142, change: 0.00451655\n",
      "Epoch 143, change: 0.00446113\n",
      "Epoch 144, change: 0.00445200\n",
      "Epoch 145, change: 0.00441244\n",
      "Epoch 146, change: 0.00438625\n",
      "Epoch 147, change: 0.00435202\n",
      "Epoch 148, change: 0.00432986\n",
      "Epoch 149, change: 0.00429641\n",
      "Epoch 150, change: 0.00427564\n",
      "Epoch 151, change: 0.00423635\n",
      "Epoch 152, change: 0.00420971\n",
      "Epoch 153, change: 0.00418495\n",
      "Epoch 154, change: 0.00415047\n",
      "Epoch 155, change: 0.00413487\n",
      "Epoch 156, change: 0.00409557\n",
      "Epoch 157, change: 0.00407009\n",
      "Epoch 158, change: 0.00404737\n",
      "Epoch 159, change: 0.00402862\n",
      "Epoch 160, change: 0.00399959\n",
      "Epoch 161, change: 0.00397286\n",
      "Epoch 162, change: 0.00394535\n",
      "Epoch 163, change: 0.00392900\n",
      "Epoch 164, change: 0.00390090\n",
      "Epoch 165, change: 0.00387682\n",
      "Epoch 166, change: 0.00385131\n",
      "Epoch 167, change: 0.00382646\n",
      "Epoch 168, change: 0.00380605\n",
      "Epoch 169, change: 0.00378556\n",
      "Epoch 170, change: 0.00375679\n",
      "Epoch 171, change: 0.00373938\n",
      "Epoch 172, change: 0.00371535\n",
      "Epoch 173, change: 0.00369643\n",
      "Epoch 174, change: 0.00367194\n",
      "Epoch 175, change: 0.00364834\n",
      "Epoch 176, change: 0.00363188\n",
      "Epoch 177, change: 0.00361102\n",
      "Epoch 178, change: 0.00359554\n",
      "Epoch 179, change: 0.00356209\n",
      "Epoch 180, change: 0.00354807\n",
      "Epoch 181, change: 0.00352606\n",
      "Epoch 182, change: 0.00350619\n",
      "Epoch 183, change: 0.00349374\n",
      "Epoch 184, change: 0.00346640\n",
      "Epoch 185, change: 0.00344785\n",
      "Epoch 186, change: 0.00343476\n",
      "Epoch 187, change: 0.00340963\n",
      "Epoch 188, change: 0.00339260\n",
      "Epoch 189, change: 0.00337509\n",
      "Epoch 190, change: 0.00335351\n",
      "Epoch 191, change: 0.00333771\n",
      "Epoch 192, change: 0.00331762\n",
      "Epoch 193, change: 0.00330636\n",
      "Epoch 194, change: 0.00328747\n",
      "Epoch 195, change: 0.00327255\n",
      "Epoch 196, change: 0.00324409\n",
      "Epoch 197, change: 0.00323517\n",
      "Epoch 198, change: 0.00322205\n",
      "Epoch 199, change: 0.00319854\n",
      "Epoch 200, change: 0.00318260\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.22047582\n",
      "Epoch 3, change: 0.12435532\n",
      "Epoch 4, change: 0.06895050\n",
      "Epoch 5, change: 0.04276631\n",
      "Epoch 6, change: 0.03563651\n",
      "Epoch 7, change: 0.03240542\n",
      "Epoch 8, change: 0.02654671\n",
      "Epoch 9, change: 0.02432830\n",
      "Epoch 10, change: 0.02234953\n",
      "Epoch 11, change: 0.02106535\n",
      "Epoch 12, change: 0.02000270\n",
      "Epoch 13, change: 0.01785178\n",
      "Epoch 14, change: 0.01745565\n",
      "Epoch 15, change: 0.01654919\n",
      "Epoch 16, change: 0.01531927\n",
      "Epoch 17, change: 0.01485029\n",
      "Epoch 18, change: 0.01427053\n",
      "Epoch 19, change: 0.01379257\n",
      "Epoch 20, change: 0.01383189\n",
      "Epoch 21, change: 0.01298539\n",
      "Epoch 22, change: 0.01315099\n",
      "Epoch 23, change: 0.01282275\n",
      "Epoch 24, change: 0.01290661\n",
      "Epoch 25, change: 0.01223610\n",
      "Epoch 26, change: 0.01224672\n",
      "Epoch 27, change: 0.01245109\n",
      "Epoch 28, change: 0.01192335\n",
      "Epoch 29, change: 0.01197704\n",
      "Epoch 30, change: 0.01192544\n",
      "Epoch 31, change: 0.01158156\n",
      "Epoch 32, change: 0.01153546\n",
      "Epoch 33, change: 0.01141426\n",
      "Epoch 34, change: 0.01099032\n",
      "Epoch 35, change: 0.01098594\n",
      "Epoch 36, change: 0.01090372\n",
      "Epoch 37, change: 0.01067914\n",
      "Epoch 38, change: 0.01038001\n",
      "Epoch 39, change: 0.01043213\n",
      "Epoch 40, change: 0.01047545\n",
      "Epoch 41, change: 0.01027204\n",
      "Epoch 42, change: 0.00984022\n",
      "Epoch 43, change: 0.00997742\n",
      "Epoch 44, change: 0.00988882\n",
      "Epoch 45, change: 0.00975511\n",
      "Epoch 46, change: 0.00951255\n",
      "Epoch 47, change: 0.00946606\n",
      "Epoch 48, change: 0.00920213\n",
      "Epoch 49, change: 0.00891602\n",
      "Epoch 50, change: 0.00870742\n",
      "Epoch 51, change: 0.00866710\n",
      "Epoch 52, change: 0.00848058\n",
      "Epoch 53, change: 0.00821119\n",
      "Epoch 54, change: 0.00818193\n",
      "Epoch 55, change: 0.00792247\n",
      "Epoch 56, change: 0.00776943\n",
      "Epoch 57, change: 0.00764722\n",
      "Epoch 58, change: 0.00743671\n",
      "Epoch 59, change: 0.00740366\n",
      "Epoch 60, change: 0.00737682\n",
      "Epoch 61, change: 0.00709529\n",
      "Epoch 62, change: 0.00699167\n",
      "Epoch 63, change: 0.00678950\n",
      "Epoch 64, change: 0.00678522\n",
      "Epoch 65, change: 0.00666245\n",
      "Epoch 66, change: 0.00660001\n",
      "Epoch 67, change: 0.00650668\n",
      "Epoch 68, change: 0.00650252\n",
      "Epoch 69, change: 0.00640921\n",
      "Epoch 70, change: 0.00640229\n",
      "Epoch 71, change: 0.00629963\n",
      "Epoch 72, change: 0.00633145\n",
      "Epoch 73, change: 0.00629063\n",
      "Epoch 74, change: 0.00623795\n",
      "Epoch 75, change: 0.00616129\n",
      "Epoch 76, change: 0.00616989\n",
      "Epoch 77, change: 0.00610957\n",
      "Epoch 78, change: 0.00603470\n",
      "Epoch 79, change: 0.00608423\n",
      "Epoch 80, change: 0.00793522\n",
      "Epoch 81, change: 0.00779330\n",
      "Epoch 82, change: 0.00771794\n",
      "Epoch 83, change: 0.00766783\n",
      "Epoch 84, change: 0.00757554\n",
      "Epoch 85, change: 0.00748929\n",
      "Epoch 86, change: 0.00738537\n",
      "Epoch 87, change: 0.00732556\n",
      "Epoch 88, change: 0.00722356\n",
      "Epoch 89, change: 0.00713772\n",
      "Epoch 90, change: 0.00711051\n",
      "Epoch 91, change: 0.00701825\n",
      "Epoch 92, change: 0.00692576\n",
      "Epoch 93, change: 0.00687588\n",
      "Epoch 94, change: 0.00678660\n",
      "Epoch 95, change: 0.00673554\n",
      "Epoch 96, change: 0.00667782\n",
      "Epoch 97, change: 0.00660110\n",
      "Epoch 98, change: 0.00653623\n",
      "Epoch 99, change: 0.00647129\n",
      "Epoch 100, change: 0.00641238\n",
      "Epoch 101, change: 0.00636691\n",
      "Epoch 102, change: 0.00627212\n",
      "Epoch 103, change: 0.00624150\n",
      "Epoch 104, change: 0.00616327\n",
      "Epoch 105, change: 0.00610365\n",
      "Epoch 106, change: 0.00607437\n",
      "Epoch 107, change: 0.00600350\n",
      "Epoch 108, change: 0.00593604\n",
      "Epoch 109, change: 0.00589280\n",
      "Epoch 110, change: 0.00584257\n",
      "Epoch 111, change: 0.00579630\n",
      "Epoch 112, change: 0.00573579\n",
      "Epoch 113, change: 0.00570235\n",
      "Epoch 114, change: 0.00564303\n",
      "Epoch 115, change: 0.00559374\n",
      "Epoch 116, change: 0.00556254\n",
      "Epoch 117, change: 0.00549990\n",
      "Epoch 118, change: 0.00545099\n",
      "Epoch 119, change: 0.00541581\n",
      "Epoch 120, change: 0.00536603\n",
      "Epoch 121, change: 0.00531664\n",
      "Epoch 122, change: 0.00528733\n",
      "Epoch 123, change: 0.00523440\n",
      "Epoch 124, change: 0.00519876\n",
      "Epoch 125, change: 0.00515619\n",
      "Epoch 126, change: 0.00511883\n",
      "Epoch 127, change: 0.00506834\n",
      "Epoch 128, change: 0.00503868\n",
      "Epoch 129, change: 0.00499716\n",
      "Epoch 130, change: 0.00496330\n",
      "Epoch 131, change: 0.00491510\n",
      "Epoch 132, change: 0.00489278\n",
      "Epoch 133, change: 0.00485325\n",
      "Epoch 134, change: 0.00482350\n",
      "Epoch 135, change: 0.00477314\n",
      "Epoch 136, change: 0.00474653\n",
      "Epoch 137, change: 0.00471126\n",
      "Epoch 138, change: 0.00467494\n",
      "Epoch 139, change: 0.00464155\n",
      "Epoch 140, change: 0.00461637\n",
      "Epoch 141, change: 0.00457044\n",
      "Epoch 142, change: 0.00455086\n",
      "Epoch 143, change: 0.00449326\n",
      "Epoch 144, change: 0.00448494\n",
      "Epoch 145, change: 0.00444510\n",
      "Epoch 146, change: 0.00441800\n",
      "Epoch 147, change: 0.00438442\n",
      "Epoch 148, change: 0.00436160\n",
      "Epoch 149, change: 0.00432712\n",
      "Epoch 150, change: 0.00430771\n",
      "Epoch 151, change: 0.00426735\n",
      "Epoch 152, change: 0.00424048\n",
      "Epoch 153, change: 0.00421541\n",
      "Epoch 154, change: 0.00418007\n",
      "Epoch 155, change: 0.00416476\n",
      "Epoch 156, change: 0.00412417\n",
      "Epoch 157, change: 0.00409790\n",
      "Epoch 158, change: 0.00407579\n",
      "Epoch 159, change: 0.00405734\n",
      "Epoch 160, change: 0.00402835\n",
      "Epoch 161, change: 0.00400142\n",
      "Epoch 162, change: 0.00397430\n",
      "Epoch 163, change: 0.00395718\n",
      "Epoch 164, change: 0.00392890\n",
      "Epoch 165, change: 0.00390401\n",
      "Epoch 166, change: 0.00387861\n",
      "Epoch 167, change: 0.00385282\n",
      "Epoch 168, change: 0.00383127\n",
      "Epoch 169, change: 0.00381150\n",
      "Epoch 170, change: 0.00378390\n",
      "Epoch 171, change: 0.00376551\n",
      "Epoch 172, change: 0.00374155\n",
      "Epoch 173, change: 0.00372195\n",
      "Epoch 174, change: 0.00369711\n",
      "Epoch 175, change: 0.00367356\n",
      "Epoch 176, change: 0.00365678\n",
      "Epoch 177, change: 0.00363577\n",
      "Epoch 178, change: 0.00361987\n",
      "Epoch 179, change: 0.00358616\n",
      "Epoch 180, change: 0.00357216\n",
      "Epoch 181, change: 0.00354894\n",
      "Epoch 182, change: 0.00352935\n",
      "Epoch 183, change: 0.00351714\n",
      "Epoch 184, change: 0.00348879\n",
      "Epoch 185, change: 0.00346944\n",
      "Epoch 186, change: 0.00345664\n",
      "Epoch 187, change: 0.00343107\n",
      "Epoch 188, change: 0.00341456\n",
      "Epoch 189, change: 0.00339642\n",
      "Epoch 190, change: 0.00337396\n",
      "Epoch 191, change: 0.00335831\n",
      "Epoch 192, change: 0.00333824\n",
      "Epoch 193, change: 0.00332716\n",
      "Epoch 194, change: 0.00330819\n",
      "Epoch 195, change: 0.00329205\n",
      "Epoch 196, change: 0.00326414\n",
      "Epoch 197, change: 0.00325443\n",
      "Epoch 198, change: 0.00324217\n",
      "Epoch 199, change: 0.00321771\n",
      "Epoch 200, change: 0.00320138\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.21875995\n",
      "Epoch 3, change: 0.11898138\n",
      "Epoch 4, change: 0.06443170\n",
      "Epoch 5, change: 0.04352827\n",
      "Epoch 6, change: 0.03695263\n",
      "Epoch 7, change: 0.03273297\n",
      "Epoch 8, change: 0.02754956\n",
      "Epoch 9, change: 0.02451485\n",
      "Epoch 10, change: 0.02316904\n",
      "Epoch 11, change: 0.02175020\n",
      "Epoch 12, change: 0.02062338\n",
      "Epoch 13, change: 0.01841731\n",
      "Epoch 14, change: 0.01797541\n",
      "Epoch 15, change: 0.01702557\n",
      "Epoch 16, change: 0.01574031\n",
      "Epoch 17, change: 0.01530237\n",
      "Epoch 18, change: 0.h 141, change: 0.00015023\n",
      "Epoch 142, change: 0.00014508\n",
      "Epoch 143, change: 0.00014030\n",
      "Epoch 144, change: 0.00013575\n",
      "Epoch 145, change: 0.00013117\n",
      "Epoch 146, change: 0.00012657\n",
      "Epoch 147, change: 0.00012243\n",
      "Epoch 148, change: 0.00011824\n",
      "Epoch 149, change: 0.00011416\n",
      "Epoch 150, change: 0.00011025\n",
      "Epoch 151, change: 0.00010640\n",
      "Epoch 152, change: 0.00010292\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.46492035\n",
      "Epoch 3, change: 0.13870705\n",
      "Epoch 4, change: 0.10855037\n",
      "Epoch 5, change: 0.05345119\n",
      "Epoch 6, change: 0.09587660\n",
      "Epoch 7, change: 0.07396647\n",
      "Epoch 8, change: 0.05841450\n",
      "Epoch 9, change: 0.04687450\n",
      "Epoch 10, change: 0.03945885\n",
      "Epoch 11, change: 0.02495144\n",
      "Epoch 12, change: 0.02576785\n",
      "Epoch 13, change: 0.02633052\n",
      "Epoch 14, change: 0.02384006\n",
      "Epoch 15, change: 0.02155807\n",
      "Epoch 16, change: 0.01991160\n",
      "Epoch 17, change: 0.02079195\n",
      "Epoch 18, change: 0.01900593\n",
      "Epoch 19, change: 0.01709103\n",
      "Epoch 20, change: 0.01697547\n",
      "Epoch 21, change: 0.01566644\n",
      "Epoch 22, change: 0.01490982\n",
      "Epoch 23, change: 0.01442449\n",
      "Epoch 24, change: 0.01347860\n",
      "Epoch 25, change: 0.01244562\n",
      "Epoch 26, change: 0.01193174\n",
      "Epoch 27, change: 0.01132208\n",
      "Epoch 28, change: 0.01057324\n",
      "Epoch 29, change: 0.01028118\n",
      "Epoch 30, change: 0.00958354\n",
      "Epoch 31, change: 0.00904154\n",
      "Epoch 32, change: 0.00859882\n",
      "Epoch 33, change: 0.00820104\n",
      "Epoch 34, change: 0.00759979\n",
      "Epoch 35, change: 0.00737105\n",
      "Epoch 36, change: 0.00687174\n",
      "Epoch 37, change: 0.00641882\n",
      "Epoch 38, change: 0.00612255\n",
      "Epoch 39, change: 0.00592200\n",
      "Epoch 40, change: 0.00548172\n",
      "Epoch 41, change: 0.00509502\n",
      "Epoch 42, change: 0.00497609\n",
      "Epoch 43, change: 0.00469968\n",
      "Epoch 44, change: 0.00448176\n",
      "Epoch 45, change: 0.00415978\n",
      "Epoch 46, change: 0.00393699\n",
      "Epoch 47, change: 0.00369952\n",
      "Epoch 48, change: 0.00354413\n",
      "Epoch 49, change: 0.00337593\n",
      "Epoch 50, change: 0.00315679\n",
      "Epoch 51, change: 0.00301569\n",
      "Epoch 52, change: 0.00286396\n",
      "Epoch 53, change: 0.00269390\n",
      "Epoch 54, change: 0.00253386\n",
      "Epoch 55, change: 0.00239560\n",
      "Epoch 56, change: 0.00229626\n",
      "Epoch 57, change: 0.00216442\n",
      "Epoch 58, change: 0.00210664\n",
      "Epoch 59, change: 0.00196723\n",
      "Epoch 60, change: 0.00188216\n",
      "Epoch 61, change: 0.00176603\n",
      "Epoch 62, change: 0.00168966\n",
      "Epoch 63, change: 0.00160482\n",
      "Epoch 64, change: 0.00150581\n",
      "Epoch 65, change: 0.00142330\n",
      "Epoch 66, change: 0.00137909\n",
      "Epoch 67, change: 0.00130422\n",
      "Epoch 68, change: 0.00123192\n",
      "Epoch 69, change: 0.00117768\n",
      "Epoch 70, change: 0.00110979\n",
      "Epoch 71, change: 0.00106201\n",
      "Epoch 72, change: 0.00101574\n",
      "Epoch 73, change: 0.00097941\n",
      "Epoch 74, change: 0.00094149\n",
      "Epoch 75, change: 0.00091741\n",
      "Epoch 76, change: 0.00087748\n",
      "Epoch 77, change: 0.00084688\n",
      "Epoch 78, change: 0.00081627\n",
      "Epoch 79, change: 0.00079680\n",
      "Epoch 80, change: 0.00077174\n",
      "Epoch 81, change: 0.00073900\n",
      "Epoch 82, change: 0.00071762\n",
      "Epoch 83, change: 0.00069243\n",
      "Epoch 84, change: 0.00066748\n",
      "Epoch 85, change: 0.00064383\n",
      "Epoch 86, change: 0.00062323\n",
      "Epoch 87, change: 0.00060432\n",
      "Epoch 88, change: 0.00058128\n",
      "Epoch 89, change: 0.00056253\n",
      "Epoch 90, change: 0.00054936\n",
      "Epoch 91, change: 0.00052710\n",
      "Epoch 92, change: 0.00050945\n",
      "Epoch 93, change: 0.00049359\n",
      "Epoch 94, change: 0.00047888\n",
      "Epoch 95, change: 0.00046055\n",
      "Epoch 96, change: 0.00044598\n",
      "Epoch 97, change: 0.00042888\n",
      "Epoch 98, change: 0.00041524\n",
      "Epoch 99, change: 0.00040552\n",
      "Epoch 100, change: 0.00038671\n",
      "Epoch 101, change: 0.00037412\n",
      "Epoch 102, change: 0.00036396\n",
      "Epoch 103, change: 0.00035228\n",
      "Epoch 104, change: 0.00034153\n",
      "Epoch 105, change: 0.00032989\n",
      "Epoch 106, change: 0.00031724\n",
      "Epoch 107, change: 0.00030669\n",
      "Epoch 108, change: 0.00029810\n",
      "Epoch 109, change: 0.00028583\n",
      "Epoch 110, change: 0.00027630\n",
      "Epoch 111, change: 0.00026914\n",
      "Epoch 112, change: 0.00026225\n",
      "Epoch 113, change: 0.00025099\n",
      "Epoch 114, change: 0.00024223\n",
      "Epoch 115, change: 0.00023606\n",
      "Epoch 116, change: 0.00022827\n",
      "Epoch 117, change: 0.00021941\n",
      "Epoch 118, change: 0.00021273\n",
      "Epoch 119, change: 0.00020589\n",
      "Epoch 120, change: 0.00019999\n",
      "Epoch 121, change: 0.00019219\n",
      "Epoch 122, change: 0.00018630\n",
      "Epoch 123, change: 0.00017977\n",
      "Epoch 124, change: 0.00017507\n",
      "Epoch 125, change: 0.00016815\n",
      "Epoch 126, change: 0.00016254\n",
      "Epoch 127, change: 0.00015760\n",
      "Epoch 128, change: 0.00015296\n",
      "Epoch 1201469179\n",
      "Epoch 19, change: 0.01434753\n",
      "Epoch 20, change: 0.01446081\n",
      "Epoch 21, change: 0.01355930\n",
      "Epoch 22, change: 0.01375598\n",
      "Epoch 23, change: 0.01338598\n",
      "Epoch 24, change: 0.01345789\n",
      "Epoch 25, change: 0.01275266\n",
      "Epoch 26, change: 0.01270923\n",
      "Epoch 27, change: 0.01292506\n",
      "Epoch 28, change: 0.01236634\n",
      "Epoch 29, change: 0.01237464\n",
      "Epoch 30, change: 0.01229205\n",
      "Epoch 31, change: 0.01191542\n",
      "Epoch 32, change: 0.01183629\n",
      "Epoch 33, change: 0.01170252\n",
      "Epoch 34, change: 0.01124049\n",
      "Epoch 35, change: 0.01119662\n",
      "Epoch 36, change: 0.01108310\n",
      "Epoch 37, change: 0.01087041\n",
      "Epoch 38, change: 0.01050147\n",
      "Epoch 39, change: 0.01057209\n",
      "Epoch 40, change: 0.01059565\n",
      "Epoch 41, change: 0.01031840\n",
      "Epoch 42, change: 0.00988928\n",
      "Epoch 43, change: 0.00994496\n",
      "Epoch 44, change: 0.00972066\n",
      "Epoch 45, change: 0.00951786\n",
      "Epoch 46, change: 0.00914715\n",
      "Epoch 47, change: 0.00910235\n",
      "Epoch 48, change: 0.00881936\n",
      "Epoch 49, change: 0.00856356\n",
      "Epoch 50, change: 0.00834075\n",
      "Epoch 51, change: 0.00827749\n",
      "Epoch 52, change: 0.00807734\n",
      "Epoch 53, change: 0.00785916\n",
      "Epoch 54, change: 0.00782342\n",
      "Epoch 55, change: 0.00777606\n",
      "Epoch 56, change: 0.00763888\n",
      "Epoch 57, change: 0.00772891\n",
      "Epoch 58, change: 0.00755932\n",
      "Epoch 59, change: 0.00748480\n",
      "Epoch 60, change: 0.00743692\n",
      "Epoch 61, change: 0.00738774\n",
      "Epoch 62, change: 0.00737396\n",
      "Epoch 63, change: 0.00735655\n",
      "Epoch 64, change: 0.00717734\n",
      "Epoch 65, change: 0.00721889\n",
      "Epoch 66, change: 0.00711072\n",
      "Epoch 67, change: 0.00709145\n",
      "Epoch 68, change: 0.00709393\n",
      "Epoch 69, change: 0.00699828\n",
      "Epoch 70, change: 0.00699265\n",
      "Epoch 71, change: 0.00688348\n",
      "Epoch 72, change: 0.00689018\n",
      "Epoch 73, change: 0.00685476\n",
      "Epoch 74, change: 0.00681674\n",
      "Epoch 75, change: 0.00672130\n",
      "Epoch 76, change: 0.00672512\n",
      "Epoch 77, change: 0.00667514\n",
      "Epoch 78, change: 0.00658240\n",
      "Epoch 79, change: 0.00664036\n",
      "Epoch 80, change: 0.00650064\n",
      "Epoch 81, change: 0.00658054\n",
      "Epoch 82, change: 0.00648272\n",
      "Epoch 83, change: 0.00643337\n",
      "Epoch 84, change: 0.00638452\n",
      "Epoch 85, change: 0.00639177\n",
      "Epoch 86, change: 0.00630401\n",
      "Epoch 87, change: 0.00630148\n",
      "Epoch 88, change: 0.00630451\n",
      "Epoch 89, change: 0.00623428\n",
      "Epoch 90, change: 0.00618182\n",
      "Epoch 91, change: 0.00617688\n",
      "Epoch 92, change: 0.00613951\n",
      "Epoch 93, change: 0.00611711\n",
      "Epoch 94, change: 0.00605591\n",
      "Epoch 95, change: 0.00604498\n",
      "Epoch 96, change: 0.00601661\n",
      "Epoch 97, change: 0.00598098\n",
      "Epoch 98, change: 0.00593721\n",
      "Epoch 99, change: 0.00592146\n",
      "Epoch 100, change: 0.00592340\n",
      "Epoch 101, change: 0.00585695\n",
      "Epoch 102, change: 0.00583790\n",
      "Epoch 103, change: 0.00580639\n",
      "Epoch 104, change: 0.00578081\n",
      "Epoch 105, change: 0.00579237\n",
      "Epoch 106, change: 0.00569742\n",
      "Epoch 107, change: 0.00570971\n",
      "Epoch 108, change: 0.00568429\n",
      "Epoch 109, change: 0.00565451\n",
      "Epoch 110, change: 0.00565735\n",
      "Epoch 111, change: 0.00559896\n",
      "Epoch 112, change: 0.00558183\n",
      "Epoch 113, change: 0.00555801\n",
      "Epoch 114, change: 0.00555002\n",
      "Epoch 115, change: 0.00549077\n",
      "Epoch 116, change: 0.00547466\n",
      "Epoch 117, change: 0.00545911\n",
      "Epoch 118, change: 0.00542805\n",
      "Epoch 119, change: 0.00543263\n",
      "Epoch 120, change: 0.00539961\n",
      "Epoch 121, change: 0.00537223\n",
      "Epoch 122, change: 0.00536059\n",
      "Epoch 123, change: 0.00531000\n",
      "Epoch 124, change: 0.00533953\n",
      "Epoch 125, change: 0.00526685\n",
      "Epoch 126, change: 0.00527279\n",
      "Epoch 127, change: 0.00525515\n",
      "Epoch 128, change: 0.00523459\n",
      "Epoch 129, change: 0.00520166\n",
      "Epoch 130, change: 0.00518453\n",
      "Epoch 131, change: 0.00516904\n",
      "Epoch 132, change: 0.00514540\n",
      "Epoch 133, change: 0.00514247\n",
      "Epoch 134, change: 0.00509713\n",
      "Epoch 135, change: 0.00509670\n",
      "Epoch 136, change: 0.00508872\n",
      "Epoch 137, change: 0.00504136\n",
      "Epoch 138, change: 0.00503996\n",
      "Epoch 139, change: 0.00501975\n",
      "Epoch 140, change: 0.00500411\n",
      "Epoch 141, change: 0.00498926\n",
      "Epoch 142, change: 0.00497908\n",
      "Epoch 143, change: 0.00494164\n",
      "Epoch 144, change: 0.00492871\n",
      "Epoch 145, change: 0.00491312\n",
      "Epoch 146, change: 0.00490728\n",
      "Epoch 147, change: 0.00488884\n",
      "Epoch 148, change: 0.00487915\n",
      "Epoch 149, change: 0.00483536\n",
      "Epoch 150, change: 0.00484627\n",
      "Epoch 151, change: 0.00482555\n",
      "Epoch 152, change: 0.00483225\n",
      "Epoch 153, change: 0.00477263\n",
      "Epoch 154, change: 0.00477166\n",
      "Epoch 155, change: 0.00475541\n",
      "Epoch 156, change: 0.00475339\n",
      "Epoch 157, change: 0.00475880.00595595\n",
      "Epoch 81, change: 0.00602653\n",
      "Epoch 82, change: 0.00593230\n",
      "Epoch 83, change: 0.00587977\n",
      "Epoch 84, change: 0.00585111\n",
      "Epoch 85, change: 0.00585383\n",
      "Epoch 86, change: 0.00577220\n",
      "Epoch 87, change: 0.00576713\n",
      "Epoch 88, change: 0.00576917\n",
      "Epoch 89, change: 0.00569744\n",
      "Epoch 90, change: 0.00565591\n",
      "Epoch 91, change: 0.00565229\n",
      "Epoch 92, change: 0.00561675\n",
      "Epoch 93, change: 0.00559240\n",
      "Epoch 94, change: 0.00553995\n",
      "Epoch 95, change: 0.00552319\n",
      "Epoch 96, change: 0.00550526\n",
      "Epoch 97, change: 0.00546267\n",
      "Epoch 98, change: 0.00542869\n",
      "Epoch 99, change: 0.00541184\n",
      "Epoch 100, change: 0.00540892\n",
      "Epoch 101, change: 0.00534291\n",
      "Epoch 102, change: 0.00532957\n",
      "Epoch 103, change: 0.00529873\n",
      "Epoch 104, change: 0.00527726\n",
      "Epoch 105, change: 0.00528537\n",
      "Epoch 106, change: 0.00519622\n",
      "Epoch 107, change: 0.00520323\n",
      "Epoch 108, change: 0.00517504\n",
      "Epoch 109, change: 0.00514358\n",
      "Epoch 110, change: 0.00514511\n",
      "Epoch 111, change: 0.00510919\n",
      "Epoch 112, change: 0.00508865\n",
      "Epoch 113, change: 0.00505584\n",
      "Epoch 114, change: 0.00505620\n",
      "Epoch 115, change: 0.00499193\n",
      "Epoch 116, change: 0.00498706\n",
      "Epoch 117, change: 0.00496314\n",
      "Epoch 118, change: 0.00494049\n",
      "Epoch 119, change: 0.00493610\n",
      "Epoch 120, change: 0.00490526\n",
      "Epoch 121, change: 0.00487506\n",
      "Epoch 122, change: 0.00486799\n",
      "Epoch 123, change: 0.00482583\n",
      "Epoch 124, change: 0.00484620\n",
      "Epoch 125, change: 0.00477054\n",
      "Epoch 126, change: 0.00477988\n",
      "Epoch 127, change: 0.00477163\n",
      "Epoch 128, change: 0.00474432\n",
      "Epoch 129, change: 0.00470922\n",
      "Epoch 130, change: 0.00469801\n",
      "Epoch 131, change: 0.00468821\n",
      "Epoch 132, change: 0.00465493\n",
      "Epoch 133, change: 0.00465395\n",
      "Epoch 134, change: 0.00460753\n",
      "Epoch 135, change: 0.00461024\n",
      "Epoch 136, change: 0.00459928\n",
      "Epoch 137, change: 0.00455504\n",
      "Epoch 138, change: 0.00455298\n",
      "Epoch 139, change: 0.00453274\n",
      "Epoch 140, change: 0.00451917\n",
      "Epoch 141, change: 0.00450155\n",
      "Epoch 142, change: 0.00449211\n",
      "Epoch 143, change: 0.00444924\n",
      "Epoch 144, change: 0.00444605\n",
      "Epoch 145, change: 0.00442590\n",
      "Epoch 146, change: 0.00441466\n",
      "Epoch 147, change: 0.00439934\n",
      "Epoch 148, change: 0.00438972\n",
      "Epoch 149, change: 0.00434456\n",
      "Epoch 150, change: 0.00436016\n",
      "Epoch 151, change: 0.00433516\n",
      "Epoch 152, change: 0.00433398\n",
      "Epoch 153, change: 0.00428903\n",
      "Epoch 154, change: 0.00428134\n",
      "Epoch 155, change: 0.00426372\n",
      "Epoch 156, change: 0.00426421\n",
      "Epoch 157, change: 0.00426567\n",
      "Epoch 158, change: 0.00420120\n",
      "Epoch 159, change: 0.00421840\n",
      "Epoch 160, change: 0.00419460\n",
      "Epoch 161, change: 0.00418694\n",
      "Epoch 162, change: 0.00416011\n",
      "Epoch 163, change: 0.00415587\n",
      "Epoch 164, change: 0.00413805\n",
      "Epoch 165, change: 0.00413565\n",
      "Epoch 166, change: 0.00410618\n",
      "Epoch 167, change: 0.00409686\n",
      "Epoch 168, change: 0.00407921\n",
      "Epoch 169, change: 0.00407286\n",
      "Epoch 170, change: 0.00404243\n",
      "Epoch 171, change: 0.00404299\n",
      "Epoch 172, change: 0.00404120\n",
      "Epoch 173, change: 0.00401680\n",
      "Epoch 174, change: 0.00400511\n",
      "Epoch 175, change: 0.00398103\n",
      "Epoch 176, change: 0.00398080\n",
      "Epoch 177, change: 0.00397423\n",
      "Epoch 178, change: 0.00394643\n",
      "Epoch 179, change: 0.00394143\n",
      "Epoch 180, change: 0.00394633\n",
      "Epoch 181, change: 0.00389659\n",
      "Epoch 182, change: 0.00390562\n",
      "Epoch 183, change: 0.00388880\n",
      "Epoch 184, change: 0.00388561\n",
      "Epoch 185, change: 0.00386658\n",
      "Epoch 186, change: 0.00385392\n",
      "Epoch 187, change: 0.00384911\n",
      "Epoch 188, change: 0.00383762\n",
      "Epoch 189, change: 0.00382521\n",
      "Epoch 190, change: 0.00380429\n",
      "Epoch 191, change: 0.00378704\n",
      "Epoch 192, change: 0.00379886\n",
      "Epoch 193, change: 0.00376836\n",
      "Epoch 194, change: 0.00375523\n",
      "Epoch 195, change: 0.00375498\n",
      "Epoch 196, change: 0.00374484\n",
      "Epoch 197, change: 0.00372842\n",
      "Epoch 198, change: 0.00372249\n",
      "Epoch 199, change: 0.00370522\n",
      "Epoch 200, change: 0.00369639\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27383064\n",
      "Epoch 3, change: 0.12447582\n",
      "Epoch 4, change: 0.07959562\n",
      "Epoch 5, change: 0.05059700\n",
      "Epoch 6, change: 0.05447367\n",
      "Epoch 7, change: 0.05184782\n",
      "Epoch 8, change: 0.04588687\n",
      "Epoch 9, change: 0.03546751\n",
      "Epoch 10, change: 0.03719931\n",
      "Epoch 11, change: 0.02958532\n",
      "Epoch 12, change: 0.03116529\n",
      "Epoch 13, change: 0.03080377\n",
      "Epoch 14, change: 0.02750639\n",
      "Epoch 15, change: 0.02643935\n",
      "Epoch 16, change: 0.02507620\n",
      "Epoch 17, change: 0.02475532\n",
      "Epoch 18, change: 0.0.00662555\n",
      "Epoch 81, change: 0.00653748\n",
      "Epoch 82, change: 0.00651202\n",
      "Epoch 83, change: 0.00631145\n",
      "Epoch 84, change: 0.00636375\n",
      "Epoch 85, change: 0.00614638\n",
      "Epoch 86, change: 0.00604719\n",
      "Epoch 87, change: 0.00601894\n",
      "Epoch 88, change: 0.00589498\n",
      "Epoch 89, change: 0.00584707\n",
      "Epoch 90, change: 0.00571595\n",
      "Epoch 91, change: 0.00566637\n",
      "Epoch 92, change: 0.00563839\n",
      "Epoch 93, change: 0.00551902\n",
      "Epoch 94, change: 0.00544847\n",
      "Epoch 95, change: 0.00535922\n",
      "Epoch 96, change: 0.00531461\n",
      "Epoch 97, change: 0.00525687\n",
      "Epoch 98, change: 0.00512679\n",
      "Epoch 99, change: 0.00513118\n",
      "Epoch 100, change: 0.00504704\n",
      "Epoch 101, change: 0.00498515\n",
      "Epoch 102, change: 0.00488866\n",
      "Epoch 103, change: 0.00484161\n",
      "Epoch 104, change: 0.00478230\n",
      "Epoch 105, change: 0.00472206\n",
      "Epoch 106, change: 0.00468394\n",
      "Epoch 107, change: 0.00457583\n",
      "Epoch 108, change: 0.00455936\n",
      "Epoch 109, change: 0.00446790\n",
      "Epoch 110, change: 0.00443837\n",
      "Epoch 111, change: 0.00436498\n",
      "Epoch 112, change: 0.00433931\n",
      "Epoch 113, change: 0.00425294\n",
      "Epoch 114, change: 0.00425151\n",
      "Epoch 115, change: 0.00418621\n",
      "Epoch 116, change: 0.00412717\n",
      "Epoch 117, change: 0.00407852\n",
      "Epoch 118, change: 0.00398774\n",
      "Epoch 119, change: 0.00398674\n",
      "Epoch 120, change: 0.00392273\n",
      "Epoch 121, change: 0.00390848\n",
      "Epoch 122, change: 0.00384102\n",
      "Epoch 123, change: 0.00380867\n",
      "Epoch 124, change: 0.00374285\n",
      "Epoch 125, change: 0.00373531\n",
      "Epoch 126, change: 0.00365748\n",
      "Epoch 127, change: 0.00363896\n",
      "Epoch 128, change: 0.00359317\n",
      "Epoch 129, change: 0.00354901\n",
      "Epoch 130, change: 0.00351371\n",
      "Epoch 131, change: 0.00346717\n",
      "Epoch 132, change: 0.00342754\n",
      "Epoch 133, change: 0.00342808\n",
      "Epoch 134, change: 0.00337905\n",
      "Epoch 135, change: 0.00332272\n",
      "Epoch 136, change: 0.00329857\n",
      "Epoch 137, change: 0.00327383\n",
      "Epoch 138, change: 0.00320296\n",
      "Epoch 139, change: 0.00319655\n",
      "Epoch 140, change: 0.00317398\n",
      "Epoch 141, change: 0.00309846\n",
      "Epoch 142, change: 0.00310183\n",
      "Epoch 143, change: 0.00306905\n",
      "Epoch 144, change: 0.00299965\n",
      "Epoch 145, change: 0.00299006\n",
      "Epoch 146, change: 0.00298367\n",
      "Epoch 147, change: 0.00293713\n",
      "Epoch 148, change: 0.00290356\n",
      "Epoch 149, change: 0.00287921\n",
      "Epoch 150, change: 0.00284839\n",
      "Epoch 151, change: 0.00281469\n",
      "Epoch 152, change: 0.00279946\n",
      "Epoch 153, change: 0.00276742\n",
      "Epoch 154, change: 0.00272869\n",
      "Epoch 155, change: 0.00271714\n",
      "Epoch 156, change: 0.00267432\n",
      "Epoch 157, change: 0.00264049\n",
      "Epoch 158, change: 0.00263114\n",
      "Epoch 159, change: 0.00261114\n",
      "Epoch 160, change: 0.00260109\n",
      "Epoch 161, change: 0.00259617\n",
      "Epoch 162, change: 0.00258440\n",
      "Epoch 163, change: 0.00255087\n",
      "Epoch 164, change: 0.00259361\n",
      "Epoch 165, change: 0.00254757\n",
      "Epoch 166, change: 0.00254648\n",
      "Epoch 167, change: 0.00253361\n",
      "Epoch 168, change: 0.00252782\n",
      "Epoch 169, change: 0.00253185\n",
      "Epoch 170, change: 0.00251525\n",
      "Epoch 171, change: 0.00250977\n",
      "Epoch 172, change: 0.00251402\n",
      "Epoch 173, change: 0.00247910\n",
      "Epoch 174, change: 0.00248402\n",
      "Epoch 175, change: 0.00247801\n",
      "Epoch 176, change: 0.00246540\n",
      "Epoch 177, change: 0.00247077\n",
      "Epoch 178, change: 0.00245666\n",
      "Epoch 179, change: 0.00244238\n",
      "Epoch 180, change: 0.00245018\n",
      "Epoch 181, change: 0.00243454\n",
      "Epoch 182, change: 0.00242139\n",
      "Epoch 183, change: 0.00242322\n",
      "Epoch 184, change: 0.00241247\n",
      "Epoch 185, change: 0.00240830\n",
      "Epoch 186, change: 0.00240805\n",
      "Epoch 187, change: 0.00239240\n",
      "Epoch 188, change: 0.00239024\n",
      "Epoch 189, change: 0.00237273\n",
      "Epoch 190, change: 0.00238282\n",
      "Epoch 191, change: 0.00237181\n",
      "Epoch 192, change: 0.00235502\n",
      "Epoch 193, change: 0.00236058\n",
      "Epoch 194, change: 0.00234738\n",
      "Epoch 195, change: 0.00234850\n",
      "Epoch 196, change: 0.00233508\n",
      "Epoch 197, change: 0.00233428\n",
      "Epoch 198, change: 0.00230946\n",
      "Epoch 199, change: 0.00234200\n",
      "Epoch 200, change: 0.00231297\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27517742\n",
      "Epoch 3, change: 0.14767862\n",
      "Epoch 4, change: 0.07043110\n",
      "Epoch 5, change: 0.05149986\n",
      "Epoch 6, change: 0.04508114\n",
      "Epoch 7, change: 0.04037601\n",
      "Epoch 8, change: 0.03416220\n",
      "Epoch 9, change: 0.02940053\n",
      "Epoch 10, change: 0.02808252\n",
      "Epoch 11, change: 0.02583519\n",
      "Epoch 12, change: 0.02502242\n",
      "Epoch 13, change: 0.02240895\n",
      "Epoch 14, change: 0.02225729\n",
      "Epoch 15, change: 0.02106882\n",
      "Epoch 16, change: 0.02056849\n",
      "Epoch 17, change: 0.01996087\n",
      "Epoch 18, change: 0.0\n",
      "Epoch 158, change: 0.00468781\n",
      "Epoch 159, change: 0.00471938\n",
      "Epoch 160, change: 0.00469355\n",
      "Epoch 161, change: 0.00467788\n",
      "Epoch 162, change: 0.00465562\n",
      "Epoch 163, change: 0.00465698\n",
      "Epoch 164, change: 0.00462427\n",
      "Epoch 165, change: 0.00463134\n",
      "Epoch 166, change: 0.00460765\n",
      "Epoch 167, change: 0.00459762\n",
      "Epoch 168, change: 0.00458580\n",
      "Epoch 169, change: 0.00456133\n",
      "Epoch 170, change: 0.00454325\n",
      "Epoch 171, change: 0.00453807\n",
      "Epoch 172, change: 0.00454032\n",
      "Epoch 173, change: 0.00451531\n",
      "Epoch 174, change: 0.00449996\n",
      "Epoch 175, change: 0.00448878\n",
      "Epoch 176, change: 0.00447992\n",
      "Epoch 177, change: 0.00447422\n",
      "Epoch 178, change: 0.00445626\n",
      "Epoch 179, change: 0.00444376\n",
      "Epoch 180, change: 0.00444859\n",
      "Epoch 181, change: 0.00439513\n",
      "Epoch 182, change: 0.00441324\n",
      "Epoch 183, change: 0.00439055\n",
      "Epoch 184, change: 0.00439027\n",
      "Epoch 185, change: 0.00437670\n",
      "Epoch 186, change: 0.00435411\n",
      "Epoch 187, change: 0.00436015\n",
      "Epoch 188, change: 0.00434621\n",
      "Epoch 189, change: 0.00433302\n",
      "Epoch 190, change: 0.00431773\n",
      "Epoch 191, change: 0.00429501\n",
      "Epoch 192, change: 0.00431047\n",
      "Epoch 193, change: 0.00428186\n",
      "Epoch 194, change: 0.00427043\n",
      "Epoch 195, change: 0.00426729\n",
      "Epoch 196, change: 0.00425587\n",
      "Epoch 197, change: 0.00424728\n",
      "Epoch 198, change: 0.00424115\n",
      "Epoch 199, change: 0.00422030\n",
      "Epoch 200, change: 0.00421531\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.21875995\n",
      "Epoch 3, change: 0.11898138\n",
      "Epoch 4, change: 0.06443170\n",
      "Epoch 5, change: 0.04352827\n",
      "Epoch 6, change: 0.03695263\n",
      "Epoch 7, change: 0.03273297\n",
      "Epoch 8, change: 0.02754956\n",
      "Epoch 9, change: 0.02451485\n",
      "Epoch 10, change: 0.02316904\n",
      "Epoch 11, change: 0.02175020\n",
      "Epoch 12, change: 0.02062338\n",
      "Epoch 13, change: 0.01841731\n",
      "Epoch 14, change: 0.01797541\n",
      "Epoch 15, change: 0.01702557\n",
      "Epoch 16, change: 0.01574031\n",
      "Epoch 17, change: 0.01530237\n",
      "Epoch 18, change: 0.01469179\n",
      "Epoch 19, change: 0.01434753\n",
      "Epoch 20, change: 0.01446081\n",
      "Epoch 21, change: 0.01355930\n",
      "Epoch 22, change: 0.01375598\n",
      "Epoch 23, change: 0.01338598\n",
      "Epoch 24, change: 0.01345789\n",
      "Epoch 25, change: 0.01275266\n",
      "Epoch 26, change: 0.01270923\n",
      "Epoch 27, change: 0.01292506\n",
      "Epoch 28, change: 0.01236634\n",
      "Epoch 29, change: 0.01237464\n",
      "Epoch 30, change: 0.01229205\n",
      "Epoch 31, change: 0.01191542\n",
      "Epoch 32, change: 0.01183629\n",
      "Epoch 33, change: 0.01170252\n",
      "Epoch 34, change: 0.01124049\n",
      "Epoch 35, change: 0.01119662\n",
      "Epoch 36, change: 0.01108310\n",
      "Epoch 37, change: 0.01087041\n",
      "Epoch 38, change: 0.01050147\n",
      "Epoch 39, change: 0.01057209\n",
      "Epoch 40, change: 0.01059565\n",
      "Epoch 41, change: 0.01031840\n",
      "Epoch 42, change: 0.00988928\n",
      "Epoch 43, change: 0.00994496\n",
      "Epoch 44, change: 0.00972066\n",
      "Epoch 45, change: 0.00951786\n",
      "Epoch 46, change: 0.00914715\n",
      "Epoch 47, change: 0.00910235\n",
      "Epoch 48, change: 0.00881936\n",
      "Epoch 49, change: 0.00856356\n",
      "Epoch 50, change: 0.00834075\n",
      "Epoch 51, change: 0.00827749\n",
      "Epoch 52, change: 0.00807734\n",
      "Epoch 53, change: 0.00785916\n",
      "Epoch 54, change: 0.00782342\n",
      "Epoch 55, change: 0.00777606\n",
      "Epoch 56, change: 0.00763888\n",
      "Epoch 57, change: 0.00772891\n",
      "Epoch 58, change: 0.00755932\n",
      "Epoch 59, change: 0.00748480\n",
      "Epoch 60, change: 0.00743692\n",
      "Epoch 61, change: 0.00738774\n",
      "Epoch 62, change: 0.00737396\n",
      "Epoch 63, change: 0.00735655\n",
      "Epoch 64, change: 0.00717734\n",
      "Epoch 65, change: 0.00721889\n",
      "Epoch 66, change: 0.00711072\n",
      "Epoch 67, change: 0.00709145\n",
      "Epoch 68, change: 0.00709393\n",
      "Epoch 69, change: 0.00699828\n",
      "Epoch 70, change: 0.00699265\n",
      "Epoch 71, change: 0.00688348\n",
      "Epoch 72, change: 0.00689018\n",
      "Epoch 73, change: 0.00685476\n",
      "Epoch 74, change: 0.00681674\n",
      "Epoch 75, change: 0.00672130\n",
      "Epoch 76, change: 0.00672512\n",
      "Epoch 77, change: 0.00667514\n",
      "Epoch 78, change: 0.00658240\n",
      "Epoch 79, change: 0.00664036\n",
      "Epoch 80, change: 0.00650064\n",
      "Epoch 81, change: 0.00658054\n",
      "Epoch 82, change: 0.00648272\n",
      "Epoch 83, change: 0.00643337\n",
      "Epoch 84, change: 0.00638452\n",
      "Epoch 85, change: 0.00639177\n",
      "Epoch 86, change: 0.00630401\n",
      "Epoch 87, change: 0.00630148\n",
      "Epoch 88, change: 0.00630451\n",
      "Epoch 89, change: 0.00623428\n",
      "Epoch 90, change: 0.00618182\n",
      "Epoch 91, change: 0.00617688\n",
      "Epoch 92, change: 0.00613951\n",
      "Epoch 93, change: 0.00611711\n",
      "Epoch 94, change: 0.00605591\n",
      "Epoch 95, change: 0.00604498\n",
      "Epoch 96, change: 0.00601661\n",
      "Epoch 97, change: 0.00598098\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.55225562\n",
      "Epoch 3, change: 0.08038715\n",
      "Epoch 4, change: 0.12003780\n",
      "Epoch 5, change: 0.08063309\n",
      "Epoch 6, change: 0.06107163\n",
      "Epoch 7, change: 0.04277094\n",
      "Epoch 8, change: 0.02518906\n",
      "Epoch 9, change: 0.03471958\n",
      "Epoch 10, change: 0.01798661\n",
      "Epoch 11, change: 0.01752516\n",
      "Epoch 12, change: 0.01436915\n",
      "Epoch 13, change: 0.00726959\n",
      "Epoch 14, change: 0.00650984\n",
      "Epoch 15, change: 0.00386050\n",
      "Epoch 16, change: 0.00716855\n",
      "Epoch 17, change: 0.00319739\n",
      "Epoch 18, change: 0.00204038\n",
      "Epoch 19, change: 0.00209005\n",
      "Epoch 20, change: 0.00102811\n",
      "Epoch 21, change: 0.00136590\n",
      "Epoch 22, change: 0.00112352\n",
      "Epoch 23, change: 0.00104458\n",
      "Epoch 24, change: 0.00104844\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20295687\n",
      "Epoch 3, change: 0.09264634\n",
      "Epoch 4, change: 0.07380557\n",
      "Epoch 5, change: 0.05665352\n",
      "Epoch 6, change: 0.05338349\n",
      "Epoch 7, change: 0.05614548\n",
      "Epoch 8, change: 0.04484489\n",
      "Epoch 9, change: 0.03943951\n",
      "Epoch 10, change: 0.03767334\n",
      "Epoch 11, change: 0.03421809\n",
      "Epoch 12, change: 0.03303615\n",
      "Epoch 13, change: 0.03204106\n",
      "Epoch 14, change: 0.02969738\n",
      "Epoch 15, change: 0.02873853\n",
      "Epoch 16, change: 0.02768540\n",
      "Epoch 17, change: 0.02635140\n",
      "Epoch 18, change: 0.02576330\n",
      "Epoch 19, change: 0.02473032\n",
      "Epoch 20, change: 0.02405104\n",
      "Epoch 21, change: 0.02300967\n",
      "Epoch 22, change: 0.02241350\n",
      "Epoch 23, change: 0.02176457\n",
      "Epoch 24, change: 0.02120698\n",
      "Epoch 25, change: 0.02062708\n",
      "Epoch 26, change: 0.02018086\n",
      "Epoch 27, change: 0.01968843\n",
      "Epoch 28, change: 0.01932085\n",
      "Epoch 29, change: 0.01905815\n",
      "Epoch 30, change: 0.01849845\n",
      "Epoch 31, change: 0.01826524\n",
      "Epoch 32, change: 0.01795850\n",
      "Epoch 33, change: 0.01755703\n",
      "Epoch 34, change: 0.01735033\n",
      "Epoch 35, change: 0.01700493\n",
      "Epoch 36, change: 0.01681203\n",
      "Epoch 37, change: 0.01660684\n",
      "Epoch 38, change: 0.01620382\n",
      "Epoch 39, change: 0.01577093\n",
      "Epoch 40, change: 0.01537994\n",
      "Epoch 41, change: 0.01504393\n",
      "Epoch 42, change: 0.01465939\n",
      "Epoch 43, change: 0.01430982\n",
      "Epoch 44, change: 0.01401203\n",
      "Epoch 45, change: 0.01370159\n",
      "Epoch 46, change: 0.01335524\n",
      "Epoch 47, change: 0.01312786\n",
      "Epoch 48, change: 0.01286974\n",
      "Epoch 49, change: 0.01263401\n",
      "Epoch 50, change: 0.01232462\n",
      "Epoch 51, change: 0.01217579\n",
      "Epoch 52, change: 0.01189816\n",
      "Epoch 53, change: 0.01166442\n",
      "Epoch 54, change: 0.01147348\n",
      "Epoch 55, change: 0.01128238\n",
      "Epoch 56, change: 0.01113475\n",
      "Epoch 57, change: 0.01092087\n",
      "Epoch 58, change: 0.01069563\n",
      "Epoch 59, change: 0.01058525\n",
      "Epoch 60, change: 0.01036802\n",
      "Epoch 61, change: 0.01022970\n",
      "Epoch 62, change: 0.01007485\n",
      "Epoch 63, change: 0.00996383\n",
      "Epoch 64, change: 0.00978307\n",
      "Epoch 65, change: 0.00960402\n",
      "Epoch 66, change: 0.00954268\n",
      "Epoch 67, change: 0.00933238\n",
      "Epoch 68, change: 0.00924473\n",
      "Epoch 69, change: 0.00909739\n",
      "Epoch 70, change: 0.00899284\n",
      "Epoch 71, change: 0.00887024\n",
      "Epoch 72, change: 0.00875418\n",
      "Epoch 73, change: 0.00868548\n",
      "Epoch 74, change: 0.00847562\n",
      "Epoch 75, change: 0.00842734\n",
      "Epoch 76, change: 0.00834389\n",
      "Epoch 77, change: 0.00819276\n",
      "Epoch 78, change: 0.00811328\n",
      "Epoch 79, change: 0.00802053\n",
      "Epoch 80, change: 0.00793522\n",
      "Epoch 81, change: 0.00779330\n",
      "Epoch 82, change: 0.00771794\n",
      "Epoch 83, change: 0.00766783\n",
      "Epoch 84, change: 0.00757554\n",
      "Epoch 85, change: 0.00748929\n",
      "Epoch 86, change: 0.00738537\n",
      "Epoch 87, change: 0.00732556\n",
      "Epoch 88, change: 0.00722356\n",
      "Epoch 89, change: 0.00713772\n",
      "Epoch 90, change: 0.00711051\n",
      "Epoch 91, change: 0.00701825\n",
      "Epoch 92, change: 0.00692576\n",
      "Epoch 93, change: 0.00687588\n",
      "Epoch 94, change: 0.00678660\n",
      "Epoch 95, change: 0.00673554\n",
      "Epoch 96, change: 0.00667782\n",
      "Epoch 97, change: 0.00660110\n",
      "Epoch 98, change: 0.00653623\n",
      "Epoch 99, change: 0.00647129\n",
      "Epoch 100, change: 0.00641238\n",
      "Epoch 101, change: 0.00636691\n",
      "Epoch 102, change: 0.00627212\n",
      "Epoch 103, change: 0.00624150\n",
      "Epoch 104, change: 0.00616327\n",
      "Epoch 105, change: 0.00610365\n",
      "Epoch 106, change: 0.00607437\n",
      "Epoch 107, change: 0.00600350\n",
      "Epoch 108, change: 0.00593604\n",
      "Epoch 109, change: 0.00589280\n",
      "Epoch 110, change: 0.00584257\n",
      "Epoch 111, change: 0.00579630\n",
      "Epoch 112, change: 0.00573579\n",
      "Epoch 113, change: 0.00570235\n",
      "Epoch 114, change: 0.00564303\n",
      "Epoch 115, change: 0.00559374\n",
      "Epoch 116, change: 0.00556254\n",
      "Epoch 117, change: 0.00549990\n",
      "Epoch 101915652\n",
      "Epoch 19, change: 0.01880030\n",
      "Epoch 20, change: 0.01812712\n",
      "Epoch 21, change: 0.01720976\n",
      "Epoch 22, change: 0.01731401\n",
      "Epoch 23, change: 0.01669386\n",
      "Epoch 24, change: 0.01628648\n",
      "Epoch 25, change: 0.01566274\n",
      "Epoch 26, change: 0.01562426\n",
      "Epoch 27, change: 0.01542762\n",
      "Epoch 28, change: 0.01511022\n",
      "Epoch 29, change: 0.01487745\n",
      "Epoch 30, change: 0.01442925\n",
      "Epoch 31, change: 0.01453712\n",
      "Epoch 32, change: 0.01431708\n",
      "Epoch 33, change: 0.01379923\n",
      "Epoch 34, change: 0.01367109\n",
      "Epoch 35, change: 0.01343692\n",
      "Epoch 36, change: 0.01332153\n",
      "Epoch 37, change: 0.01287991\n",
      "Epoch 38, change: 0.01283145\n",
      "Epoch 39, change: 0.01278524\n",
      "Epoch 40, change: 0.01236676\n",
      "Epoch 41, change: 0.01246652\n",
      "Epoch 42, change: 0.01217262\n",
      "Epoch 43, change: 0.01189978\n",
      "Epoch 44, change: 0.01173311\n",
      "Epoch 45, change: 0.01174459\n",
      "Epoch 46, change: 0.01148600\n",
      "Epoch 47, change: 0.01126665\n",
      "Epoch 48, change: 0.01116111\n",
      "Epoch 49, change: 0.01097656\n",
      "Epoch 50, change: 0.01080792\n",
      "Epoch 51, change: 0.01078481\n",
      "Epoch 52, change: 0.01066580\n",
      "Epoch 53, change: 0.01032447\n",
      "Epoch 54, change: 0.01034736\n",
      "Epoch 55, change: 0.01007021\n",
      "Epoch 56, change: 0.00992475\n",
      "Epoch 57, change: 0.00981119\n",
      "Epoch 58, change: 0.00952775\n",
      "Epoch 59, change: 0.00938657\n",
      "Epoch 60, change: 0.00925769\n",
      "Epoch 61, change: 0.00904349\n",
      "Epoch 62, change: 0.00890657\n",
      "Epoch 63, change: 0.00868808\n",
      "Epoch 64, change: 0.00868251\n",
      "Epoch 65, change: 0.00839723\n",
      "Epoch 66, change: 0.00835304\n",
      "Epoch 67, change: 0.00812795\n",
      "Epoch 68, change: 0.00802869\n",
      "Epoch 69, change: 0.00788943\n",
      "Epoch 70, change: 0.00775101\n",
      "Epoch 71, change: 0.00760885\n",
      "Epoch 72, change: 0.00761669\n",
      "Epoch 73, change: 0.00735215\n",
      "Epoch 74, change: 0.00726688\n",
      "Epoch 75, change: 0.00724655\n",
      "Epoch 76, change: 0.00707486\n",
      "Epoch 77, change: 0.00699980\n",
      "Epoch 78, change: 0.00679925\n",
      "Epoch 79, change: 0.00677072\n",
      "Epoch 80, change: 0.00662555\n",
      "Epoch 81, change: 0.00653748\n",
      "Epoch 82, change: 0.00651202\n",
      "Epoch 83, change: 0.00631145\n",
      "Epoch 84, change: 0.00636375\n",
      "Epoch 85, change: 0.00614638\n",
      "Epoch 86, change: 0.00604719\n",
      "Epoch 87, change: 0.00601894\n",
      "Epoch 88, change: 0.00589498\n",
      "Epoch 89, change: 0.00584707\n",
      "Epoch 90, change: 0.00571595\n",
      "Epoch 91, change: 0.00566637\n",
      "Epoch 92, change: 0.00563839\n",
      "Epoch 93, change: 0.00551902\n",
      "Epoch 94, change: 0.00544847\n",
      "Epoch 95, change: 0.00535922\n",
      "Epoch 96, change: 0.00531461\n",
      "Epoch 97, change: 0.00525687\n",
      "Epoch 98, change: 0.00512679\n",
      "Epoch 99, change: 0.00513118\n",
      "Epoch 100, change: 0.00504704\n",
      "Epoch 101, change: 0.00498515\n",
      "Epoch 102, change: 0.00488866\n",
      "Epoch 103, change: 0.00484161\n",
      "Epoch 104, change: 0.00478230\n",
      "Epoch 105, change: 0.00472206\n",
      "Epoch 106, change: 0.00468394\n",
      "Epoch 107, change: 0.00457583\n",
      "Epoch 108, change: 0.00455936\n",
      "Epoch 109, change: 0.00446790\n",
      "Epoch 110, change: 0.00443837\n",
      "Epoch 111, change: 0.00436498\n",
      "Epoch 112, change: 0.00433931\n",
      "Epoch 113, change: 0.00425294\n",
      "Epoch 114, change: 0.00425151\n",
      "Epoch 115, change: 0.00418621\n",
      "Epoch 116, change: 0.00412717\n",
      "Epoch 117, change: 0.00407852\n",
      "Epoch 118, change: 0.00398774\n",
      "Epoch 119, change: 0.00398674\n",
      "Epoch 120, change: 0.00392273\n",
      "Epoch 121, change: 0.00390848\n",
      "Epoch 122, change: 0.00384102\n",
      "Epoch 123, change: 0.00380867\n",
      "Epoch 124, change: 0.00374285\n",
      "Epoch 125, change: 0.00373531\n",
      "Epoch 126, change: 0.00365748\n",
      "Epoch 127, change: 0.00363896\n",
      "Epoch 128, change: 0.00359317\n",
      "Epoch 129, change: 0.00354901\n",
      "Epoch 130, change: 0.00351371\n",
      "Epoch 131, change: 0.00346717\n",
      "Epoch 132, change: 0.00342754\n",
      "Epoch 133, change: 0.00342808\n",
      "Epoch 134, change: 0.00337905\n",
      "Epoch 135, change: 0.00332272\n",
      "Epoch 136, change: 0.00329857\n",
      "Epoch 137, change: 0.00327383\n",
      "Epoch 138, change: 0.00320296\n",
      "Epoch 139, change: 0.00319655\n",
      "Epoch 140, change: 0.00317398\n",
      "Epoch 141, change: 0.00309846\n",
      "Epoch 142, change: 0.00310183\n",
      "Epoch 143, change: 0.00306905\n",
      "Epoch 144, change: 0.00299965\n",
      "Epoch 145, change: 0.00299006\n",
      "Epoch 146, change: 0.00298367\n",
      "Epoch 147, change: 0.00293713\n",
      "Epoch 148, change: 0.00290356\n",
      "Epoch 149, change: 0.00287921\n",
      "Epoch 150, change: 0.00284839\n",
      "Epoch 151, change: 0.00281469\n",
      "Epoch 152, change: 0.00279946\n",
      "Epoch 153, change: 0.00276742\n",
      "Epoch 154, change: 0.00272869\n",
      "Epoch 155, change: 0.00271714\n",
      "Epoch 156, change: 0.00267432\n",
      "Epoch 157, change: 0.0026404Saved model to /lustre_scratch/orlando-code/datasets/model_params/best_models/maxent_0-2500d_tuned.pickle.\n",
      "maxent_0-2500d_tuned metadata saved to /lustre_scratch/orlando-code/datasets/model_params/best_models/maxent_0-2500d_tuned_metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=70, min_samples_leaf=1, min_samples_split=10, n_estimators=1600; total time=   3.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   1.0s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=1, min_samples_split=5, n_estimators=1400; total time=   3.3s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.8s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=sag, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=newton-cholesky, tol=0.0001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=none, solver=saga, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=none, solver=saga, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=sag, tol=0.01, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=sag, tol=0.01, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=elasticnet, solver=saga, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=100, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.0001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=1, min_samples_split=2, n_estimators=1600; total time=   3.8s\n",
      "[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=70, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time=   3.4s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=1, min_samples_split=5, n_estimators=1400; total time=   3.1s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=multinomial, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=100, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.0001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=100, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.0001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=2, min_samples_split=2, n_estimators=1800; total time=   4.1s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=2000; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=   2.5s\n",
      "[CV] END bootstrap=True, max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.7s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=saga, tol=0.0001, verbose=2, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=newton-cholesky, tol=0.0001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=none, solver=saga, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=none, solver=sag, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=sag, tol=0.01, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=500, multi_class=ovr, penalty=none, solver=sag, tol=0.0001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class=auto, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class=auto, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=auto, penalty=l1, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=1, min_samples_split=10, n_estimators=1400; total time=   3.1s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time=   3.4s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=2, min_samples_split=2, n_estimators=1600; total time=   3.7s\n",
      "[CV] END bootstrap=False, max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   1.8s\n",
      "convergence after 29 epochs took 0 seconds\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=ovr, penalty=l1, solver=saga, tol=0.001, verbose=1, warm_start=False; total time=   0.1s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=500, multi_class=ovr, penalty=l2, solver=sag, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=500, multi_class=ovr, penalty=l2, solver=sag, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.22295701\n",
      "Epoch 3, change: 0.15214785\n",
      "Epoch 4, change: 0.07871537\n",
      "Epoch 5, change: 0.01755463\n",
      "Epoch 6, change: 0.04711574\n",
      "Epoch 7, change: 0.01784232\n",
      "Epoch 8, change: 0.02658353\n",
      "Epoch 9, change: 0.01436901\n",
      "Epoch 10, change: 0.01422669\n",
      "Epoch 11, change: 0.01327202\n",
      "Epoch 12, change: 0.01181861\n",
      "Epoch 13, change: 0.00991990\n",
      "Epoch 14, change: 0.00823474\n",
      "Epoch 15, change: 0.00739954\n",
      "Epoch 16, change: 0.00599279\n",
      "Epoch 17, change: 0.00538712\n",
      "Epoch 18, change: 0.00508602\n",
      "Epoch 19, change: 0.00408344\n",
      "Epoch 20, change: 0.00318924\n",
      "Epoch 21, change: 0.00293784\n",
      "Epoch 22, change: 0.00255854\n",
      "Epoch 23, change: 0.00222379\n",
      "Epoch 24, change: 0.00191604\n",
      "Epoch 25, change: 0.00167724\n",
      "Epoch 26, change: 0.00135367\n",
      "Epoch 27, change: 0.00126920\n",
      "Epoch 28, change: 0.00106974\n",
      "[CV] END bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=1, min_samples_split=10, n_estimators=1400; total time=   3.2s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=2000; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=   2.6s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.6s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=saga, tol=0.0001, verbose=2, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.01, verbose=1, warm_start=True; total time=   0.0s\n",
      "Newton iter=1\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.027268774080123317 <= 0.001\n",
      "Newton iter=2\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.00869733157513386 <= 0.001\n",
      "Newton iter=3\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.0016227359023570534 <= 0.001\n",
      "Newton iter=4\n",
      "  Check Convergence\n",
      "    1. max |gradient| 8.342205125148167e-05 <= 0.001\n",
      "    2. Newton decrement 0.00013561884818352822 <= 0.001\n",
      "  Solver did converge at loss = 0.36150501013745245.\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=200, multi_class=auto, penalty=l2, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=auto, penalty=l1, solver=sag, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=auto, penalty=l1, solver=sag, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "Newton iter=1\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -0.1353630496370417 <= -0.00012758559143951338 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.00869680305758936 <= 0.0001\n",
      "Newton iter=2\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -0.0011913493444800638 <= -1.1315588521148224e-06 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.00039888308696271746 <= 0.0001\n",
      "Newton iter=3\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -2.390596228418751e-06 <= -2.3309719722471645e-09 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 9.373611598289394e-07 <= 0.0001\n",
      "    2. Newton decrement 2.3869152995810965e-06 <= 0.0001\n",
      "  Solver did converge at loss = 0.5565903909821953.\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=ovr, penalty=l2, solver=newton-cholesky, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=l2, solver=saga, tol=0.0001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1600; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   0.8s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=2, min_samples_split=5, n_estimators=1600; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=sag, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.01, verbose=1, warm_start=True; total time=   0.0s\n",
      "Newton iter=1\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.01959979932720498 <= 0.001\n",
      "Newton iter=2\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.005675152867237612 <= 0.001\n",
      "Newton iter=3\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.0009056774704686869 <= 0.001\n",
      "    2. Newton decrement 0.002378685491864227 <= 0.001\n",
      "Newton iter=4\n",
      "  Check Convergence\n",
      "    1. max |gradient| 3.0461912043731193e-05 <= 0.001\n",
      "    2. Newton decrement 7.376033637500763e-05 <= 0.001\n",
      "  Solver did converge at loss = 0.4376214542724082.\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=200, multi_class=auto, penalty=l2, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "Newton iter=1\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.018192109693605713 <= 0.001\n",
      "Newton iter=2\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.004786090542432682 <= 0.001\n",
      "Newton iter=3\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.0007279480705153389 <= 0.001\n",
      "    2. Newton decrement 0.0030988641601856857 <= 0.001\n",
      "Newton iter=4\n",
      "  Check Convergence\n",
      "    1. max |gradient| 6.0979055449504566e-05 <= 0.001\n",
      "    2. Newton decrement 0.00016565266861327636 <= 0.001\n",
      "  Solver did converge at loss = 0.4359454604516716.\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=200, multi_class=auto, penalty=l2, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "Newton iter=1\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -0.11115585093089586 <= -0.00010474704954674598 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.0061155739849821835 <= 0.0001\n",
      "Newton iter=2\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -0.0007993336319405131 <= -7.680689671213474e-07 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.0001691869114155871 <= 0.0001\n",
      "Newton iter=3\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -5.102066016338114e-07 <= -4.980130200873103e-10 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 1.3198541236325578e-07 <= 0.0001\n",
      "    2. Newton decrement 5.099653325694058e-07 <= 0.0001\n",
      "  Solver did converge at loss = 0.5811914857905072.\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=ovr, penalty=l2, solver=newton-cholesky, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=l2, solver=saga, tol=0.0001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=1400; total time=   3.1s\n",
      "[CV] END bootstrap=False, max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=1200; total time=   2.8s\n",
      "[CV] END bootstrap=True, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   1.2s\n",
      "[CV] END bootstrap=False, max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=2000; total time=   4.8s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=auto, penalty=l2, solver=sag, tol=0.01, verbose=0, warm_start=False; total time=   0.1s\n",
      "convergence after 7 epochs took 0 seconds\n",
      "[CV] END C=0.1, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=ovr, penalty=l2, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "convergence after 7 epochs took 0 seconds\n",
      "[CV] END C=0.1, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=ovr, penalty=l2, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=ovr, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=l1, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=ovr, penalty=elasticnet, solver=newton-cholesky, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=l2, solver=saga, tol=0.0001, verbose=1, warm_start=False; total time=   0.0s\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.15188743\n",
      "Epoch 3, change: 0.08108586\n",
      "Epoch 4, change: 0.03148417\n",
      "Epoch 5, change: 0.02689285\n",
      "Epoch 6, change: 0.01975685\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.32928035\n",
      "Epoch 3, change: 0.04014290\n",
      "Epoch 4, change: 0.04356912\n",
      "Epoch 5, change: 0.04570425\n",
      "Epoch 6, change: 0.03316503\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=2, min_samples_split=2, n_estimators=1800; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   2.1s\n",
      "[CV] END bootstrap=False, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.8s\n",
      "[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=60, min_samples_leaf=2, min_samples_split=10, n_estimators=1800; total time=   3.9s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=saga, tol=0.0001, verbose=2, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=multinomial, penalty=none, solver=newton-cholesky, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.01, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=multinomial, penalty=elasticnet, solver=saga, tol=0.01, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=100, multi_class=multinomial, penalty=l2, solver=saga, tol=0.001, verbose=0, warm_start=True; total time=   0.0s\n",
      "convergence after 143 epochs took 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=auto, penalty=l2, solver=sag, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=ovr, penalty=elasticnet, solver=newton-cholesky, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=multinomial, penalty=elasticnet, solver=newton-cholesky, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "h 141, change: 0.00010565\n",
      "Epoch 142, change: 0.00010159\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=1, min_samples_split=2, n_estimators=1600; total time=   3.8s\n",
      "[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time=   2.4s\n",
      "[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=2000; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=10, n_estimators=1600; total time=   3.3s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=multinomial, penalty=none, solver=newton-cholesky, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.01, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=100, multi_class=multinomial, penalty=l2, solver=saga, tol=0.001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=100, multi_class=multinomial, penalty=l2, solver=saga, tol=0.001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=elasticnet, solver=saga, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=elasticnet, solver=saga, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=ovr, penalty=elasticnet, solver=newton-cholesky, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=multinomial, penalty=elasticnet, solver=newton-cholesky, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=90, min_samples_leaf=4, min_samples_split=2, n_estimators=1800; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=1400; total time=   3.2s\n",
      "[CV] END bootstrap=False, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=1, min_samples_split=5, n_estimators=1400; total time=   3.3s\n",
      "[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=   1.6s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=newton-cholesky, tol=0.0001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.01, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=multinomial, penalty=elasticnet, solver=saga, tol=0.01, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=multinomial, penalty=elasticnet, solver=saga, tol=0.01, verbose=0, warm_start=False; total time=   0.0s\n",
      "convergence after 153 epochs took 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=auto, penalty=l2, solver=sag, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "convergence after 141 epochs took 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=auto, penalty=l2, solver=sag, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=multinomial, penalty=elasticnet, solver=newton-cholesky, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "9, change: 0.00014788\n",
      "Epoch 130, change: 0.00014243\n",
      "Epoch 131, change: 0.00013807\n",
      "Epoch 132, change: 0.00013333\n",
      "Epoch 133, change: 0.00012909\n",
      "Epoch 134, change: 0.00012470\n",
      "Epoch 135, change: 0.00012066\n",
      "Epoch 136, change: 0.00011714\n",
      "Epoch 137, change: 0.00011343\n",
      "Epoch 138, change: 0.00010910\n",
      "Epoch 139, change: 0.00010595\n",
      "Epoch 140, change: 0.00010230\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=70, min_samples_leaf=1, min_samples_split=10, n_estimators=1600; total time=   3.5s\n",
      "[CV] END bootstrap=False, max_depth=60, min_samples_leaf=2, min_samples_split=10, n_estimators=1400; total time=   3.3s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=True, max_depth=60, min_samples_leaf=2, min_samples_split=10, n_estimators=1800; total time=   3.8s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=none, solver=newton-cholesky, tol=0.01, verbose=1, warm_start=True; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=l1, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.1s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "convergence after 18 epochs took 0 seconds\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=ovr, penalty=l2, solver=sag, tol=0.001, verbose=2, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=multinomial, penalty=none, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "h 141, change: 0.00443755\n",
      "Epoch 142, change: 0.00441625\n",
      "Epoch 143, change: 0.00436254\n",
      "Epoch 144, change: 0.00435310\n",
      "Epoch 145, change: 0.00431475\n",
      "Epoch 146, change: 0.00428942\n",
      "Epoch 147, change: 0.00425763\n",
      "Epoch 148, change: 0.00423942\n",
      "Epoch 149, change: 0.00420149\n",
      "Epoch 150, change: 0.00418534\n",
      "Epoch 151, change: 0.00414767\n",
      "Epoch 152, change: 0.00412110\n",
      "Epoch 153, change: 0.00409610\n",
      "Epoch 154, change: 0.00406221\n",
      "Epoch 155, change: 0.00404645\n",
      "Epoch 156, change: 0.00400404\n",
      "Epoch 157, change: 0.00398310\n",
      "Epoch 158, change: 0.00396477\n",
      "Epoch 159, change: 0.00394447\n",
      "Epoch 160, change: 0.00391717\n",
      "Epoch 161, change: 0.00389127\n",
      "Epoch 162, change: 0.00386270\n",
      "Epoch 163, change: 0.00384880\n",
      "Epoch 164, change: 0.00381063\n",
      "Epoch 165, change: 0.00371663\n",
      "Epoch 166, change: 0.00368236\n",
      "Epoch 167, change: 0.00364374\n",
      "Epoch 168, change: 0.00361227\n",
      "Epoch 169, change: 0.00358546\n",
      "Epoch 170, change: 0.00353784\n",
      "Epoch 171, change: 0.00352245\n",
      "Epoch 172, change: 0.00348543\n",
      "Epoch 173, change: 0.00346510\n",
      "Epoch 174, change: 0.00342776\n",
      "Epoch 175, change: 0.00339857\n",
      "Epoch 176, change: 0.00337865\n",
      "Epoch 177, change: 0.00335319\n",
      "Epoch 178, change: 0.00333029\n",
      "Epoch 179, change: 0.00329012\n",
      "Epoch 180, change: 0.00327292\n",
      "Epoch 181, change: 0.00324700\n",
      "Epoch 182, change: 0.00322390\n",
      "Epoch 183, change: 0.00320023\n",
      "Epoch 184, change: 0.00317780\n",
      "Epoch 185, change: 0.00314887\n",
      "Epoch 186, change: 0.00313608\n",
      "Epoch 187, change: 0.00310506\n",
      "Epoch 188, change: 0.00308586\n",
      "Epoch 189, change: 0.00306604\n",
      "Epoch 190, change: 0.00303583\n",
      "Epoch 191, change: 0.00302240\n",
      "Epoch 192, change: 0.00299526\n",
      "Epoch 193, change: 0.00298267\n",
      "Epoch 194, change: 0.00296161\n",
      "Epoch 195, change: 0.00294355\n",
      "Epoch 196, change: 0.00291720\n",
      "Epoch 197, change: 0.00290309\n",
      "Epoch 198, change: 0.00288961\n",
      "Epoch 199, change: 0.00286366\n",
      "Epoch 200, change: 0.00284488\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.52618474\n",
      "Epoch 3, change: 0.29228801\n",
      "Epoch 4, change: 0.16142353\n",
      "Epoch 5, change: 0.18346359\n",
      "Epoch 6, change: 0.07527164\n",
      "Epoch 7, change: 0.10368484\n",
      "Epoch 8, change: 0.02460194\n",
      "Epoch 9, change: 0.03956358\n",
      "Epoch 10, change: 0.04248749\n",
      "Epoch 11, change: 0.01972888\n",
      "Epoch 12, change: 0.01338342\n",
      "Epoch 13, change: 0.00419808\n",
      "Epoch 14, change: 0.00226736\n",
      "Epoch 15, change: 0.00806558\n",
      "Epoch 16, change: 0.00393473\n",
      "Epoch 17, change: 0.00317288\n",
      "[CV] END bootstrap=True, max_depth=90, min_samples_leaf=4, min_samples_split=2, n_estimators=1800; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.6s\n",
      "[CV] END bootstrap=False, max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=1200; total time=   2.6s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=2, min_samples_split=2, n_estimators=1600; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=10, n_estimators=1600; total time=   3.1s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=auto, penalty=l2, solver=sag, tol=0.01, verbose=0, warm_start=False; total time=   0.1s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=2.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=2.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "convergence after 22 epochs took 0 seconds\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=ovr, penalty=l2, solver=sag, tol=0.001, verbose=2, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=multinomial, penalty=none, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.17796132\n",
      "Epoch 3, change: 0.20052936\n",
      "Epoch 4, change: 0.12385268\n",
      "Epoch 5, change: 0.10679404\n",
      "Epoch 6, change: 0.13382610\n",
      "Epoch 7, change: 0.07899330\n",
      "Epoch 8, change: 0.04100290\n",
      "Epoch 9, change: 0.02918313\n",
      "Epoch 10, change: 0.02322500\n",
      "Epoch 11, change: 0.01673880\n",
      "Epoch 12, change: 0.01615768\n",
      "Epoch 13, change: 0.01712901\n",
      "Epoch 14, change: 0.01061453\n",
      "Epoch 15, change: 0.00694053\n",
      "Epoch 16, change: 0.00309060\n",
      "Epoch 17, change: 0.00504138\n",
      "Epoch 18, change: 0.00278719\n",
      "Epoch 19, change: 0.00218966\n",
      "Epoch 20, change: 0.00179638\n",
      "Epoch 21, change: 0.00135719\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=1, min_samples_split=2, n_estimators=1600; total time=   3.8s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.6s\n",
      "[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=2000; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   1.9s\n",
      "convergence after 13 epochs took 0 seconds\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=ovr, penalty=l1, solver=saga, tol=0.001, verbose=1, warm_start=False; total time=   0.1s\n",
      "convergence after 9 epochs took 0 seconds\n",
      "[CV] END C=0.1, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=ovr, penalty=l2, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=2.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=ovr, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=ovr, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "Newton iter=1\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -0.10128056680054132 <= -9.587128969103008e-05 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 0.0045108709414858235 <= 0.0001\n",
      "Newton iter=2\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -0.000498282944673023 <= -4.818557443405212e-07 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 7.949431061123578e-05 <= 0.0001\n",
      "    2. Newton decrement 0.0004934202822046938 <= 0.0001\n",
      "Newton iter=3\n",
      "  Backtracking Line Search\n",
      "    eps=10 * finfo.eps=3.552713678800501e-15\n",
      "    line search iteration=1, step size=1\n",
      "      check loss improvement <= armijo term: -1.148589043875603e-07 <= -1.1214733817012833e-10 True\n",
      "  Check Convergence\n",
      "    1. max |gradient| 2.457994839090427e-08 <= 0.0001\n",
      "    2. Newton decrement 1.1483887428621138e-07 <= 0.0001\n",
      "  Solver did converge at loss = 0.5913682159558264.\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=500, multi_class=ovr, penalty=l2, solver=newton-cholesky, tol=0.0001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=multinomial, penalty=none, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.21756103\n",
      "Epoch 3, change: 0.13482752\n",
      "Epoch 4, change: 0.07805540\n",
      "Epoch 5, change: 0.01861851\n",
      "Epoch 6, change: 0.03045836\n",
      "Epoch 7, change: 0.02657897\n",
      "Epoch 8, change: 0.00639245\n",
      "Epoch 9, change: 0.00568366\n",
      "Epoch 10, change: 0.00755041\n",
      "Epoch 11, change: 0.00152711\n",
      "Epoch 12, change: 0.00116625\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20046249\n",
      "Epoch 3, change: 0.03351309\n",
      "Epoch 4, change: 0.05596623\n",
      "Epoch 5, change: 0.03942292\n",
      "Epoch 6, change: 0.02887451\n",
      "Epoch 7, change: 0.02173723\n",
      "Epoch 8, change: 0.01452304\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=2, min_samples_split=2, n_estimators=1800; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=1400; total time=   3.3s\n",
      "[CV] END bootstrap=False, max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=1200; total time=   2.8s\n",
      "[CV] END bootstrap=True, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.4s\n",
      "[CV] END bootstrap=True, max_depth=60, min_samples_leaf=2, min_samples_split=10, n_estimators=1800; total time=   4.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=none, solver=newton-cholesky, tol=0.01, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.01, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=none, solver=sag, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=none, solver=sag, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=500, multi_class=ovr, penalty=none, solver=sag, tol=0.0001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=5.0, max_iter=500, multi_class=ovr, penalty=none, solver=sag, tol=0.0001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=1.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class=auto, penalty=elasticnet, solver=newton-cholesky, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=multinomial, penalty=none, solver=sag, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=saga, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20832138\n",
      "Epoch 3, change: 0.17876909\n",
      "Epoch 4, change: 0.11260684\n",
      "Epoch 5, change: 0.09805748\n",
      "Epoch 6, change: 0.06865841\n",
      "Epoch 7, change: 0.05010775\n",
      "Epoch 8, change: 0.04749935\n",
      "Epoch 9, change: 0.04484169\n",
      "Epoch 10, change: 0.04396122\n",
      "Epoch 11, change: 0.03703223\n",
      "Epoch 12, change: 0.03743425\n",
      "Epoch 13, change: 0.03716255\n",
      "Epoch 14, change: 0.03267384\n",
      "Epoch 15, change: 0.03255732\n",
      "Epoch 16, change: 0.03101801\n",
      "Epoch 17, change: 0.02947175\n",
      "Epoch 18, change: 0.02940917\n",
      "Epoch 19, change: 0.02793599\n",
      "Epoch 20, change: 0.02817279\n",
      "Epoch 21, change: 0.02701255\n",
      "Epoch 22, change: 0.02597441\n",
      "Epoch 23, change: 0.02477886\n",
      "Epoch 24, change: 0.02389526\n",
      "Epoch 25, change: 0.02284425\n",
      "Epoch 26, change: 0.02208045\n",
      "Epoch 27, change: 0.02129472\n",
      "Epoch 28, change: 0.02052022\n",
      "Epoch 29, change: 0.02002233\n",
      "Epoch 30, change: 0.01914059\n",
      "Epoch 31, change: 0.01883784\n",
      "Epoch 32, change: 0.01839626\n",
      "Epoch 33, change: 0.01779765\n",
      "Epoch 34, change: 0.01738579\n",
      "Epoch 35, change: 0.01682171\n",
      "Epoch 36, change: 0.01641157\n",
      "Epoch 37, change: 0.01598508\n",
      "Epoch 38, change: 0.01566682\n",
      "Epoch 39, change: 0.01524345\n",
      "Epoch 40, change: 0.01493046\n",
      "Epoch 41, change: 0.01462259\n",
      "Epoch 42, change: 0.01430647\n",
      "Epoch 43, change: 0.01394439\n",
      "Epoch 44, change: 0.01372314\n",
      "Epoch 45, change: 0.01332566\n",
      "Epoch 46, change: 0.01315282\n",
      "Epoch 47, change: 0.01292054\n",
      "Epoch 48, change: 0.01262815\n",
      "Epoch 49, change: 0.01243007\n",
      "Epoch 50, change: 0.01214232\n",
      "Epoch 51, change: 0.01198733\n",
      "Epoch 52, change: 0.01175698\n",
      "Epoch 53, change: 0.01150482\n",
      "Epoch 54, change: 0.01136606\n",
      "Epoch 55, change: 0.01114360\n",
      "Epoch 56, change: 0.01098770\n",
      "Epoch 57, change: 0.01079129\n",
      "Epoch 58, change: 0.01058801\n",
      "Epoch 59, change: 0.01045095\n",
      "Epoch 60, change: 0.01024034\n",
      "Epoch 61, change: 0.01009938\n",
      "Epoch 62, change: 0.00995690\n",
      "Epoch 63, change: 0.00984151\n",
      "Epoch 64, change: 0.00968563\n",
      "Epoch 65, change: 0.00948070\n",
      "Epoch 66, change: 0.00941415\n",
      "Epoch 67, change: 0.00920342\n",
      "Epoch 68, change: 0.00911326\n",
      "Epoch 69, change: 0.00897211\n",
      "Epoch 70, change: 0.00885456\n",
      "Epoch 71, change: 0.00872293\n",
      "Epoch 72, change: 0.00861666\n",
      "Epoch 73, change: 0.00854401\n",
      "Epoch 74, change: 0.00835975\n",
      "Epoch 75, change: 0.00826512\n",
      "Epoch 76, change: 0.00819116\n",
      "Epoch 77, change: 0.00805060\n",
      "Epoch 78, change: 0.00796929\n",
      "Epoch 79, change: 0.00786050\n",
      "Epoch 80, change: 0.00776160\n",
      "Epoch 81, change: 0.00763985\n",
      "Epoch 82, change: 0.00743087\n",
      "Epoch 83, change: 0.00730986\n",
      "Epoch 84, change: 0.00716006\n",
      "Epoch 85, change: 0.00701956\n",
      "Epoch 86, change: 0.00691711\n",
      "Epoch 87, change: 0.00680864\n",
      "Epoch 88, change: 0.00667384\n",
      "Epoch 89, change: 0.00654983\n",
      "Epoch 90, change: 0.00652547\n",
      "Epoch 91, change: 0.00641152\n",
      "Epoch 92, change: 0.00627751\n",
      "Epoch 93, change: 0.00622591\n",
      "Epoch 94, change: 0.00609561\n",
      "Epoch 95, change: 0.00604511\n",
      "Epoch 96, change: 0.00596356\n",
      "Epoch 97, change: 0.00587612\n",
      "Epoch 98, change: 0.00579691\n",
      "Epoch 99, change: 0.00571758\n",
      "Epoch 100, change: 0.00565741\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=70, min_samples_leaf=1, min_samples_split=10, n_estimators=1600; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=60, min_samples_leaf=2, min_samples_split=10, n_estimators=1400; total time=   3.1s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=2, min_samples_split=5, n_estimators=1600; total time=   3.5s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=   1.7s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=none, solver=newton-cholesky, tol=0.01, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=multinomial, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=multinomial, penalty=elasticnet, solver=newton-cholesky, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=elasticnet, solver=saga, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=auto, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=multinomial, penalty=none, solver=sag, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=True, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=multinomial, penalty=none, solver=sag, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=saga, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.19697594\n",
      "Epoch 3, change: 0.07691763\n",
      "Epoch 4, change: 0.06097530\n",
      "Epoch 5, change: 0.05919548\n",
      "Epoch 6, change: 0.06077106\n",
      "Epoch 7, change: 0.04534073\n",
      "Epoch 8, change: 0.03593272\n",
      "Epoch 9, change: 0.03054622\n",
      "Epoch 10, change: 0.04287920\n",
      "Epoch 11, change: 0.02203636\n",
      "Epoch 12, change: 0.02562387\n",
      "Epoch 13, change: 0.02384128\n",
      "Epoch 14, change: 0.02137772\n",
      "Epoch 15, change: 0.02264890\n",
      "Epoch 16, change: 0.02131660\n",
      "Epoch 17, change: 0.02022302\n",
      "Epoch 18, change: 0.01824577\n",
      "Epoch 19, change: 0.01628315\n",
      "Epoch 20, change: 0.01592983\n",
      "Epoch 21, change: 0.01430495\n",
      "Epoch 22, change: 0.01496776\n",
      "Epoch 23, change: 0.01411803\n",
      "Epoch 24, change: 0.01367206\n",
      "Epoch 25, change: 0.01404626\n",
      "Epoch 26, change: 0.01368622\n",
      "Epoch 27, change: 0.01386695\n",
      "Epoch 28, change: 0.01349453\n",
      "Epoch 29, change: 0.01129759\n",
      "Epoch 30, change: 0.01039401\n",
      "Epoch 31, change: 0.01014492\n",
      "Epoch 32, change: 0.00930335\n",
      "Epoch 33, change: 0.00929099\n",
      "Epoch 34, change: 0.00821769\n",
      "Epoch 35, change: 0.00827123\n",
      "Epoch 36, change: 0.00783112\n",
      "Epoch 37, change: 0.00754323\n",
      "Epoch 38, change: 0.00685864\n",
      "Epoch 39, change: 0.00701608\n",
      "Epoch 40, change: 0.00674962\n",
      "Epoch 41, change: 0.00662123\n",
      "Epoch 42, change: 0.00662221\n",
      "Epoch 43, change: 0.00656018\n",
      "Epoch 44, change: 0.00663663\n",
      "Epoch 45, change: 0.00703476\n",
      "Epoch 46, change: 0.00712735\n",
      "Epoch 47, change: 0.00720127\n",
      "Epoch 48, change: 0.00705587\n",
      "Epoch 49, change: 0.00708494\n",
      "Epoch 50, change: 0.00719346\n",
      "Epoch 51, change: 0.00705868\n",
      "Epoch 52, change: 0.00715714\n",
      "Epoch 53, change: 0.00703879\n",
      "Epoch 54, change: 0.00700654\n",
      "Epoch 55, change: 0.00706099\n",
      "Epoch 56, change: 0.00697171\n",
      "Epoch 57, change: 0.00705622\n",
      "Epoch 58, change: 0.00692555\n",
      "Epoch 59, change: 0.00686029\n",
      "Epoch 60, change: 0.00687672\n",
      "Epoch 61, change: 0.00682566\n",
      "Epoch 62, change: 0.00685547\n",
      "Epoch 63, change: 0.00678514\n",
      "Epoch 64, change: 0.00673361\n",
      "Epoch 65, change: 0.00673629\n",
      "Epoch 66, change: 0.00667361\n",
      "Epoch 67, change: 0.00666211\n",
      "Epoch 68, change: 0.00668506\n",
      "Epoch 69, change: 0.00659988\n",
      "Epoch 70, change: 0.00664212\n",
      "Epoch 71, change: 0.00649907\n",
      "Epoch 72, change: 0.00654582\n",
      "Epoch 73, change: 0.00651147\n",
      "Epoch 74, change: 0.00649424\n",
      "Epoch 75, change: 0.00642372\n",
      "Epoch 76, change: 0.00644436\n",
      "Epoch 77, change: 0.00635523\n",
      "Epoch 78, change: 0.00632175\n",
      "Epoch 79, change: 0.00636521\n",
      "Epoch 80, change: 0.00626957\n",
      "Epoch 81, change: 0.00631842\n",
      "Epoch 82, change: 0.00625204\n",
      "Epoch 83, change: 0.00622398\n",
      "Epoch 84, change: 0.00617247\n",
      "Epoch 85, change: 0.00617765\n",
      "Epoch 86, change: 0.00612016\n",
      "Epoch 87, change: 0.00612340\n",
      "Epoch 88, change: 0.00611248\n",
      "Epoch 89, change: 0.00608650\n",
      "Epoch 90, change: 0.00601763\n",
      "Epoch 91, change: 0.00602033\n",
      "Epoch 92, change: 0.00599571\n",
      "Epoch 93, change: 0.00599081\n",
      "Epoch 94, change: 0.00591569\n",
      "Epoch 95, change: 0.00591747\n",
      "Epoch 96, change: 0.00589491\n",
      "Epoch 97, change: 0.00586719\n",
      "Epoch 98, change: 0.00580113\n",
      "Epoch 99, change: 0.00583684\n",
      "Epoch 100, change: 0.00582406\n",
      "[CV] END bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=80, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   2.2s\n",
      "[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   2.4s\n",
      "[CV] END bootstrap=True, max_depth=70, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=60, min_samples_leaf=2, min_samples_split=10, n_estimators=1400; total time=   3.3s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=2, min_samples_split=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=2000; total time=   4.5s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=auto, penalty=l2, solver=sag, tol=0.01, verbose=0, warm_start=False; total time=   0.1s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=none, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=none, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=saga, tol=0.001, verbose=2, warm_start=False; total time=   0.0s\n",
      "02356455\n",
      "Epoch 19, change: 0.02262669\n",
      "Epoch 20, change: 0.02138494\n",
      "Epoch 21, change: 0.02017870\n",
      "Epoch 22, change: 0.01810117\n",
      "Epoch 23, change: 0.01836915\n",
      "Epoch 24, change: 0.01758698\n",
      "Epoch 25, change: 0.01704775\n",
      "Epoch 26, change: 0.01656935\n",
      "Epoch 27, change: 0.01585393\n",
      "Epoch 28, change: 0.01575201\n",
      "Epoch 29, change: 0.01512099\n",
      "Epoch 30, change: 0.01460385\n",
      "Epoch 31, change: 0.01429611\n",
      "Epoch 32, change: 0.01362209\n",
      "Epoch 33, change: 0.01293120\n",
      "Epoch 34, change: 0.01296691\n",
      "Epoch 35, change: 0.01242819\n",
      "Epoch 36, change: 0.01227559\n",
      "Epoch 37, change: 0.01177572\n",
      "Epoch 38, change: 0.01137201\n",
      "Epoch 39, change: 0.01122048\n",
      "Epoch 40, change: 0.01074764\n",
      "Epoch 41, change: 0.01049835\n",
      "Epoch 42, change: 0.01032479\n",
      "Epoch 43, change: 0.00999805\n",
      "Epoch 44, change: 0.00958632\n",
      "Epoch 45, change: 0.00948223\n",
      "Epoch 46, change: 0.00922742\n",
      "Epoch 47, change: 0.00889898\n",
      "Epoch 48, change: 0.00876238\n",
      "Epoch 49, change: 0.00841256\n",
      "Epoch 50, change: 0.00821352\n",
      "Epoch 51, change: 0.00814758\n",
      "Epoch 52, change: 0.00801036\n",
      "Epoch 53, change: 0.00763573\n",
      "Epoch 54, change: 0.00760603\n",
      "Epoch 55, change: 0.00738797\n",
      "Epoch 56, change: 0.00717222\n",
      "Epoch 57, change: 0.00703732\n",
      "Epoch 58, change: 0.00686202\n",
      "Epoch 59, change: 0.00667676\n",
      "Epoch 60, change: 0.00657599\n",
      "Epoch 61, change: 0.00647417\n",
      "Epoch 62, change: 0.00622470\n",
      "Epoch 63, change: 0.00605874\n",
      "Epoch 64, change: 0.00610937\n",
      "Epoch 65, change: 0.00591090\n",
      "Epoch 66, change: 0.00573620\n",
      "Epoch 67, change: 0.00565683\n",
      "Epoch 68, change: 0.00551832\n",
      "Epoch 69, change: 0.00542431\n",
      "Epoch 70, change: 0.00530576\n",
      "Epoch 71, change: 0.00517908\n",
      "Epoch 72, change: 0.00518252\n",
      "Epoch 73, change: 0.00493329\n",
      "Epoch 74, change: 0.00490680\n",
      "Epoch 75, change: 0.00481868\n",
      "Epoch 76, change: 0.00473348\n",
      "Epoch 77, change: 0.00462327\n",
      "Epoch 78, change: 0.00451218\n",
      "Epoch 79, change: 0.00447187\n",
      "Epoch 80, change: 0.00435854\n",
      "Epoch 81, change: 0.00430641\n",
      "Epoch 82, change: 0.00420704\n",
      "Epoch 83, change: 0.00408400\n",
      "Epoch 84, change: 0.00409778\n",
      "Epoch 85, change: 0.00394189\n",
      "Epoch 86, change: 0.00391121\n",
      "Epoch 87, change: 0.00386495\n",
      "Epoch 88, change: 0.00370721\n",
      "Epoch 89, change: 0.00375968\n",
      "Epoch 90, change: 0.00373187\n",
      "Epoch 91, change: 0.00365730\n",
      "Epoch 92, change: 0.00360554\n",
      "Epoch 93, change: 0.00350724\n",
      "Epoch 94, change: 0.00340145\n",
      "Epoch 95, change: 0.00335361\n",
      "Epoch 96, change: 0.00330673\n",
      "Epoch 97, change: 0.00323827\n",
      "Epoch 98, change: 0.00313467\n",
      "Epoch 99, change: 0.00311824\n",
      "Epoch 100, change: 0.00300636\n",
      "[CV] END bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1600; total time=   3.5s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time=   2.3s\n",
      "[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=1, min_samples_split=10, n_estimators=600; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=2, min_samples_split=5, n_estimators=1600; total time=   3.7s\n",
      "[CV] END bootstrap=False, max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time=   1.7s\n",
      "convergence after 11 epochs took 0 seconds\n",
      "[CV] END C=0.1, dual=False, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=ovr, penalty=l1, solver=saga, tol=0.001, verbose=1, warm_start=False; total time=   0.1s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=none, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=500, multi_class=ovr, penalty=l2, solver=sag, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=0.1, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=auto, penalty=l1, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "278442\n",
      "Epoch 132, change: 0.00277309\n",
      "Epoch 133, change: 0.00277338\n",
      "Epoch 134, change: 0.00273421\n",
      "Epoch 135, change: 0.00275499\n",
      "Epoch 136, change: 0.00274806\n",
      "Epoch 137, change: 0.00274488\n",
      "Epoch 138, change: 0.00271045\n",
      "Epoch 139, change: 0.00273093\n",
      "Epoch 140, change: 0.00272268\n",
      "Epoch 141, change: 0.00269682\n",
      "Epoch 142, change: 0.00271053\n",
      "Epoch 143, change: 0.00270013\n",
      "Epoch 144, change: 0.00266891\n",
      "Epoch 145, change: 0.00266177\n",
      "Epoch 146, change: 0.00268537\n",
      "Epoch 147, change: 0.00267426\n",
      "Epoch 148, change: 0.00266661\n",
      "Epoch 149, change: 0.00264949\n",
      "Epoch 150, change: 0.00266375\n",
      "Epoch 151, change: 0.00262413\n",
      "Epoch 152, change: 0.00263415\n",
      "Epoch 153, change: 0.00263763\n",
      "Epoch 154, change: 0.00261817\n",
      "Epoch 155, change: 0.00262067\n",
      "Epoch 156, change: 0.00260473\n",
      "Epoch 157, change: 0.00260206\n",
      "Epoch 158, change: 0.00261234\n",
      "Epoch 159, change: 0.00259209\n",
      "Epoch 160, change: 0.00257654\n",
      "Epoch 161, change: 0.00259063\n",
      "Epoch 162, change: 0.00257613\n",
      "Epoch 163, change: 0.00254785\n",
      "Epoch 164, change: 0.00259805\n",
      "Epoch 165, change: 0.00255588\n",
      "Epoch 166, change: 0.00255130\n",
      "Epoch 167, change: 0.00254111\n",
      "Epoch 168, change: 0.00253486\n",
      "Epoch 169, change: 0.00255500\n",
      "Epoch 170, change: 0.00253016\n",
      "Epoch 171, change: 0.00253424\n",
      "Epoch 172, change: 0.00254063\n",
      "Epoch 173, change: 0.00251341\n",
      "Epoch 174, change: 0.00251496\n",
      "Epoch 175, change: 0.00251356\n",
      "Epoch 176, change: 0.00250987\n",
      "Epoch 177, change: 0.00250173\n",
      "Epoch 178, change: 0.00249334\n",
      "Epoch 179, change: 0.00248302\n",
      "Epoch 180, change: 0.00250493\n",
      "Epoch 181, change: 0.00247781\n",
      "Epoch 182, change: 0.00247386\n",
      "Epoch 183, change: 0.00247743\n",
      "Epoch 184, change: 0.00246855\n",
      "Epoch 185, change: 0.00246420\n",
      "Epoch 186, change: 0.00247284\n",
      "Epoch 187, change: 0.00245902\n",
      "Epoch 188, change: 0.00246134\n",
      "Epoch 189, change: 0.00244510\n",
      "Epoch 190, change: 0.00244484\n",
      "Epoch 191, change: 0.00244630\n",
      "Epoch 192, change: 0.00243338\n",
      "Epoch 193, change: 0.00243648\n",
      "Epoch 194, change: 0.00242301\n",
      "Epoch 195, change: 0.00242996\n",
      "Epoch 196, change: 0.00242405\n",
      "Epoch 197, change: 0.00241794\n",
      "Epoch 198, change: 0.00240374\n",
      "Epoch 199, change: 0.00242720\n",
      "Epoch 200, change: 0.00239729\n",
      "[CV] END bootstrap=True, max_depth=90, min_samples_leaf=4, min_samples_split=2, n_estimators=1800; total time=   3.9s\n",
      "[CV] END bootstrap=False, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=2000; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=   2.5s\n",
      "[CV] END bootstrap=True, max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=2000; total time=   4.7s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=True; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=200, multi_class=multinomial, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=1, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=multinomial, penalty=elasticnet, solver=newton-cholesky, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=multinomial, penalty=elasticnet, solver=newton-cholesky, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=auto, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=500, multi_class=auto, penalty=l1, solver=newton-cholesky, tol=0.001, verbose=0, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=l1, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=ovr, penalty=l1, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "convergence after 25 epochs took 0 seconds\n",
      "[CV] END C=1.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=200, multi_class=ovr, penalty=l2, solver=sag, tol=0.001, verbose=2, warm_start=True; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=none, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "18, change: 0.00545099\n",
      "Epoch 119, change: 0.00541581\n",
      "Epoch 120, change: 0.00536603\n",
      "Epoch 121, change: 0.00531664\n",
      "Epoch 122, change: 0.00528733\n",
      "Epoch 123, change: 0.00523440\n",
      "Epoch 124, change: 0.00519876\n",
      "Epoch 125, change: 0.00515619\n",
      "Epoch 126, change: 0.00511883\n",
      "Epoch 127, change: 0.00506834\n",
      "Epoch 128, change: 0.00503868\n",
      "Epoch 129, change: 0.00499716\n",
      "Epoch 130, change: 0.00496330\n",
      "Epoch 131, change: 0.00491510\n",
      "Epoch 132, change: 0.00489278\n",
      "Epoch 133, change: 0.00485325\n",
      "Epoch 134, change: 0.00482350\n",
      "Epoch 135, change: 0.00477314\n",
      "Epoch 136, change: 0.00474653\n",
      "Epoch 137, change: 0.00471126\n",
      "Epoch 138, change: 0.00467494\n",
      "Epoch 139, change: 0.00464155\n",
      "Epoch 140, change: 0.00461637\n",
      "Epoch 141, change: 0.00457044\n",
      "Epoch 142, change: 0.00455086\n",
      "Epoch 143, change: 0.00449326\n",
      "Epoch 144, change: 0.00448494\n",
      "Epoch 145, change: 0.00444510\n",
      "Epoch 146, change: 0.00441800\n",
      "Epoch 147, change: 0.00438442\n",
      "Epoch 148, change: 0.00436160\n",
      "Epoch 149, change: 0.00432712\n",
      "Epoch 150, change: 0.00430771\n",
      "Epoch 151, change: 0.00426735\n",
      "Epoch 152, change: 0.00424048\n",
      "Epoch 153, change: 0.00421541\n",
      "Epoch 154, change: 0.00418007\n",
      "Epoch 155, change: 0.00416476\n",
      "Epoch 156, change: 0.00412417\n",
      "Epoch 157, change: 0.00409790\n",
      "Epoch 158, change: 0.00407579\n",
      "Epoch 159, change: 0.00405734\n",
      "Epoch 160, change: 0.00402835\n",
      "Epoch 161, change: 0.00400142\n",
      "Epoch 162, change: 0.00397430\n",
      "Epoch 163, change: 0.00395718\n",
      "Epoch 164, change: 0.00392890\n",
      "Epoch 165, change: 0.00390401\n",
      "Epoch 166, change: 0.00387861\n",
      "Epoch 167, change: 0.00385282\n",
      "Epoch 168, change: 0.00383127\n",
      "Epoch 169, change: 0.00381150\n",
      "Epoch 170, change: 0.00378390\n",
      "Epoch 171, change: 0.00376551\n",
      "Epoch 172, change: 0.00374155\n",
      "Epoch 173, change: 0.00372195\n",
      "Epoch 174, change: 0.00369711\n",
      "Epoch 175, change: 0.00367356\n",
      "Epoch 176, change: 0.00365678\n",
      "Epoch 177, change: 0.00363577\n",
      "Epoch 178, change: 0.00361987\n",
      "Epoch 179, change: 0.00358616\n",
      "Epoch 180, change: 0.00357216\n",
      "Epoch 181, change: 0.00354894\n",
      "Epoch 182, change: 0.00352935\n",
      "Epoch 183, change: 0.00351714\n",
      "Epoch 184, change: 0.00348879\n",
      "Epoch 185, change: 0.00346944\n",
      "Epoch 186, change: 0.00345664\n",
      "Epoch 187, change: 0.00343107\n",
      "Epoch 188, change: 0.00341456\n",
      "Epoch 189, change: 0.00339642\n",
      "Epoch 190, change: 0.00337396\n",
      "Epoch 191, change: 0.00335831\n",
      "Epoch 192, change: 0.00333824\n",
      "Epoch 193, change: 0.00332716\n",
      "Epoch 194, change: 0.00330819\n",
      "Epoch 195, change: 0.00329205\n",
      "Epoch 196, change: 0.00326414\n",
      "Epoch 197, change: 0.00325443\n",
      "Epoch 198, change: 0.00324217\n",
      "Epoch 199, change: 0.00321771\n",
      "Epoch 200, change: 0.00320138\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=110, min_samples_leaf=1, min_samples_split=10, n_estimators=1400; total time=   3.0s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=1200; total time=   2.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time=   3.5s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=2, min_samples_split=2, n_estimators=1600; total time=   3.5s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.8s\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=False, intercept_scaling=1.0, max_iter=100, multi_class=multinomial, penalty=l1, solver=sag, tol=0.0001, verbose=0, warm_start=False; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=l1, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.1s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=none, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=none, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=none, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "Epoch 98, change: 0.00593721\n",
      "Epoch 99, change: 0.00592146\n",
      "Epoch 100, change: 0.00592340\n",
      "Epoch 101, change: 0.00585695\n",
      "Epoch 102, change: 0.00583790\n",
      "Epoch 103, change: 0.00580639\n",
      "Epoch 104, change: 0.00578081\n",
      "Epoch 105, change: 0.00579237\n",
      "Epoch 106, change: 0.00569742\n",
      "Epoch 107, change: 0.00570971\n",
      "Epoch 108, change: 0.00568429\n",
      "Epoch 109, change: 0.00565451\n",
      "Epoch 110, change: 0.00565735\n",
      "Epoch 111, change: 0.00559896\n",
      "Epoch 112, change: 0.00558183\n",
      "Epoch 113, change: 0.00555801\n",
      "Epoch 114, change: 0.00555002\n",
      "Epoch 115, change: 0.00549077\n",
      "Epoch 116, change: 0.00547466\n",
      "Epoch 117, change: 0.00545911\n",
      "Epoch 118, change: 0.00542805\n",
      "Epoch 119, change: 0.00543263\n",
      "Epoch 120, change: 0.00539961\n",
      "Epoch 121, change: 0.00537223\n",
      "Epoch 122, change: 0.00536059\n",
      "Epoch 123, change: 0.00531000\n",
      "Epoch 124, change: 0.00533953\n",
      "Epoch 125, change: 0.00526685\n",
      "Epoch 126, change: 0.00527279\n",
      "Epoch 127, change: 0.00525515\n",
      "Epoch 128, change: 0.00523459\n",
      "Epoch 129, change: 0.00520166\n",
      "Epoch 130, change: 0.00518453\n",
      "Epoch 131, change: 0.00516904\n",
      "Epoch 132, change: 0.00514540\n",
      "Epoch 133, change: 0.00514247\n",
      "Epoch 134, change: 0.00509713\n",
      "Epoch 135, change: 0.00509670\n",
      "Epoch 136, change: 0.00508872\n",
      "Epoch 137, change: 0.00504136\n",
      "Epoch 138, change: 0.00503996\n",
      "Epoch 139, change: 0.00501975\n",
      "Epoch 140, change: 0.00500411\n",
      "Epoch 141, change: 0.00498926\n",
      "Epoch 142, change: 0.00497908\n",
      "Epoch 143, change: 0.00494164\n",
      "Epoch 144, change: 0.00492871\n",
      "Epoch 145, change: 0.00491312\n",
      "Epoch 146, change: 0.00490728\n",
      "Epoch 147, change: 0.00488884\n",
      "Epoch 148, change: 0.00487915\n",
      "Epoch 149, change: 0.00483536\n",
      "Epoch 150, change: 0.00484627\n",
      "Epoch 151, change: 0.00482555\n",
      "Epoch 152, change: 0.00483225\n",
      "Epoch 153, change: 0.00477263\n",
      "Epoch 154, change: 0.00477166\n",
      "Epoch 155, change: 0.00475541\n",
      "Epoch 156, change: 0.00475339\n",
      "Epoch 157, change: 0.00475880\n",
      "Epoch 158, change: 0.00468781\n",
      "Epoch 159, change: 0.00471938\n",
      "Epoch 160, change: 0.00469355\n",
      "Epoch 161, change: 0.00467788\n",
      "Epoch 162, change: 0.00465562\n",
      "Epoch 163, change: 0.00465698\n",
      "Epoch 164, change: 0.00462427\n",
      "Epoch 165, change: 0.00463134\n",
      "Epoch 166, change: 0.00460765\n",
      "Epoch 167, change: 0.00459762\n",
      "Epoch 168, change: 0.00458580\n",
      "Epoch 169, change: 0.00456133\n",
      "Epoch 170, change: 0.00454325\n",
      "Epoch 171, change: 0.00453807\n",
      "Epoch 172, change: 0.00454032\n",
      "Epoch 173, change: 0.00451531\n",
      "Epoch 174, change: 0.00449996\n",
      "Epoch 175, change: 0.00448878\n",
      "Epoch 176, change: 0.00447992\n",
      "Epoch 177, change: 0.00447422\n",
      "Epoch 178, change: 0.00445626\n",
      "Epoch 179, change: 0.00444376\n",
      "Epoch 180, change: 0.00444859\n",
      "Epoch 181, change: 0.00439513\n",
      "Epoch 182, change: 0.00441324\n",
      "Epoch 183, change: 0.00439055\n",
      "Epoch 184, change: 0.00439027\n",
      "Epoch 185, change: 0.00437670\n",
      "Epoch 186, change: 0.00435411\n",
      "Epoch 187, change: 0.00436015\n",
      "Epoch 188, change: 0.00434621\n",
      "Epoch 189, change: 0.00433302\n",
      "Epoch 190, change: 0.00431773\n",
      "Epoch 191, change: 0.00429501\n",
      "Epoch 192, change: 0.00431047\n",
      "Epoch 193, change: 0.00428186\n",
      "Epoch 194, change: 0.00427043\n",
      "Epoch 195, change: 0.00426729\n",
      "Epoch 196, change: 0.00425587\n",
      "Epoch 197, change: 0.00424728\n",
      "Epoch 198, change: 0.00424115\n",
      "Epoch 199, change: 0.00422030\n",
      "Epoch 200, change: 0.00421531\n",
      "[CV] END bootstrap=False, max_depth=80, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1600; total time=   3.6s\n",
      "[CV] END bootstrap=False, max_depth=70, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=70, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=2000; total time=   4.7s\n",
      "[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=10, n_estimators=1600; total time=   3.4s\n",
      "[CV] END C=10.0, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=500, multi_class=multinomial, penalty=none, solver=newton-cholesky, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=1.0, max_iter=200, multi_class=auto, penalty=l1, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.1s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=10.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=none, solver=saga, tol=0.001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=True, fit_intercept=True, intercept_scaling=5.0, max_iter=100, multi_class=auto, penalty=l1, solver=sag, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "[CV] END C=0.1, dual=False, fit_intercept=False, intercept_scaling=2.0, max_iter=100, multi_class=auto, penalty=l1, solver=sag, tol=0.01, verbose=2, warm_start=False; total time=   0.0s\n",
      "max_iter reached after 0 seconds\n",
      "[CV] END C=1.0, dual=False, fit_intercept=False, intercept_scaling=5.0, max_iter=200, multi_class=ovr, penalty=none, solver=saga, tol=0.0001, verbose=1, warm_start=True; total time=   0.0s\n",
      "9\n",
      "Epoch 158, change: 0.00263114\n",
      "Epoch 159, change: 0.00261114\n",
      "Epoch 160, change: 0.00260109\n",
      "Epoch 161, change: 0.00259617\n",
      "Epoch 162, change: 0.00258440\n",
      "Epoch 163, change: 0.00255087\n",
      "Epoch 164, change: 0.00259361\n",
      "Epoch 165, change: 0.00254757\n",
      "Epoch 166, change: 0.00254648\n",
      "Epoch 167, change: 0.00253361\n",
      "Epoch 168, change: 0.00252782\n",
      "Epoch 169, change: 0.00253185\n",
      "Epoch 170, change: 0.00251525\n",
      "Epoch 171, change: 0.00250977\n",
      "Epoch 172, change: 0.00251402\n",
      "Epoch 173, change: 0.00247910\n",
      "Epoch 174, change: 0.00248402\n",
      "Epoch 175, change: 0.00247801\n",
      "Epoch 176, change: 0.00246540\n",
      "Epoch 177, change: 0.00247077\n",
      "Epoch 178, change: 0.00245666\n",
      "Epoch 179, change: 0.00244238\n",
      "Epoch 180, change: 0.00245018\n",
      "Epoch 181, change: 0.00243454\n",
      "Epoch 182, change: 0.00242139\n",
      "Epoch 183, change: 0.00242322\n",
      "Epoch 184, change: 0.00241247\n",
      "Epoch 185, change: 0.00240830\n",
      "Epoch 186, change: 0.00240805\n",
      "Epoch 187, change: 0.00239240\n",
      "Epoch 188, change: 0.00239024\n",
      "Epoch 189, change: 0.00237273\n",
      "Epoch 190, change: 0.00238282\n",
      "Epoch 191, change: 0.00237181\n",
      "Epoch 192, change: 0.00235502\n",
      "Epoch 193, change: 0.00236058\n",
      "Epoch 194, change: 0.00234738\n",
      "Epoch 195, change: 0.00234850\n",
      "Epoch 196, change: 0.00233508\n",
      "Epoch 197, change: 0.00233428\n",
      "Epoch 198, change: 0.00230946\n",
      "Epoch 199, change: 0.00234200\n",
      "Epoch 200, change: 0.00231297\n"
     ]
    }
   ],
   "source": [
    "# TODO: sort these\n",
    "\n",
    "def spatial_split_train_test(\n",
    "    xa_dss: list[xa.Dataset],\n",
    "    gt_label: str = \"gt\",\n",
    "    data_type: str = \"continuous\",\n",
    "    ignore_vars: list = [\"spatial_ref\", \"band\", \"depth\"],\n",
    "    split_type: str = \"pixel\",\n",
    "    test_fraction: float = 0.25,\n",
    "    train_test_lat_divide: int = -18,\n",
    "    train_direction: str = \"N\",\n",
    "    bath_mask: bool = True,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Split the input dataset into train and test sets based on spatial coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        xa_ds (xa.Dataset): The input xarray Dataset.\n",
    "        gt_label: The ground truth label.\n",
    "        ignore_vars (list): A list of variables to ignore during splitting. Default is\n",
    "            [\"time\", \"spatial_ref\", \"band\", \"depth\"].\n",
    "        split_type (str): The split type, either \"pixel\" or \"region\". Default is \"pixel\".\n",
    "        test_lats (tuple[float]): The latitude range for the test region. Required for \"region\" split type.\n",
    "            Default is None.\n",
    "        test_lons (tuple[float]): The longitude range for the test region. Required for \"region\" split type.\n",
    "            Default is None.\n",
    "        test_fraction (float): The fraction of data to be used for the test set. Default is 0.2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tuple: A tuple containing X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    # send input to list if not already\n",
    "    xa_dss = utils.cast_to_list(xa_dss)\n",
    "    # flatten datasets to pandas dataframes and process\n",
    "    flattened_data_dfs = baselines.xa_dss_to_df(xa_dss, bath_mask=bath_mask)\n",
    "    # generate training and testing coordinates\n",
    "    train_coords_list, test_coords_list = generate_test_train_coords_from_dfs(\n",
    "        flattened_data_dfs,\n",
    "        test_fraction=test_fraction,\n",
    "        split_type=split_type,\n",
    "        train_test_lat_divide=train_test_lat_divide,\n",
    "        train_direction=train_direction,\n",
    "    )\n",
    "\n",
    "    # normalise dataframe via min/max scaling\n",
    "    normalised_dfs = [\n",
    "        (flattened_data - flattened_data.min())\n",
    "        / (flattened_data.max() - flattened_data.min())\n",
    "        for flattened_data in flattened_data_dfs\n",
    "    ]\n",
    "\n",
    "    # train_rows, test_rows = [], []\n",
    "    X_trains, X_tests, y_trains, y_tests = [], [], [], []\n",
    "    for i in range(len(flattened_data_dfs)):\n",
    "        # return train and test data rows from dataframe\n",
    "        train_rows = utils.select_df_rows_by_coords(\n",
    "            normalised_dfs[i], train_coords_list[i]\n",
    "        )\n",
    "        test_rows = utils.select_df_rows_by_coords(\n",
    "            normalised_dfs[i], test_coords_list[i]\n",
    "        )\n",
    "        # determine the corresponding labels\n",
    "        y_train, y_test = train_rows[\"gt\"], test_rows[\"gt\"]\n",
    "        if data_type == \"discrete\":\n",
    "            y_train, y_test = threshold_array(y_train), threshold_array(y_test)\n",
    "        # append everything to where it needs to be\n",
    "        X_trains.append(train_rows.drop(\"gt\", axis=1))\n",
    "        X_tests.append(test_rows.drop(\"gt\", axis=1))\n",
    "        y_trains.append(y_train), y_tests.append(y_test)\n",
    "\n",
    "    # for now, merge all lists together\n",
    "    X_trains = utils.flatten_list(X_trains)\n",
    "    X_tests = utils.flatten_list(X_tests)\n",
    "    y_trains = utils.flatten_list(y_trains)\n",
    "    y_tests = utils.flatten_list(y_tests)\n",
    "    train_coords_list = utils.flatten_list(train_coords_list)\n",
    "    test_coords_list = utils.flatten_list(test_coords_list)\n",
    "\n",
    "    return X_trains, X_tests, y_trains, y_tests, train_coords_list, test_coords_list\n",
    "\n",
    "\n",
    "def generate_test_train_coords_from_dfs(\n",
    "    dfs: list[pd.DataFrame],\n",
    "    test_fraction: float = 0.25,\n",
    "    split_type: str = \"pixel\",\n",
    "    train_test_lat_divide: int = -18,\n",
    "    train_direction: str = \"N\",\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_coords_list, test_coords_list = [], []\n",
    "    for df in dfs:\n",
    "        lists = generate_test_train_coords_from_df(\n",
    "            df=df,\n",
    "            test_fraction=test_fraction,\n",
    "            split_type=split_type,\n",
    "            train_test_lat_divide=train_test_lat_divide,\n",
    "            train_direction=train_direction,\n",
    "        )\n",
    "        train_coords_list.append(lists[0])\n",
    "        test_coords_list.append(lists[1])\n",
    "\n",
    "    return train_coords_list, test_coords_list\n",
    "\n",
    "\n",
    "def generate_test_train_coords_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    test_fraction: float = 0.25,\n",
    "    split_type: str = \"pixel\",\n",
    "    train_test_lat_divide: int = -18,\n",
    "    train_direction: str = \"N\",\n",
    "    random_seed: int = 42,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Shuffle the filtered DataFrame randomly\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed)\n",
    "\n",
    "    if split_type == \"pixel\":\n",
    "        # num datapoints\n",
    "        num_samples = len(df)\n",
    "        # Calculate the split sizes\n",
    "        test_size = int(num_samples * test_fraction)\n",
    "        train_size = num_samples - test_size\n",
    "        # Split the coordinates into two lists based on the split sizes\n",
    "        train_coordinates = (\n",
    "            df_shuffled[[\"latitude\", \"longitude\"]].values[:train_size].tolist()\n",
    "        )\n",
    "        test_coordinates = (\n",
    "            df_shuffled[[\"latitude\", \"longitude\"]]\n",
    "            .values[train_size : train_size + test_size]  # noqa\n",
    "            .tolist()\n",
    "        )\n",
    "    elif split_type == \"spatial\":\n",
    "        if train_direction == \"S\":\n",
    "            train_rows = df_shuffled.loc[\n",
    "                (\n",
    "                    train_test_lat_divide\n",
    "                    >= df_shuffled.index.get_level_values(\"latitude\")\n",
    "                )\n",
    "            ]\n",
    "            test_rows = df_shuffled.loc[\n",
    "                (\n",
    "                    train_test_lat_divide\n",
    "                    <= df_shuffled.index.get_level_values(\"latitude\")\n",
    "                )\n",
    "            ]\n",
    "        elif train_direction == \"N\":\n",
    "            train_rows = df_shuffled.loc[\n",
    "                (\n",
    "                    train_test_lat_divide\n",
    "                    <= df_shuffled.index.get_level_values(\"latitude\")\n",
    "                )\n",
    "            ]\n",
    "            test_rows = df_shuffled.loc[\n",
    "                (\n",
    "                    train_test_lat_divide\n",
    "                    >= df_shuffled.index.get_level_values(\"latitude\")\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Train direction: {train_direction} not recognised.\")\n",
    "\n",
    "        train_coordinates = train_rows[[\"latitude\", \"longitude\"]].values.tolist()\n",
    "        test_coordinates = test_rows[[\"latitude\", \"longitude\"]].values.tolist()\n",
    "    else:\n",
    "        print(f\"Unrecognised split type {split_type}.\")\n",
    "\n",
    "    return train_coordinates, test_coordinates\n",
    "\n",
    "def train_tune_across_models(\n",
    "    model_types: list[str],\n",
    "    d_resolution: float = 1/27,\n",
    "    data: xa.Dataset = None,\n",
    "    split_type: str = \"pixel\",\n",
    "    test_fraction: float = 0.25,\n",
    "    train_test_lat_divide: int = None,\n",
    "    train_direction: str = \"N\",\n",
    "    cv: int = 3,\n",
    "    n_iter: int = 10,\n",
    "    num_samples: int = 3,\n",
    "    search_type: str = \"random\",\n",
    "    best_params_dict: list[dict] = None\n",
    "):\n",
    "    model_comp_dir = file_ops.guarantee_existence(\n",
    "        directories.get_datasets_dir() / \"model_params/best_models\"\n",
    "    )\n",
    "    # if data not provided, run on all comparison data\n",
    "    if not data:\n",
    "        all_data = baselines.get_comparison_xa_ds(d_resolution=d_resolution)\n",
    "        all_data = spatial_data.combine_ds_tiles(all_data, d_resolution)\n",
    "\n",
    "    res_string = utils.replace_dot_with_dash(f\"{d_resolution:.04f}d\")\n",
    "\n",
    "    # define train/test split so it's the same for all models\n",
    "    (X_trains, X_tests, y_trains, y_tests, _, _) = spatial_split_train_test(\n",
    "        xa_dss = all_data,\n",
    "        gt_label = \"gt\",\n",
    "        split_type=split_type,\n",
    "        test_fraction=test_fraction,\n",
    "        train_test_lat_divide=train_test_lat_divide,\n",
    "        train_direction = train_direction\n",
    "    )\n",
    "\n",
    "    for i, model in tqdm(\n",
    "        enumerate(model_types), total=len(model_types), desc=\"Fitting each model via random search\"\n",
    "    ):\n",
    "        baselines.train_tune(\n",
    "            X_train=X_trains,\n",
    "            y_train=y_trains,\n",
    "            model_type=model,\n",
    "            resolution=d_resolution,\n",
    "            save_dir=model_comp_dir,\n",
    "            name=f\"{model}_{res_string}_tuned\",\n",
    "            test_fraction=test_fraction,\n",
    "            cv=cv,\n",
    "            n_iter=n_iter,\n",
    "            num_samples=num_samples,\n",
    "            search_type=search_type,\n",
    "            # best_params_dict=best_params_dict[i]\n",
    "        )\n",
    "    return X_trains, X_tests, y_trains, y_tests\n",
    "\n",
    "X_trains, X_tests, y_trains, y_tests = train_tune_across_models(\n",
    "    # model_types=[\"maxent\", \"rf_reg\", \"brt\", \"rf_cla\"],\n",
    "    # model_types=[\"rf_reg\", \"brt\"],\n",
    "    model_types=[\"rf_cla\", \"maxent\"],\n",
    "    d_resolution=0.25, cv=3, n_iter=50, num_samples=1, split_type=\"pixel\", train_test_lat_divide=-18, \n",
    "    search_type=\"random\", train_direction=\"N\",\n",
    "    # best_params_dict=[rf_reg.best_params_, brt.best_params_]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_train_test_spatial(xa_da: xa.DataArray, figsize: tuple[float,float] = (7,7), bath_mask: xa.DataArray = None):\n",
    "#     \"\"\"\n",
    "#     Plot two spatial variables from a dataset with different colors and labels.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset (xarray.Dataset): The dataset containing the variables.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     # Create a figure and axes\n",
    "#     fig, ax = plt.subplots(figsize = figsize, subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "#     cmap = spatial_plots.get_cbar()\n",
    "#     bounds = [0,0.5,1]\n",
    "#     # TODO: fix cmap\n",
    "#     # https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.BoundaryNorm.html    \n",
    "#     if bath_mask.any():\n",
    "#         xa_da = xa_da.where(bath_mask, np.nan)\n",
    "\n",
    "#     im = xa_da.isel(time=-1).plot.pcolormesh(ax=ax, cmap = cmap, add_colorbar=False)\n",
    "#     ax.set_aspect(\"equal\")\n",
    "#     ax.set_title(\"Geographical visualisation of train-test split\")\n",
    "#     ax.add_feature(\n",
    "#         cfeature.NaturalEarthFeature(\n",
    "#             \"physical\", \"land\", \"10m\", edgecolor=\"black\", facecolor=\"#cccccc\"\n",
    "#         )\n",
    "#     )    \n",
    "#     ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "#     # plt.colorbar(im)\n",
    "#     # format categorical colorbar\n",
    "#     bounds = [0,0.5,1]\n",
    "#     norm = mpl.colors.BoundaryNorm(bounds, 2)\n",
    "    \n",
    "#     # calculate the position of the tick labels\n",
    "#     min_, max_ = 0,1\n",
    "#     positions = [0.25, 0.75]\n",
    "#     val_lookup = dict(zip(positions, [\"train\", \"test\"]))\n",
    "\n",
    "#     def formatter_func(x, pos):\n",
    "#         'The two args are the value and tick position'\n",
    "#         val = val_lookup[x]\n",
    "#         return str(val)\n",
    "\n",
    "#     formatter = plt.FuncFormatter(formatter_func)\n",
    "#     fig.colorbar(ax=ax, mappable=mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "#         ticks=positions, format=formatter, spacing='proportional', pad=0.1, fraction=0.046);\n",
    "#     return xa_da\n",
    "\n",
    "\n",
    "# def generate_reproducing_metrics_at_different_resolutions(resolutions: list[float], units: list[str]) -> xa.Dataset:\n",
    "\n",
    "#     target_resolutions = [spatial_data.choose_resolution(number, string)[1] for number, string in zip(resolutions, units)]\n",
    "#     for res in tqdm(target_resolutions, total=len(target_resolutions), desc=\"Processing metrics at various resolutions\"):\n",
    "        \n",
    "#         files_dir = directories.get_comparison_dir() / utils.replace_dot_with_dash(f\"{res:.05f}d_arrays\")\n",
    "#         files = file_ops.return_list_filepaths(files_dir, \".nc\")\n",
    "#         xa_ds_dict = {}\n",
    "\n",
    "#         for fl in files:\n",
    "#             name = (fl.stem).split(\"_\")[0]\n",
    "#             ds = xa.open_dataset(fl).rio.write_crs(\"epsg:4326\")\n",
    "#             variable_name = next((var_name for var_name in ds.data_vars if var_name != \"spatial_ref\"), None)\n",
    "#             xa_ds_dict[name] = ds[variable_name]\n",
    "\n",
    "#         generate_reproducing_metrics(xa_ds_dict, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = [1,  0.5, 0.25, 1/12, 4000]\n",
    "units = [\"d\", \"d\", \"d\", \"d\", \"m\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(directories.get_datasets_dir() / \"model_params/all_0-03691d_comparative_10_runs_0.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = [1,  0.5, 0.25, 1/12, 4000]\n",
    "units = [\"d\", \"d\", \"d\", \"d\", \"m\"]\n",
    "\n",
    "target_resolutions = [spatial_data.choose_resolution(number, string)[1] for number, string in zip(resolutions, units)]\n",
    "target_resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"rf_reg\", \"brt\", \"maxent\", \"rf_cla\"]\n",
    "\n",
    "all_outputs = train_test_across_models(models, d_resolution=0.03691)\n",
    "# run_outputs = train_test_visualise_roc_across_resolutions(d_resolutions=target_resolutions,model_type=\"rf_cla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathymetry_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = get_comparison_xa_ds(d_resolution=0.03691)\n",
    "\n",
    "\n",
    "# Assuming you have an xarray dataset called 'ds' with variables 'gt' and 'bathymetry'\n",
    "# Access the variables\n",
    "gt_values = all_data['gt']\n",
    "bathymetry_values = all_data['bathymetry_A']\n",
    "\n",
    "# Flatten the variables into 1D arrays\n",
    "gt_flat = gt_values.values.flatten()\n",
    "bathymetry_flat = bathymetry_values.values.flatten()\n",
    "\n",
    "# Remove NaN values\n",
    "valid_indices = np.logical_and(np.isfinite(gt_flat), np.isfinite(bathymetry_flat))\n",
    "gt_valid = gt_flat[valid_indices]\n",
    "bathymetry_valid = bathymetry_flat[valid_indices]\n",
    "\n",
    "# Define the threshold values for bathymetry\n",
    "threshold_min = -100  # Minimum threshold\n",
    "threshold_max = 100  # Maximum threshold\n",
    "\n",
    "# Create a mask to filter out values below and above the thresholds\n",
    "mask = np.logical_and(bathymetry_valid >= threshold_min, bathymetry_valid <= threshold_max)\n",
    "gt_filtered = gt_valid[mask]\n",
    "bathymetry_filtered = bathymetry_valid[mask]\n",
    "\n",
    "# Plot the histogram\n",
    "# Plot the histogram\n",
    "plt.hist2d(bathymetry_filtered, gt_filtered, bins=50, cmap='viridis')\n",
    "plt.xlabel('Bathymetry')\n",
    "plt.ylabel('gt')\n",
    "plt.colorbar(label='Count')\n",
    "plt.title('Histogram of gt vs Bathymetry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"bathymetry_A\"].plot(vmin=-40, vmax=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"gt\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the \"gt\" variable from the dataset\n",
    "gt_variable = all_data['gt']\n",
    "\n",
    "# Count the number of non-zero and zero values\n",
    "non_zero_count = (gt_variable != 0).sum()\n",
    "zero_count = (gt_variable == 0).sum()\n",
    "\n",
    "# Create a bar chart\n",
    "labels = ['Non-Zero', 'Zero']\n",
    "values = [non_zero_count, zero_count]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Non-Zero and Zero Values in \"gt\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the variable into a 1D array and remove NaN values\n",
    "gt_values = gt_variable.values.flatten()\n",
    "gt_values = gt_values[~np.isnan(gt_values)]\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(gt_values, bins=10)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of \"gt\" Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_class_balance(all_data: xa.Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(run_outputs['1.00000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs_n_runs(run_outputs['0.08333'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0.083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SST (sea water potential temperature)\n",
    "# load in daily sea water potential temp\n",
    "thetao_daily = xa.open_dataarray(directories.get_processed_dir() / \"arrays/thetao.nc\")\n",
    "\n",
    "# annual average, stdev of annual averages, annual minimum, annual maximum\n",
    "thetao_annual_average, _, (thetao_annual_min, thetao_annual_max) = baselines.calc_timeseries_params(thetao_daily, \"y\", \"thetao\")\n",
    "# monthly average, stdev of monthly averages, monthly minimum, monthly maximum\n",
    "thetao_monthly_average, thetao_monthly_stdev, (thetao_monthly_min, thetao_monthly_max) = baselines.calc_timeseries_params(\n",
    "    thetao_daily, \"m\", \"thetao\")\n",
    "# annual range (monthly max - monthly min)\n",
    "thetao_annual_range = (thetao_annual_max - thetao_annual_min).rename(\"thetao_annual_range\")\n",
    "# weekly minimum, weekly maximum\n",
    "_, _, (thetao_weekly_min, thetao_weekly_max) = baselines.calc_timeseries_params(thetao_daily, \"w\", \"thetao\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Salinity\n",
    "# load in daily sea water salinity means\n",
    "salinity_daily = xa.open_dataarray(directories.get_processed_dir() / \"arrays/so.nc\")\n",
    "\n",
    "# annual average\n",
    "salinity_annual_average, _, _ = baselines.calc_timeseries_params(salinity_daily, \"y\", \"salinity\")\n",
    "# monthly min, monthly max\n",
    "_, _, (salinity_monthly_min, salinity_monthly_max) = baselines.calc_timeseries_params(salinity_daily, \"m\", \"salinity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Current speed (dot product of horizontal and vertical)\n",
    "# load in daily currents (longitudinal and latitudinal)\n",
    "uo_daily = xa.open_dataarray(directories.get_processed_dir() / \"arrays/uo.nc\")\n",
    "vo_daily = xa.open_dataarray(directories.get_processed_dir() / \"arrays/vo.nc\")\n",
    "# calculate current magnitude\n",
    "current_daily = baselines.calculate_magnitude(uo_daily, vo_daily)\n",
    "\n",
    "# annual average\n",
    "current_annual_average, _, _ = baselines.calc_timeseries_params(current_daily, \"y\", \"current\")\n",
    "# monthly min, monthly max\n",
    "_, _, (current_monthly_min, current_monthly_max) = baselines.calc_timeseries_params(current_daily, \"m\", \"current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Light penetration proxy\n",
    "# Load in bathymetry and scale to climate variable resolution\n",
    "bathymetry = xa.open_dataset(\n",
    "    directories.get_bathymetry_datasets_dir() / \"bathymetry_A_0-00030d.nc\").rio.write_crs(\"EPSG:4326\")[\"bathymetry_A\"]\n",
    "bathymetry_climate_res = spatial_data.upsample_xa_d_to_other(bathymetry, thetao_annual_average, name = \"bathymetry\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in ERA5 surface net solar radiation and upscale to climate variable resolution\n",
    "solar_radiation = xa.open_dataarray(\n",
    "    directories.get_era5_data_dir() / \"weather_parameters/VAR_surface_net_solar_radiation_LATS_-10_-17_LONS_142_147_YEAR_1993-2020.nc\"\n",
    "    ).rio.write_crs(\"EPSG:4326\")\n",
    "    \n",
    "# average solar_radiation daily\n",
    "solar_radiation_daily = solar_radiation.resample(time=\"1D\").mean(dim=\"time\")\n",
    "solar_climate_res = spatial_data.upsample_xa_d_to_other(solar_radiation_daily, thetao_annual_average, name = \"solar_radiation\")\n",
    "\n",
    "# annual average\n",
    "solar_annual_average, _, _ = baselines.calc_timeseries_params(solar_climate_res, \"y\", \"net_solar\")\n",
    "# monthly min, monthly max\n",
    "_, _, (solar_monthly_min, solar_monthly_max) = baselines.calc_timeseries_params(solar_climate_res, \"m\", \"net_solar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load in ground truth data\n",
    "gt_1000m = xa.open_dataarray(directories.get_processed_dir() / \"arrays/coral_raster_1000m.nc\")\n",
    "# upsample to same resolution as climate (1/12 of a degree)\n",
    "gt_climate_res = spatial_data.upsample_xa_d_to_other(gt_1000m, thetao_annual_average, name = \"gt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display different resolutions\n",
    "fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(16,9), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "ax1 = spatial_plots.plot_spatial(gt_1000m, fax= (fig,ax_left), title=\"Ground truth coral presence map at 1000m\")\n",
    "ax2 = spatial_plots.plot_spatial(\n",
    "    gt_climate_res, fax=(fig, ax_right), val_lims = (0,1), title=\"Ground truth coral presence map at 1/12 degree\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "all_data = xa.merge([thetao_annual_average, thetao_annual_range, thetao_monthly_min, thetao_monthly_max, \n",
    "    thetao_monthly_stdev, thetao_weekly_min, thetao_weekly_max, \n",
    "    salinity_annual_average, salinity_monthly_min, salinity_monthly_max,\n",
    "    current_annual_average, current_monthly_min, current_monthly_max,\n",
    "    bathymetry_climate_res, solar_annual_average, solar_monthly_min, solar_monthly_max,\n",
    "    gt_climate_res\n",
    "    ])\n",
    "    \n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = all_data.to_dataframe()\n",
    "# test_df = test_df.dropna(subset=subset, how=\"any\",axis=0)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "features_df = pd.get_dummies(test_df).drop([\"spatial_ref\",\"band\",\"depth\",\"gt\"], axis=1)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = test_df[\"gt\"]\n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test, train_coordinates,test_coordinates, xa_masked = spatial_split_train_test(\n",
    "#     all_data, \"gt\", split_type=\"pixel\", test_fraction = 0.2)\n",
    "\n",
    "# test_train_da = visualise_train_test_split(all_data, train_coordinates, test_coordinates)\n",
    "bath_mask = generate_var_mask(all_data)\n",
    "test_xa = plot_train_test_spatial(test_train_da, bath_mask=bath_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_outcomes = n_random_runs_preds(rf_random, 10, all_data)\n",
    "\n",
    "rocs_n_runs(run_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_resolution_predictions(xa_ds: xa.DataArray):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.mean(spatial_data.calculate_spatial_resolution(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_label_thresholds(\n",
    "    thresholds: list[float],\n",
    "    y_test: np.ndarray | pd.Series,\n",
    "    y_predictions: np.ndarray | pd.Series,\n",
    "    figsize=[7, 7],\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot ROC curves with multiple lines for different label thresholds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        thresholds (list[float]): List of label thresholds.\n",
    "        y_test (np.ndarray or pd.Series): True labels.\n",
    "        y_predictions (np.ndarray or pd.Series): Predicted labels.\n",
    "        figsize (list, optional): Figure size for the plot. Default is [7, 7].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    # TODO: fix UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
    "\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "    # prepare colour assignment\n",
    "    color_map = spatial_plots.get_cbar(\"seq\")\n",
    "    num_colors = len(thresholds)\n",
    "    colors = [color_map(i / num_colors) for i in range(num_colors)]\n",
    "\n",
    "    # plot ROC curves\n",
    "    for c, thresh in enumerate(thresholds):\n",
    "        binary_y_labels, binary_predictions = model_results.threshold_label(\n",
    "            y_test, y_predictions, thresh\n",
    "        )\n",
    "        fpr, tpr, _ = sklmetrics.roc_curve(\n",
    "            binary_y_labels, binary_predictions, drop_intermediate=False\n",
    "        )\n",
    "        roc_auc = sklmetrics.auc(fpr, tpr)\n",
    "\n",
    "        label = f\"{thresh:.01f} | {roc_auc:.02f}\"\n",
    "        ax.plot(fpr, tpr, label=label, color=colors[c])\n",
    "\n",
    "    # format\n",
    "    title = \"Receiver Operating Characteristic (ROC) Curve\\nfor several coral presence/absence thresholds\"\n",
    "    format_roc(ax=ax, title=title)\n",
    "    ax.legend(title=\"threshold value | auc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random_preds = rf_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate_label_thresholds(np.linspace(0,1,10), y_test, rf_random_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "rf_reg = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "# rf_reg.get_params()\n",
    "# rf_random = RandomizedSearchCV(\n",
    "#     estimator = rf_reg, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, \n",
    "#     random_state=RANDOM_STATE, n_jobs = -1)\n",
    "\n",
    "rf_reg.fit(X_train, y_train)\n",
    "# rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_params={\"n_estimators\":400,\n",
    "\"min_samples_split\":2,\n",
    "\"min_samples_leaf\":4,\n",
    "\"max_features\":\"sqrt\",\n",
    "\"max_depth\":10,\n",
    "\"bootstrap\":True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = rf_reg.set_params(**best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random_search_best_params = rf_random.best_params_ \n",
    "# save best parameteers to json (in coralshift folder)\n",
    "import json\n",
    "\n",
    "with open(\"rf_random_search_best_params.json\", \"w\") as fp:\n",
    "    json.dump(rf_random_search_best_params, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = rf_random.predict(X_test)\n",
    "# np.shape(predictions)\n",
    "bce = log_loss(y_true=list(y_test), y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn import metrics\n",
    "# predictions = rf_random.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = rf_reg.predict(X_test)\n",
    "# predictions\n",
    "sum(y_test.where(y_test <= 0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0\n",
    "for i in range(11):\n",
    "    print(val, sum(y_test.where(y_test >= val, 1)))\n",
    "    val += 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array(y_test)\n",
    "sum(np.where(out > 0.1, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "rf_reg_preds = rf_reg.predict(X_test)\n",
    "rf_random_preds = rf_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(sum(np.array(y_test)) > 0.1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: function to do N model runs and plot resultant ROC\n",
    "# TODO: test whether training/optimising on binary helps\n",
    "\n",
    "\n",
    "model_results.investigate_label_thresholds(np.linspace(0,1,100), y_test, rf_reg_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_labels_bin, predictions_bin, drop_intermediate=False)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                            estimator_name='example estimator')\n",
    "display.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#convert y values to categorical values\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxent = LogisticRegression(random_state=0)\n",
    "# maxent.fit(features_df, y_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_maxent = maxent.predict(features_df)\n",
    "\n",
    "# predicted_maxent_data = xa.DataArray(pred_maxent.reshape((85,61,28)),\n",
    "#     coords=all_data.coords, \n",
    "#     dims=all_data.dims)\n",
    "\n",
    "# f,a = plt.subplots(1,2,figsize=[14,7])\n",
    "# all_data[\"gt\"].plot(ax=a[0])\n",
    "# predicted_maxent_data.isel(time=-1).plot(ax=a[1]\n",
    "#     , vmin=all_data[\"gt\"].values.min(), vmax=all_data[\"gt\"].values.max()\n",
    "#     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf_lim = RandomForestRegressor()\n",
    "\n",
    "num_vals = 50000\n",
    "clf.fit(features_df, target_df)\n",
    "clf_lim.fit(features_df[:num_vals], target_df[:num_vals])\n",
    "pred_lim = clf_lim.predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(features_df, target_df)\n",
    "pred = clf.predict(features_df)\n",
    "predicted_data = xa.DataArray(pred.reshape((85,61,28)),\n",
    "    coords=all_data.coords, \n",
    "    dims=all_data.dims).isel(time=0)\n",
    "\n",
    "predicted_lim_data = xa.DataArray(pred_lim.reshape((85,61,28)),\n",
    "    coords=all_data.coords, \n",
    "    dims=all_data.dims).isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare predicted and ground truth values\n",
    "spatial_plots.plot_spatial_diffs(predicted_data, gt_climate_res, figsize=(14,13))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_plots.plot_spatial_diffs(predicted_lim_data, gt_climate_res, figsize=(14,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test_random_forest(xa_ds: xa.Dataset, target_variable: str = \"gt\", test_fraction=0.5, random_state=None):\n",
    "    # TODO: tidy up and document\n",
    "    # Extract latitude, longitude, and time values from the spatial image\n",
    "    lats = xa_ds.latitude.values\n",
    "    lons = xa_ds.longitude.values\n",
    "    times = xa_ds.time.values\n",
    "\n",
    "\n",
    "    if len(xa_ds.dims) > 2:\n",
    "        # Flatten the spatial image into 2D arrays for training and testing\n",
    "        flattened_data = xa_ds.stack(points=(\"latitude\", \"longitude\", \"time\")).compute().to_dataframe().fillna(0).drop(\n",
    "            [\"time\",\"spatial_ref\",\"band\",\"depth\"], axis=1).astype(\"float32\")\n",
    "    else:\n",
    "        flattened_data = xa_ds.stack(points=(\"latitude\", \"longitude\")).compute().to_dataframe().fillna(0).drop(\n",
    "            [\"time\",\"spatial_ref\",\"band\",\"depth\"], axis=1).astype(\"float32\")\n",
    "\n",
    "    features = flattened_data.drop(\"gt\", axis=1)\n",
    "    # flattened_data = np.transpose(flattened_data, axes=(1, 0))\n",
    "    labels = flattened_data[\"gt\"]\n",
    "\n",
    "    # # Split the data into training and testing datasets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     features, labels, test_size=test_fraction, random_state=random_state\n",
    "    # )\n",
    "\n",
    "    # Train the random forest regressor\n",
    "    regressor = RandomForestRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the target variable for the testing dataset\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    train_indices = X_train.index.values\n",
    "    test_indices = X_test.index.values\n",
    "\n",
    "    lat_spacing = xa_ds.latitude.values[1] - xa_ds.latitude.values[0]\n",
    "    lon_spacing = xa_ds.longitude.values[1] - xa_ds.longitude.values[0]\n",
    "\n",
    "    # TODO: refer to generic data_var dimension rather than calling by variable\n",
    "    train_pixs = np.empty(xa_ds[\"thetao_y_mean\"].values.shape)\n",
    "    train_pixs[:] = np.nan\n",
    "    test_pixs = np.empty(xa_ds[\"thetao_y_mean\"].values.shape)\n",
    "    test_pixs[:] = np.nan\n",
    "    # Color the spatial pixels corresponding to training and testing regions\n",
    "    for train_index in tqdm(train_indices, desc=\"Coloring in training indices...\"):\n",
    "        row, col = find_index_pair(xa_ds, train_index[0], train_index[1], lat_spacing, lon_spacing)\n",
    "        # ax.add_patch(Rectangle((lons[col], lats[row]), 1, 1, facecolor=train_color, alpha=0.2))\n",
    "        train_pixs[row,col] = 0\n",
    "\n",
    "    for test_index in tqdm(test_indices, desc=\"Coloring in training indices...\"):\n",
    "        row, col = find_index_pair(xa_ds, test_index[0], test_index[1], lat_spacing, lon_spacing)\n",
    "        # ax.add_patch(Rectangle((lons[col], lats[row]), 1, 1, facecolor=test_color, alpha=0.2))\n",
    "        test_pixs[row,col] = 1\n",
    "\n",
    "    return regressor, y_pred, ds\n",
    "\n",
    "\n",
    "reg,random_pred,ds = train_test_random_forest(all_data.isel(time=0), target_variable=all_data[\"gt\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pred_data = xa.DataArray(random_pred.reshape((85,61,28)),\n",
    "    coords=all_data.coords,\n",
    "    dims=all_data.dims)\n",
    "\n",
    "\n",
    "spatial_plots.plot_spatial_diffs(random_pred_data, gt_climate_res, figsize=(14,13))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_spatial(dataset, figsize: tuple[float,float] = (7,7)):\n",
    "    \"\"\"\n",
    "    Plot two spatial variables from a dataset with different colors and labels.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (xarray.Dataset): The dataset containing the variables.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize = figsize, subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "    # Plot variable1 with color and label\n",
    "    dataset[\"train_pixs\"].plot(ax=ax, vmin=0, vmax=1, levels=2, label=\"train_pixs\",add_colorbar=False)\n",
    "\n",
    "    # Plot variable2 with color and label\n",
    "    dataset[\"test_pixs\"].plot(ax=ax, vmin=0, vmax=1, levels=2, label=\"test_pixs\", add_colorbar=False)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.coastlines(resolution=\"10m\", color=\"red\", linewidth=3)\n",
    "    # Add a colorbar for each variable\n",
    "    # cbar1 = plt.colorbar(ax=ax, mappable=dataset[\"train_pixs\"])\n",
    "    # cbar2 = plt.colorbar(ax=ax, mappable=dataset[\"test_pixs\"])\n",
    "    ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_train_test_spatial(ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_categorical_data(dataset, variable_name,  color1='red', color2='blue'):\n",
    "    \"\"\"\n",
    "    Plot categorical spatial data from an xarray dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (xarray.Dataset): The dataset containing the categorical data.\n",
    "    variable_name (str): The name of the variable in the dataset to plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract the required data variable\n",
    "    variable = dataset[variable_name]\n",
    "\n",
    "    # Get the latitude, longitude, and values arrays\n",
    "    latitudes = variable.latitude.values\n",
    "    longitudes = variable.longitude.values\n",
    "    values = variable.values\n",
    "\n",
    "    # Create a meshgrid from latitude and longitude arrays\n",
    "    lon_mesh, lat_mesh = np.meshgrid(longitudes, latitudes)\n",
    "\n",
    "    # Plot the categorical data\n",
    "    mask1 = values == 0\n",
    "    mask2 = values == 1\n",
    "\n",
    "    # Plot the binary categorical data\n",
    "    plt.pcolormesh(lon_mesh, lat_mesh, mask1, cmap='Greys', facecolor=color1)\n",
    "    plt.pcolormesh(lon_mesh, lat_mesh, mask2, cmap='Greys', facecolor=color2)\n",
    "\n",
    "    # Add colorbar and labels\n",
    "    # plt.colorbar()\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(variable_name)\n",
    "    plt.legend()\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_data(ds,\"test_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predicted_lim_data = xa.DataArray(pred_lim.reshape((85,61,28)),\n",
    "    # coords=all_data.coords, \n",
    "    dims=all_data.dims)\n",
    "\n",
    "f,a = plt.subplots(1,2,figsize=[14,7])\n",
    "predicted_data.isel(time=0).plot(ax=a[0])\n",
    "predicted_lim_data.isel(time=0).plot(ax=a[1], vmin=predicted_data.min(), vmax=predicted_data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training and testing on subsamples of data (train_test_split for linear, somehow something spatial...)\n",
    "# add in bathymetry\n",
    "# try binary (classifier)\n",
    "# function to plot difference between predicted and true\n",
    "# hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Regression Trees (BRT)\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "    features_df, target_df, test_size=0.1, random_state=13\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "    test_score[i] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Deviance\")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1,\n",
    "    reg.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set Deviance\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = reg.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(features_df.columns)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(features_df.columns)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_pred = reg.predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_gbr_data = xa.DataArray(gbr_pred.reshape((85,61,28)),\n",
    "    coords=all_data.coords, \n",
    "    dims=all_data.dims).isel(time=0)\n",
    "\n",
    "\n",
    "spatial_plots.plot_spatial_diffs(predicted_gbr_data, gt_climate_res, figsize=(14,13))\n",
    "\n",
    "\n",
    "\n",
    "# f,a = plt.subplots(1,2,figsize=[14,7])\n",
    "# all_data[\"gt\"].plot(ax=a[0])\n",
    "# predicted_gbr_data.isel(time=-1).plot(ax=a[1]\n",
    "#     , vmin=all_data[\"gt\"].values.min(), vmax=all_data[\"gt\"].values.max()\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

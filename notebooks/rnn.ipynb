{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 21:40:56.955820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      9\u001b[0m \u001b[39m#issues with numpy deprecation in pytorch_env\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcoralshift\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "import math as m\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in xa_array\n",
    "array_path = Path(\"/Volumes/MRes Drive/global_ocean_reanalysis/monthly_means/baseline_area/coral_climate_1_12.nc\")\n",
    "xa_coral_climate_1_12 = xa.open_dataset(array_path)\n",
    "xa_coral_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xa_coral_climate_1_12[\"usi\"].isel(time=-1).values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return only parameter values (exclude target and spatial reference)\n",
    "parameter_names = list(set(list(xa_coral_climate_1_12.data_vars))-set(['spatial_ref', 'coral_algae_1-12_degree']))\n",
    "xa_coral_climate_1_12_features = xa_coral_climate_1_12[parameter_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3D array from xarray dataset variables. Shape: (num_samples, num_parameters, sequence_len)\n",
    "X_with_nans = data.xa_ds_to_3d_numpy(xa_coral_climate_1_12_features)\n",
    "print(f'X_with_nans shape (num_samples: {X_with_nans.shape[0]}, total num_parameters (includes nans parameters): {X_with_nans.shape[1]}, sequence_len: {X_with_nans.shape[2]})')\n",
    "\n",
    "for i, param in enumerate(xa_coral_climate_1_12_features.data_vars):\n",
    "    print(f\"{i}: {param}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations for which there are nan values\n",
    "\n",
    "99% sure these are are just gridcells containing land. Would be a good thing to investigate, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_with_nans\n",
    "# problem, probably with sea ice features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out columns that contain entirely NaN values\n",
    "col_mask = ~np.all(np.isnan(X), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "masked_cols = X[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "print(\"masked_cols shape:\", masked_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_1d(array):\n",
    "    \"\"\"Reshape a 2D array to a 1D array.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    array : numpy.ndarray\n",
    "        2D array to be reshaped.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        1D array representing the reshaped input array.\n",
    "    \"\"\"\n",
    "    return array.flatten()\n",
    "\n",
    "    \n",
    "def map_to_2d(array_1d, shape):\n",
    "    \"\"\"Map a 1D array back to the original 2D shape.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    array_1d : numpy.ndarray\n",
    "        1D array to be mapped.\n",
    "\n",
    "    shape : tuple\n",
    "        Shape of the original 2D array.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        2D array with values from the 1D array mapped to their correct coordinate points.\n",
    "    \"\"\"\n",
    "    return array_1d.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all rows which contain any NaN values\n",
    "row_mask = ~np.any(np.isnan(masked_cols), axis=1) # boolean mask indicating which rows to keep\n",
    "masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "masked_cols_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all depths which contain any NaN values\n",
    "depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "print(f\"X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target from coral ground truth. Shape: (num_samples, 1)\n",
    "# TODO: not sure if this is shuffling the values when reshaping\n",
    "y_with_nans = np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].sel(\n",
    "    time=xa_coral_climate_1_12.time[-1])).reshape(-1, 1)\n",
    "# remove ys with nan values in other variables\n",
    "y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "print(f\"y_with_nans shape: {y_with_nans.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_nans(X_with_nans, y_with_nans):\n",
    "    # must be in shape (num_samples, num_params, seq_length)\n",
    "\n",
    "    # filter out columns that contain entirely NaN values\n",
    "    col_mask = ~np.all(np.isnan(X_with_nans), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "    masked_cols = X_with_nans[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "\n",
    "    # filter out all rows which contain any NaN values\n",
    "    row_mask = ~np.any(np.isnan(masked_cols), axis=1) # boolean mask indicating which rows to keep\n",
    "    masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "\n",
    "    # filter out all depths which contain any NaN values\n",
    "    depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "    X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "    # swap axes so shape (num_samples, seq_length, num_params)\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "\n",
    "    y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = filter_out_nans(X_with_nans, np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class gru_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, rnn_units: list[int], num_layers: int):\n",
    "        \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "        layers to a desired output shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "        num_layers (int): number of layers in GRU model\n",
    "        \"\"\"\n",
    "        super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "        # store values for later use\n",
    "        self.num_layers = num_layers    # number of layers in GRU model\n",
    "        self.rnn_units = rnn_units\n",
    "        # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "        self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "                                   return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "                                   return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "        # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "        self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "        self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        whole_seq = inputs\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwritting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "    \n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities.\n",
    "    N.B. Binary cross-entropy loss defined as the negative log likelihood of the binary labels given the predicted\n",
    "    probabilities\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()    \n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: np.ndarray, y: np.ndarray, \n",
    "        training: bool=True, batch_size: int=None) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            num_samples = X.shape[0]\n",
    "            num_batches = num_samples // batch_size\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for batch in tqdm(range(num_batches), desc=\"batches\", position=1):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = (batch + 1) * batch_size\n",
    "\n",
    "                X_batch = X[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = gru(X, training) \n",
    "                    xent = negative_log_likelihood(y, y_pred)\n",
    "                \n",
    "                gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "\n",
    "                total_loss += xent\n",
    "\n",
    "            # TODO: return iterative loss\n",
    "            average_loss = total_loss / num_batches\n",
    "            # return predicted output values and total loss value\n",
    "            return y_pred, xent\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    return train_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GRU functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise GRU model with 32 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([500], 1) # N.B. [x] is number of hidden layers in GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.normal(size = (32, 20, 1))    # shape: (num_samples, sequence_length, num_features)\n",
    "y_dud = np.random.choice([0, 1], size = 32)\n",
    "print(\"array shape:\", array.shape)\n",
    "print(\"y_dud shape:\", y_dud.shape)\n",
    "\n",
    "x_train, y_train = X[:500], y[:500].reshape(500,)\n",
    "# x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that untrained model runs (should output array of non-nan values)\n",
    "g_model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 2\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position = 0):\n",
    "        y_pred, average_loss = tr_step(g_model, optimizer, x_train, y_train, batch_size=32, training=True)\n",
    "        \n",
    "        \n",
    "        # for batch in range(num_batches):\n",
    "        #     array, y  = batcher_fun(X, 32, 276 \n",
    "        #     #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "        #     )\n",
    "        #     y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "        #  ## validation set \n",
    "        #  ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model(X,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(y_pred.numpy().reshape(20,25))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((y_pred > 0.5).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check log likelihood is computable\n",
    "negative_log_likelihood(y[:32], g_model(X[:32]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batcher function (by space and time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher_fun(X, y, batch_size, seq_len):\n",
    "    \"\"\"\n",
    "    A function to prepare the data for training the LSTM.\n",
    "    \n",
    "    :param data: The input data to the LSTM.\n",
    "    :param batch_size: The number of samples in each batch.\n",
    "    :param seq_len: The sequence length of each sample.\n",
    "    \n",
    "    :return: A tuple of (batch_x, batch_y), where batch_x is a numpy array of shape (batch_size, seq_len, num_features) \n",
    "             and batch_y is a numpy array of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(num_samples / batch_size)\n",
    "    num_features = data.shape[1]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        \n",
    "        batch_x = np.zeros((batch_size, seq_len, num_features))\n",
    "        batch_y = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            sample = data[j]\n",
    "            X = sample[:-1]\n",
    "            y = y[]\n",
    "            \n",
    "            batch_x[j - start_idx] = x.reshape((seq_len, num_features))\n",
    "            batch_y[j - start_idx, y] = 1\n",
    "            \n",
    "        yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 1\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            array, y  = batcher_fun(X, 32, 276 \n",
    "            #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "            )\n",
    "            y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "         ## validation set \n",
    "         ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copypasta\n",
    "\n",
    "[Source](https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam # https://stackoverflow.com/questions/75356826/attributeerror-adam-object-has-no-attribute-get-updates\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Model configuration\n",
    "additional_metrics = [\"accuracy\"]\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "# embedding_output_dims = 15\n",
    "# embedding_output_dims = 10\n",
    "loss_function = BinaryCrossentropy()\n",
    "# max_sequence_length = 300\n",
    "max_sequence_length = 276\n",
    "# num_distinct_words = 5000\n",
    "# num_distinct_words = 10000\n",
    "number_of_epochs = 5\n",
    "optimizer = Adam()\n",
    "validation_split = 0.20\n",
    "verbosity_mode = 1\n",
    "\n",
    "# Disable eager execution\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\n",
    "x_train, y_train = X[:5000], y[:5000].reshape((5000,))\n",
    "x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad all sequences: keras requires sequences of equal lengths. Should be handled in pre-processing, but here for now for security\n",
    "padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "\n",
    "# (number_samples, sequence_length, num_features)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = pad_sequences(x_train[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "padded_inputs_test = pad_sequences(x_test[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "\n",
    "# (number_samples, sequence_length)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        num_distinct_words+1, embedding_output_dims, input_length=max_sequence_length\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
    "\n",
    "# Give a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    padded_inputs,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=number_of_epochs,\n",
    "    verbose=verbosity_mode,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
    "print(f\"Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_timeseries(history) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history[\"accuracy\"])\n",
    "    ax.plot(history.history[\"val_accuracy\"])\n",
    "\n",
    "    ax.set_title(\"Model accuracy against epoch\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend(['train set', 'validation set'], loc='upper left')\n",
    "\n",
    "plot_score_timeseries(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate model\n",
    "\n",
    "[Source](https://medium.com/@canerkilinc/hands-on-multivariate-time-series-sequence-to-sequence-predictions-with-lstm-tensorflow-keras-ce86f2c0e4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = X[:32*10,:10,:3]\n",
    "print(\"X_toy:\", X_toy.shape)\n",
    "y_toy = y[:32*10]\n",
    "print(\"y_toy:\", y_toy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "#####################################\n",
    "#Before do anything else do not forget to reset the backend for the next iteration (rerun the model)\n",
    "tensorflow.keras.backend.clear_session()\n",
    "#####################################\n",
    "# Initialising the LSTM Model with MAE Loss-Function\n",
    "batch_size = 32\n",
    "epochs = 120\n",
    "timesteps = 10\n",
    "num_features = 3\n",
    "input_1 = Input(batch_shape=(batch_size,timesteps,num_features))\n",
    "#each layer is the input of the next layer\n",
    "lstm_hidden_layer_1 = LSTM(10, stateful=True, return_sequences=True)(input_1)\n",
    "lstm_hidden_layer_2 = LSTM(10, stateful=True, return_sequences=True)(lstm_hidden_layer_1)\n",
    "output_1 = Dense(units = 1)(lstm_hidden_layer_2)\n",
    "regressor_mae = Model(inputs=input_1, outputs = output_1)\n",
    "#adam is fast starting off and then gets slower and more precise\n",
    "#mae -> mean absolute error loss function\n",
    "regressor_mae.compile(optimizer='adam', loss = 'mae')\n",
    "#####################################\n",
    "#Summarize and observe the layers as well as paramter configurations\n",
    "regressor_mae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_mae.fit(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralshift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "import math as m\n",
    "import tensorflow as tf\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in xa_array\n",
    "array_path = Path(\"/Volumes/MRes Drive/reef_baseline/Allen_atlas/Great-Barrier-Reef-and-Torres-Strait-20230310013521/Processed/coral_climate_1_12.nc\")\n",
    "xa_coral_climate_1_12 = xa.open_dataset(array_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return only parameter values (exclude target and spatial reference)\n",
    "parameter_names = list(set(list(xa_coral_climate_1_12.data_vars))-set(['spatial_ref', 'coral_algae_1-12_degree']))\n",
    "xa_coral_climate_1_12_features = xa_coral_climate_1_12[parameter_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 12024.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (num_samples: 25619, num_parameters: 11, sequence_len: 336)\n"
     ]
    }
   ],
   "source": [
    "# create 3D array from xarray dataset variables. Shape: (num_samples, num_parameters, sequence_len)\n",
    "X = data.xa_ds_to_3d_numpy(xa_coral_climate_1_12_features)\n",
    "print(f'X shape (num_samples: {X.shape[0]}, num_parameters: {X.shape[1]}, sequence_len: {X.shape[2]})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations for which there are nan values\n",
    "\n",
    "These are likely all those containing land. Would be a good thing to investigate, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25619, 9, 336)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out columns that contain entirely NaN values\n",
    "col_mask = ~np.all(np.isnan(X), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "masked_cols = X[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "masked_cols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14766, 9, 336)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out all rows which contain any NaN values\n",
    "row_mask = ~np.any(np.isnan(masked_cols), axis=2) # boolean mask indicating which rows to keep\n",
    "masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "masked_cols_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14766, 9, 276)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out all depths which contain any NaN values\n",
    "depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (14766, 1)\n"
     ]
    }
   ],
   "source": [
    "# create target from coral ground truth. Shape: (num_samples, 1)\n",
    "# TODO: not sure if this is shuffling the values when reshaping\n",
    "y_with_nans = np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].sel(\n",
    "    time=xa_coral_climate_1_12.time[-1])).reshape(-1, 1)\n",
    "# remove ys with nan values in other variables\n",
    "y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25619, 1)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_with_nans.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class gru_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, rnn_units: list[int], num_layers: int):\n",
    "        \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "        layers to a desired output shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "        num_layers (int): number of layers in GRU model\n",
    "        \"\"\"\n",
    "        super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "        # store values for later use\n",
    "        self.num_layers = num_layers    # number of layers in GRU model\n",
    "        self.rnn_units = rnn_units\n",
    "        # define model layers: reating new `tf.keras.layers.GRU` layer for each iteration\n",
    "        self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "                                   return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "                                   return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "        # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "        self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "        self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        whole_seq = inputs\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwritting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "    \n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities.\n",
    "    N.B. Binary cross-entropy loss defined as the negative log likelihood of the binary labels given the predicted\n",
    "    probabilities\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()    \n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: np.ndarray, y: np.ndarray, training: bool=True) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = gru(X, training) \n",
    "            xent = negative_log_likelihood(y, y_pred)\n",
    "        \n",
    "        gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "        # return predicted output values and loss value\n",
    "        return y_pred, xent\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    return train_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GRU functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise GRU model with 32 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([32], 1) # N.B. [x] is number of hidden layers in GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
       "array([[0.8274484 ],\n",
       "       [0.8210475 ],\n",
       "       [0.81479585],\n",
       "       [0.81394243],\n",
       "       [0.8120978 ],\n",
       "       [0.8118105 ],\n",
       "       [0.8142073 ],\n",
       "       [0.81293863],\n",
       "       [0.80910945],\n",
       "       [0.809272  ],\n",
       "       [0.81085116],\n",
       "       [0.80866545],\n",
       "       [0.8067812 ],\n",
       "       [0.8091517 ],\n",
       "       [0.80911815],\n",
       "       [0.8099028 ],\n",
       "       [0.812065  ],\n",
       "       [0.81272423],\n",
       "       [0.8104135 ],\n",
       "       [0.8155005 ],\n",
       "       [0.8172883 ],\n",
       "       [0.8165895 ],\n",
       "       [0.830492  ],\n",
       "       [0.8475363 ],\n",
       "       [0.85871553],\n",
       "       [0.86065084],\n",
       "       [0.8597975 ],\n",
       "       [0.8632064 ],\n",
       "       [0.8536532 ],\n",
       "       [0.8515766 ],\n",
       "       [0.85629034],\n",
       "       [0.8505629 ]], dtype=float32)>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that untrained model runs (should output array of non-nan values)\n",
    "g_model(X[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.7530972>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check log likelihood is computable\n",
    "negative_log_likelihood(y[:32], g_model(X[:32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batcher function (by space and time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 1\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            ### array, y  = batcher_fun(data, training = True) shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "            y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "         ## validation set \n",
    "         ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
       "array([[0.49866262],\n",
       "       [0.49958506],\n",
       "       [0.50092477],\n",
       "       [0.5012568 ],\n",
       "       [0.5009623 ],\n",
       "       [0.5004142 ],\n",
       "       [0.49979192],\n",
       "       [0.49927443],\n",
       "       [0.49924508],\n",
       "       [0.49662614],\n",
       "       [0.49602753],\n",
       "       [0.49631017],\n",
       "       [0.496643  ],\n",
       "       [0.49772102],\n",
       "       [0.4969528 ],\n",
       "       [0.49577612],\n",
       "       [0.4951186 ],\n",
       "       [0.49473235],\n",
       "       [0.49461022],\n",
       "       [0.4947742 ],\n",
       "       [0.4947489 ],\n",
       "       [0.49511108],\n",
       "       [0.49604115],\n",
       "       [0.49770057],\n",
       "       [0.4991103 ],\n",
       "       [0.50082684],\n",
       "       [0.5017571 ],\n",
       "       [0.50299186],\n",
       "       [0.504737  ],\n",
       "       [0.5050446 ],\n",
       "       [0.5037324 ],\n",
       "       [0.5030605 ]], dtype=float32)>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralshift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

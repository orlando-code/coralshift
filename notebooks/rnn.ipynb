{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.interpolate import interp2d\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray as rio\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories\n",
    "from coralshift.plotting import spatial_plots\n",
    "from coralshift.dataloading import data_structure, climate_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_man = data_structure.MyDatasets()\n",
    "\n",
    "location = \"remote\"\n",
    "ds_man.set_location(location)\n",
    "\n",
    "if location == \"remote\":\n",
    "    # TODO: hacky, shouldn't be necessary\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"lustre_scratch/coralshift/notebooks/rnn.ipynb\"\n",
    "\n",
    "noaa_features = ['mlotst', 'bottomT', 'uo', 'so', 'zos', 'thetao', 'vo']\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12\", xa.open_dataset(\n",
    "        ds_man.get_location() / \"global_ocean_reanalysis/monthly_means/coral_climate_1_12.nc\")\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_features\", ds_man.get_dataset(\"monthly_climate_1_12\")[noaa_features]\n",
    ")\n",
    "\n",
    "# ds_man.add_dataset(\n",
    "#     \"monthly_climate_1_12_X_y_np\", spatial_data.filter_out_nans(\n",
    "#         spatial_data.xa_ds_to_3d_numpy(ds_man.get_dataset(\"monthly_climate_1_12\")), \n",
    "#         np.array(ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1))\n",
    "# )\n",
    "ds_man.add_datasets(\n",
    "    [\"monthly_climate_1_12_X\", \"monthly_climate_1_12_y\"], \n",
    "        spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"monthly_climate_1_12\"), \n",
    "        feature_vars=noaa_features, gt_var=\"coral_algae_1-12_degree\")\n",
    ")\n",
    "\n",
    "# TODO: handle depth\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12\", xa.open_dataset(\n",
    "        Path(ds_man.get_location() / \"global_ocean_reanalysis/daily_means/dailies_combined.nc\")).isel(depth=0)\n",
    ")\n",
    "\n",
    "# TODO: streamline checking and saving process\n",
    "daily_climate_1_12_X_file_path = ds_man.get_location() / \"global_ocean_reanalysis/daily_means/daily_climate_1_12_X.npy\"\n",
    "# if daily_climate_1_12_X numpy array doesn't exist, generate and save\n",
    "if not file_ops.check_file_exists(filepath = daily_climate_1_12_X_file_path):\n",
    "    daily_climate_1_12_X = spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"daily_climate_1_12\"),\n",
    "        feature_vars = noaa_features)\n",
    "    np.save(daily_climate_1_12_X_file_path, daily_climate_1_12_X) \n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "else:\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "\n",
    "# same target as monthly (but restricted to bathymetry_A area)\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12_y\", spatial_data.generate_patch(\n",
    "        ds_man.get_dataset(\"monthly_climate_1_12\"), lat_lon_starts=(-10,141.95), coord_range=(-7.01,5.11))[0][1]\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"bathymetry_A\", rio.open_rasterio(\n",
    "        rasterio.open(ds_man.get_location() / \"bathymetry/GBR_30m/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif\"),\n",
    "        ).rename(\"bathymetry_A\").rename({\"x\": \"longitude\", \"y\": \"latitude\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_gt, lat_lon_A_vals_dict = spatial_data.sample_spatial_batch(\n",
    "        ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"], lat_lon_starts=(-10,141.95), coord_range=(-7.01,5.11))\n",
    "coral_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_climate_1_12 = ds_man.get_dataset(\"daily_climate_1_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_climate_1_12[\"coral_algae_1-12_degree\"] = coral_gt\n",
    "daily_climate_1_12\n",
    "# daily_climate_1_12 = xa.merge((ds_man.get_dataset(\"daily_climate_1_12\"), coral_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix boolean indexing\n",
    "# # For now, shallowest depth is taken (0.45)\n",
    "# # TODO: process this and export it to new file since takes a while to run\n",
    "# ds_man.add_dataset(\n",
    "#     \"daily_climate_1_12_X_np\", filter_out_nans(\n",
    "#         spatial_data.xa_ds_to_3d_numpy(ds_man.get_dataset(\"daily_climate_1_12\").isel(depth=0)), ds_man.get_dataset(\"daily_climate_1_12_y_np\"))[0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this merge into data processing pipeline\n",
    "# merge daily mean files\n",
    "# var_daily_dir = Path(\"lustre_scratch/datasets/global_ocean_reanalysis/daily_means\")\n",
    "# save_combined_dailies_path = Path(\"lustre_scratch/datasets/global_ocean_reanalysis/daily_means/dailies_combined.nc\")\n",
    "# daily_file_paths = file_ops.return_list_filepaths(var_daily_dir, \".nc\")\n",
    "# combined_dailies = xa.open_mfdataset(daily_file_paths)\n",
    "# combined_dailies.to_netcdf(save_combined_dailies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create 3D array from xarray dataset variables. Shape: (num_samples, num_parameters, sequence_len)\n",
    "# X_with_nans = spatial_data.xa_ds_to_3d_numpy(xa_coral_climate_1_12_features)\n",
    "# print(f'X_with_nans shape (num_samples: {X_with_nans.shape[0]}, total num_parameters (includes nans parameters): {X_with_nans.shape[1]}, sequence_len: {X_with_nans.shape[2]})')\n",
    "\n",
    "# for i, param in enumerate(xa_coral_climate_1_12_features.data_vars):\n",
    "#     print(f\"{i}: {param}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations for which there are nan values\n",
    "\n",
    "99% sure these are are just gridcells containing land. Would be a good thing to investigate, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_with_nans\n",
    "# # problem, probably with sea ice features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out columns that contain entirely NaN values\n",
    "# col_mask = ~np.all(np.isnan(X), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "# masked_cols = X[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "# print(\"masked_cols shape:\", masked_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out all rows which contain any NaN values\n",
    "# row_mask = ~np.any(np.isnan(masked_cols), axis=1) # boolean mask indicating which rows to keep\n",
    "# masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "# masked_cols_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out all depths which contain any NaN values\n",
    "# depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "# X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "# X = np.swapaxes(X, 1, 2)\n",
    "# print(f\"X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create target from coral ground truth. Shape: (num_samples, 1)\n",
    "# # TODO: not sure if this is shuffling the values when reshaping\n",
    "# y_with_nans = np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].sel(\n",
    "#     time=xa_coral_climate_1_12.time[-1])).reshape(-1, 1)\n",
    "# # remove ys with nan values in other variables\n",
    "# y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "# print(f\"y_with_nans shape: {y_with_nans.shape}\")\n",
    "# print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = filter_out_nans(X_with_nans, np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1))\n",
    "# print(f\"X shape: {X.shape}\")\n",
    "# print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features = ds_man.get_dataset(\"monthly_climate_features\")\n",
    "xa_coral_climate_1_12 = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "\n",
    "xa_coral_climate_1_12_working = xa_coral_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_Xs_onehot, all_lat_lon_dict_onehot = sample_spatial_batch(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=(-20,13))\n",
    "# all_Xs_onehot, all_lat_lon_dict_onehot = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=((-20,13)))\n",
    "# all_Xs_onehot = naive_X_nan_replacement(all_Xs_onehot)\n",
    "# all_ys_onehot, _ = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=(-20,13), variables = [\"coral_algae_1-12_degree\"])\n",
    "# all_ys_onehot = naive_y_nan_replacement(all_ys_onehot)\n",
    "# all_ys_onehot = all_ys_onehot[:,:,0]\n",
    "\n",
    "train_onehot_Xs, train_onehot_ys, train_onehot_subsample, train_onehot_lat_lons_vals_dict = generate_patch(xa_ds=xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-6,6), onehot=False)\n",
    "test_onehot_Xs, test_onehot_ys, test_onehot_subsample, test_onehot_lat_lons_vals_dict = generate_patch(xa_ds=xa_coral_climate_1_12, lat_lon_starts=(-16,148), coord_range=(-6,6))\n",
    "\n",
    "print(\"train_onehot_Xs shape: \", train_onehot_Xs.shape)\n",
    "print(\"train_onehot_ys shape: \", train_onehot_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bathymetry\n",
    "bath_A = ds_man.get_dataset(\"bathymetry_A\")\n",
    "bath_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 km. Struggles displaying/processing 100m, but have yet to try saving to this/inferring\n",
    "target_resolution = 1000\n",
    "_,_,av_degrees = spatial_data.distance_to_degrees(target_resolution)\n",
    "bath_A_1km = spatial_data.upsample_xarray_to_target(bath_A, av_degrees)\n",
    "# im = bath_A_1km.plot(ax=ax)\n",
    "\n",
    "spatial_plots.plot_DEM(bath_A_1km, f\" DEM upsampled to {target_resolution} meters\", vmin=-100, vmax=0)\n",
    "# spatial_plots.format_spatial_plot(im, fig, ax, f\"Upsampled to {target_resolution} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def min_max_index_extreme_values(array, min_val=-100, max_val=0):\n",
    "#     # Find indices of values above min and below max values\n",
    "#     above_max_indices = np.where(array > max_val)\n",
    "#     below_min_indices = np.where(array < min_val)\n",
    "\n",
    "#     # Find the minimum and maximum indices among the above criteria\n",
    "#     lon_min_index = np.min(np.concatenate((above_max_indices[1], below_min_indices[1])))\n",
    "#     lat_min_index = np.max(np.concatenate((above_max_indices[1], below_min_indices[1])))\n",
    "#     return lon_min_index, lat_min_index\n",
    "\n",
    "\n",
    "# def slice_coast(bath_ds, num_slices=10, bathymetric_range: tuple[float] = (-100,0)):\n",
    "\n",
    "#     # lat_lims = spatial_data.xarray_coord_limits(bath_ds, \"latitude\")\n",
    "#     # could use values or indices\n",
    "\n",
    "\n",
    "#     # slice_ranges = np.linspace(lat_lims[0], lat_lims[1], num_slices)\n",
    "#     lat_num = len(list(bath_ds[\"latitude\"]))\n",
    "#     slice_ranges = np.arange(0, lat_num, (lat_num // num_slices))\n",
    "#     # slice_height = int(np.floor(np.abs(np.diff(lat_lims)) / num_slices))\n",
    "#     # for vertical centre of each slice, find limits of relevant bathymetry\n",
    "#     for i in range(len(slice_ranges)-1):\n",
    "#         slice_ds = bath_ds.isel(latitude=slice(slice_ranges[i],slice_ranges[i+1]))\n",
    "#         #don't think I need values here\n",
    "#         lon_min_index, lat_min_index = min_max_index_extreme_values(slice_ds.values)\n",
    "#         print(lon_min_index, lat_min_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_coast(bath_1_12_degree)\n",
    "chunk_size = 20\n",
    "vmin, vmax = -100, 0\n",
    "threshold_percent = 10\n",
    "chunk_coords = spatial_data.find_chunks_with_percentage(bath_A_1km.values[0,:,:], -100, 0, chunk_size, threshold_percent)\n",
    "\n",
    "print(\"array_shape\", bath_A_1km.values[0,:,:].shape)\n",
    "print(\"extreme chunk\", chunk_coords[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_coords[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bath_A_1km.isel(band=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vars_from_ds_or_da(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_subsample_from_coord(xa_ds, chunk_coords):\n",
    "    lat_start, lat_end = chunk_coords[0][0], chunk_coords[0][1]\n",
    "    lon_start, lon_end = chunk_coords[1][0], chunk_coords[1][1]\n",
    "\n",
    "    return xa_ds.isel({\"latitude\": slice(lat_start, lat_end), \"longitude\": slice(lon_start, lon_end)})\n",
    "\n",
    "def get_vars_from_ds_or_da(xa_d: xa.DataArray | xa.Dataset) -> str | list[str]:\n",
    "    if type(xa_d) == xa.core.dataarray.DataArray:\n",
    "        vars = xa_d.name\n",
    "    elif type(xa_d) == xa.core.dataarray.Dataset:\n",
    "        vars = list(xa_d.data_vars)\n",
    "    else:\n",
    "        raise TypeError(\"Format was neither an xarray Dataset nor a DataArray\")\n",
    "\n",
    "    return vars\n",
    "\n",
    "\n",
    "def nc_chunk_files(dest_dir_path: Path | str, xa_ds: xa.Dataset, chunk_size: int = 20, \n",
    "    threshold_percent: float=10, vmin: float=-100, vmax: float=0):\n",
    "    \n",
    "    chunk_coord_pairs = spatial_data.find_chunks_with_percentage(\n",
    "        xa_ds, vmin, vmax, chunk_size, threshold_percent)\n",
    "    \n",
    "    for coord_pair in chunk_coord_pairs:\n",
    "        sub_ds = ds_subsample_from_coord(xa_ds, coord_pair)\n",
    "        # make filename\n",
    "        vars = get_vars_from_ds_or_da(xa_ds)\n",
    "        # convert coord indices to absolute coords\n",
    "        filename = climate_data.generate_spatiotemporal_var_filename_from_dict(\n",
    "            {\"vars\": vars, \"lats\": coord_pair[0], \"lons\": coord_pair[1]})\n",
    "\n",
    "        # save file\n",
    "    print(filename)\n",
    "    return sub_ds\n",
    "\n",
    "nc_chunk_files(\"asdf\", bath_A_1km.isel(band=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "da = bath_A_1km\n",
    "da.plot(ax=ax, vmin=vmin, vmax=vmax, cmap=\"BrBG\")\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "for coord in chunk_coords:\n",
    "    xy = index_to_coord(da, coord[0])\n",
    "    height, width = delta_index_to_distance(da, coord[1], coord[0])\n",
    "    rect = patches.Rectangle(xy, width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt downsampling climate in each square\n",
    "# save as individual nc file\n",
    "# set up tf dataloader: load each nc file and take batches from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_1_12_degree = spatial_data.upsample_xarray_to_target(bath_A, 1/12)\n",
    "\n",
    "bath_1_12_degree.values[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_index_extreme_values(bath_1_12_degree.sel(latitude=slice(-12,-15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_1_12_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_climate.sel(latitude=slice(-10,-17.05), longitude=slice(141.95,147.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_band_bath = bath_1_12_degree.isel(band=0)\n",
    "\n",
    "# downsample climate data to 1km\n",
    "monthly_climate = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "\n",
    "# get limits of bathymetry\n",
    "lat_lims = spatial_data.xarray_coord_limits(bath_1_12_degree, \"latitude\")\n",
    "lon_lims = spatial_data.xarray_coord_limits(bath_1_12_degree, \"longitude\")\n",
    "\n",
    "restricted_monthly_climate = monthly_climate.sel(latitude=slice(-10,-17), longitude=slice(142,147))\n",
    "\n",
    "\n",
    "# padded_restricted_monthly_climate = spatial_data.buffer_nans(restricted_monthly_climate, size=1\n",
    "km_monthly = restricted_monthly_climate.interp_like(bath_1_12_degree, method=\"linear\")\n",
    "\n",
    "coral_climate_1km = xa.combine_by_coords([km_monthly.drop(\"spatial_ref\"),no_band_bath], coords=[\"time\", \"latitude\", \"longitude\"])\n",
    "(coral_climate_1km,) = xa.broadcast(coral_climate_1km)\n",
    "coral_climate_1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_monthly_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (13,) + inhomogeneous part.\n",
    "\n",
    "# train_comb_Xs, train_comb_ys, train_comb_subsample, train_comb_lat_lons_vals_dict = generate_patch(xa_ds=coral_climate_1km, lat_lon_starts=(-10,142), coord_range=(-6,6))\n",
    "# test_comb_Xs, test_comb_ys, test_comb_subsample, test_comb_lat_lons_vals_dict = generate_patch(xa_ds=coral_climate_1km, lat_lon_starts=(-16,148), coord_range=(-6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all_Xs_onehot shape: \", all_Xs_onehot.shape)\n",
    "print(\"all_ys_onehot shape: \", all_ys_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalise along variable axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Xs, all_lat_lon_dict = sample_spatial_batch(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=(-20,13))\n",
    "all_Xs, all_lat_lon_dict = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=((-20,13)))\n",
    "all_Xs = naive_X_nan_replacement(all_Xs)\n",
    "all_ys, _ = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5), variables = [\"coral_algae_1-12_degree\"])\n",
    "all_ys = naive_y_nan_replacement(all_ys)\n",
    "all_ys = all_ys[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xs shape: \", Xs.shape)\n",
    "print(\"ys shape: \", ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, ys, all_subsample, all_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "patch1_Xs, patch1_ys, patch1_subsample, patch1_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, (-10,142), (-7,5))\n",
    "patch2_Xs, patch2_ys, patch2_subsample, patch2_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, (-17,147), (-7,5))\n",
    "patch3_Xs, patch3_ys, patch3_subsample, patch3_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, (-16,146), (-7,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch(\n",
    "    xa_ds: xa.DataArray | xa.Dataset,\n",
    "    lat_lon_starts: tuple[float, float],\n",
    "    coord_range: tuple[float, float],\n",
    "    window_dims: tuple[int, int] = None,\n",
    "    feature_vars: list[str] = [\"bottomT\", \"so\", \"mlotst\", \"uo\", \"vo\", \"zos\", \"thetao\"],\n",
    "    gt_var: str = \"coral_algae_1-12_degree\",\n",
    "    normalise: bool = True,\n",
    "    onehot: bool = True,\n",
    "):\n",
    "    \"\"\"Generate a patch for training or evaluation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xa_ds (xa.Dataset): The input xarray dataset.\n",
    "    lat_lon_starts (tuple): The starting latitude and longitude indices for sampling the patch.\n",
    "    coord_range (tuple): The latitude and longitude range for sampling the patch.\n",
    "    feature_vars (list[str], optional): List of variable names to be used as features.\n",
    "        Default is [\"bottomT\", \"so\", \"mlotst\", \"uo\", \"vo\", \"zos\", \"thetao\"].\n",
    "    gt_var (str, optional): The variable name for the ground truth. Default is \"coral_algae_1-12_degree\".\n",
    "    normalise (bool, optional): Flag indicating whether to normalize each variable between 0 and 1. Default is True.\n",
    "    onehot (bool, optional): Flag indicating whether to encode NaN values using the one-hot method. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: A tuple containing the feature array, ground truth array, subsampled dataset, and latitude/longitude values.\n",
    "    \"\"\"\n",
    "    subsample, lat_lon_vals_dict = sample_spatial_batch(\n",
    "        xa_ds,\n",
    "        lat_lon_starts=lat_lon_starts,\n",
    "        coord_range=coord_range,\n",
    "        window_dims=window_dims,\n",
    "    )\n",
    "\n",
    "    output = process_xa_ds_for_ml(\n",
    "        xa_ds=subsample,\n",
    "        feature_vars=feature_vars,\n",
    "        gt_var=gt_var,\n",
    "        normalise=normalise,\n",
    "        onehot=onehot,\n",
    "    )\n",
    "\n",
    "    return output, subsample, lat_lon_vals_dict\n",
    "\n",
    "\n",
    "def process_xa_ds_for_ml(\n",
    "    xa_ds: xa.Dataset,\n",
    "    feature_vars: list[str] = None,\n",
    "    gt_var: str = None,\n",
    "    normalise: bool = True,\n",
    "    onehot: bool = True,\n",
    ") -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"\n",
    "    Process xarray Dataset for machine learning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xa_ds : xa.Dataset\n",
    "        The input xarray dataset.\n",
    "    feature_vars : list[str], optional\n",
    "        List of variable names to be used as features. Default is None.\n",
    "    gt_var : str, optional\n",
    "        The variable name for the ground truth. Default is None.\n",
    "    normalise : bool, optional\n",
    "        Flag indicating whether to normalize each variable between 0 and 1. Default is True.\n",
    "    onehot : bool, optional\n",
    "        Flag indicating whether to encode NaN values using the one-hot method. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.ndarray, ...]\n",
    "        A tuple containing the feature array and ground truth array.\n",
    "    \"\"\"\n",
    "    to_return = []\n",
    "    if feature_vars is not None:\n",
    "        # assign features and convert to lat, lon to latxlon column\n",
    "        Xs = spatial_array_to_column(xa_d_to_np_array(xa_ds[feature_vars]))\n",
    "\n",
    "        # if normalise = True, normalise each variable between 0 and 1\n",
    "        if normalise:\n",
    "            Xs = normalise_3d_array(Xs)\n",
    "        # remove columns containing only nans. TODO: enable removal of all nan dims\n",
    "        nans_array = exclude_all_nan_dim(Xs, dim=1)\n",
    "\n",
    "        # if encoding nans using onehot method\n",
    "        if onehot:\n",
    "            Xs = encode_nans_one_hot(nans_array)\n",
    "        to_return.append(naive_nan_replacement(Xs))\n",
    "\n",
    "    if gt_var:\n",
    "        # assign ground truth and convert to column vector\n",
    "        ys = spatial_array_to_column(xa_d_to_np_array(xa_ds[gt_var]))\n",
    "        # take single time slice (since broadcasted back through time)\n",
    "        ys = ys[:, 0]\n",
    "        to_return.append(ys)\n",
    "\n",
    "    return tuple(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_Xs, daily_ys, daily_subsample, daily_lat_lons_vals_dict = spatial_data.generate_patch(xa_ds=daily_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = daily_Xs[:]\n",
    "y_train = daily_ys[:]\n",
    "\n",
    "wandb.init(\n",
    "    project=\"coralshift\",\n",
    "    entity=\"orlando-code\",\n",
    "    settings=wandb.Settings(start_method=\"fork\")\n",
    "    # config={    }\n",
    "    )\n",
    "\n",
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "# X = ds_man.get_dataset(\"monthly_climate_1_12_X_np\")\n",
    "# y = ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    "# # check that untrained model runs (should output array of non-nan values)\n",
    "# # why values change?\n",
    "# # g_model(X[:32])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     sub_X, sub_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class gru_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, rnn_units: list[int], num_layers: int, \n",
    "        # dff: int\n",
    "        ):\n",
    "        \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "        layers to a desired output shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "        num_layers (int): number of layers in GRU model\n",
    "        \"\"\"\n",
    "        super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "        # store values for later use\n",
    "        self.num_layers = num_layers    # number of layers in GRU model\n",
    "        self.rnn_units = rnn_units\n",
    "        # self.dff = dff\n",
    "        # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "        self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "                                   return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "                                   return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "        # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "        # self.w1 = tf.keras.layers.Dense(dff) # 10 units\n",
    "        self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "\n",
    "        self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "        # self.A = tf.keras.layers.Dense(30)\n",
    "        # self.B = tf.keras.layers.Dense(dff)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        # whole_seq, static_input = inputs\n",
    "        whole_seq = inputs\n",
    "\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwritting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        # static = self.B(tf.nn.gelu(self.A(static_input)))\n",
    "        # target = self.w1(final_s)  + static # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "\n",
    "# initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([500], 1) # N.B. [x] is number of hidden layers in GRU network\n",
    "\n",
    "\n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if class_weights is not None:\n",
    "        sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "        return bce(y, y_pred, sample_weight=sample_weights)\n",
    "\n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def training_batches(X: np.ndarray, y: np.ndarray, batch_num: int, batch_size: int=32):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "    X_batch = X[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "# https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "# should aim to delete the following to speed up training: but can't figure out a way to make wandb reporting work\n",
    "# without it\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: np.ndarray, y: np.ndarray, \n",
    "        training: bool=True, class_weights=class_weights, batch_num:int=None, batch_size: int=None) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            num_samples = X.shape[0]\n",
    "            num_batches = num_samples // batch_size\n",
    "            # num_batches = batch_num\n",
    "            total_epoch_loss = 0.0\n",
    "            for batch_num in tqdm(range(num_batches), desc=\"batches\", position=0, leave=True):\n",
    "                X_batch, y_batch = training_batches(X, y, batch_num=batch_num, batch_size=batch_size)\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = gru(X_batch, training) \n",
    "                    xent = negative_log_likelihood(y_batch, y_pred, class_weights)\n",
    "                    # y_pred = gru(X, training) # TO DELETE\n",
    "                    # xent = negative_log_likelihood(y, y_pred)\n",
    "                \n",
    "                gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "                # print(\"xent\", xent.numpy())\n",
    "                # print(\"total_epoch_loss\", total_epoch_loss)\n",
    "                total_epoch_loss += xent\n",
    "                # learning rate?\n",
    "                wandb.log({\"batch\": batch_num, \"loss\": xent, \"total_epoch_loss\": total_epoch_loss})\n",
    "\n",
    "            average_loss = total_epoch_loss / num_batches\n",
    "            # return predicted output values and total loss value\n",
    "            return y_pred, xent, total_epoch_loss\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    # TODO: this isn't assigned... What should it return otherwise?\n",
    "    return train_step\n",
    "\n",
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=np.reshape(y_train,-1))\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    num_epochs = 50\n",
    "    # will update so that subsamples are fed in from which batches are taken: will require recomputation\n",
    "    # of class_weight for each subsample\n",
    "    num_batches = 2\n",
    "    batch_size = 512\n",
    "    tr_step = build_graph()\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position = 0, leave=True):\n",
    "        y_pred, xent, total_epoch_loss = tr_step(\n",
    "            g_model, optimizer, X_train[:], y_train[:], class_weights=class_weights, \n",
    "            batch_size=batch_size, batch_num=num_batches, training=True)\n",
    "        wandb.log({\"epoch\": epoch, \"epoch_loss\": total_epoch_loss})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# check (with prints) that wandb is functioning\n",
    "# check against known timeseries task for correct implementation\n",
    "# find timeseries which get bad loss and debug why\n",
    "# log best loss which can be logged: save weights (and do a inference plot with best weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train/predict\n",
    "# if training:\n",
    "    # assign epoch output to dataset\n",
    "# if testing:\n",
    "# patch1_pred = g_model(patch1_Xs, training=False)\n",
    "# patch2_pred = g_model(patch2_Xs, training=False)\n",
    "# patch3_pred = g_model(patch3_Xs, training=False)\n",
    "pred = g_model(test_onehot_Xs, training=False)\n",
    "\n",
    "# all_predicted = g_model(all_Xs, training=False)\n",
    "    # assign output predictions to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pred_xa_ds_onehot_lim = reformat_prediction(xa_coral_climate_1_12_working, test_onehot_subsample, pred, test_onehot_lat_lons_vals_dict)\n",
    "\n",
    "mask = patch_pred_xa_ds_onehot_lim[\"output\"] > 0.6\n",
    "\n",
    "spatial_plots.plot_var(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pred_xa_ds_onehot_lim = reformat_prediction(xa_coral_climate_1_12_working, test_onehot_subsample, pred, test_onehot_lat_lons_vals_dict)\n",
    "\n",
    "spatial_plots.plot_var(patch_pred_xa_ds_onehot_lim[\"output\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_Xs_onehot, patch_ys_onehot, patch_subsample_onehot, patch_lat_lons_vals_dict_onehot = generate_patch(xa_coral_climate_1_12, (-10,142), (-7,5))\n",
    "\n",
    "patch_pred_xa_ds_onehot = reformat_prediction(xa_coral_climate_1_12_working, patch_subsample_onehot, pred, patch_lat_lons_vals_dict_onehot)\n",
    "\n",
    "spatial_plots.plot_var(patch_pred_xa_ds_onehot[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(patch1_pred_xa_ds[\"output\"].values,patch2_pred_xa_ds[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch1_pred_xa_ds = reformat_prediction(xa_coral_climate_1_12_working, patch1_subsample, patch1_pred, patch1_lat_lons_vals_dict)\n",
    "# patch2_pred_xa_ds = reformat_prediction(xa_coral_climate_1_12_working, patch2_subsample, patch2_pred, patch2_lat_lons_vals_dict)\n",
    "# patch3_pred_xa_ds = reformat_prediction(xa_coral_climate_1_12_working, patch3_subsample, patch3_pred, patch3_lat_lons_vals_dict)\n",
    "\n",
    "\n",
    "f, a0 = spatial_plots.plot_var_at_time(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"], \"2020-12-16\")\n",
    "# # visualise result\n",
    "# spatial_plots.plot_var(patch1_pred_xa_ds[\"output\"])\n",
    "# spatial_plots.plot_var(patch2_pred_xa_ds[\"output\"])\n",
    "# spatial_plots.plot_var(patch3_pred_xa_ds[\"output\"])\n",
    "\n",
    "# pred_xa_ds[\"coral_algae_1-12_degree\"].isel(time=-1).plot(ax=ax[0])\n",
    "# pred_xa_ds[\"output\"].plot(ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"bottomT\"].isel(time=-1).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample climate data to 1km\n",
    "monthly_climate = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "\n",
    "# get limits of bathymetry\n",
    "lat_lims = spatial_data.xarray_coord_limits(coarsened_bath_A, \"latitude\")\n",
    "lon_lims = spatial_data.xarray_coord_limits(coarsened_bath_A, \"longitude\")\n",
    "\n",
    "restricted_monthly_climate = monthly_climate.sel(latitude=slice(-10,-17), longitude=slice(142,147))\n",
    "\n",
    "\n",
    "# padded_restricted_monthly_climate = spatial_data.buffer_nans(restricted_monthly_climate, size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 3\n",
    "exclude_vars = [\"spatial_ref\", \"coral_algae_1-12_degree\", \"siconc\", \"usi\", \"vsi\", \"sithick\"]\n",
    "buffered_ds = spatially_buffer_timeseries(monthly_climate, buffer_size=buffer_size, exclude_vars=exclude_vars)\n",
    "\n",
    "buffered_ds.to_netcdf(\n",
    "    ds_man.get_location() / f\"global_ocean_reanalysis/monthly_means/monthly_climate_{buffer_size}_buffer.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a = plt.subplots(1,2, figsize=[10,5])\n",
    "monthly_climate[\"mlotst\"].isel(time=99).plot(ax=a[0], cmap=\"jet\")\n",
    "buffer_attempt[\"mlotst\"].isel(time=99).plot(ax=a[1],cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_climate_1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_attempt.equals(monthly_climate.isel(time=slice(0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_attempt.isel(time=1)[\"mlotst\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsened_bath_A.isel(band=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_climate_1km[\"bathymetry_A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a = plt.subplots(1,2, figsize=[10,5])\n",
    "coral_climate_1km.isel(time=-1)[\"bathymetry_A\"].plot(ax=a[0], vmin=-100, vmax=0)\n",
    "coral_climate_1km.isel(time=-1)[\"mlotst\"].plot(ax=a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt.isel(time=-1)[\"mlotst\"].plot()\n",
    "eg_data = buffer_attempt.isel(time=-1)[\"mlotst\"]\n",
    "\n",
    "spatial_plots.plot_DEM(eg_data, f\" DEM upsampled to {target_resolution} meters\", \n",
    "    landmask=False, vmin=np.nanmin(eg_data.values), vmax=np.nanmax(eg_data.values), cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "\n",
    "# for longitude in array\n",
    "# get \n",
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: optionally replace batching with spatial batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GRU functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    num_epochs = 5\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position = 0):\n",
    "        y_pred, average_loss = tr_step(g_model, optimizer, X_train[:1000], y_train[:1000], batch_size=32, training=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for batch in range(num_batches):\n",
    "        #     array, y  = batcher_fun(X, 32, 276 \n",
    "        #     #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "        #     )\n",
    "        #     y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "        #  ## validation set \n",
    "        #  ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_log_likelihood(y_test,g_model(X_test))\n",
    "y_test=y_test[:1000]\n",
    "y_pred = g_model(X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made unnecssary due to isel indexing\n",
    "# def pixels_to_coord_diff(xa_ds: xa.Dataset | xa.DataArray, window_dim: int, coord: str) -> list[float, float]:\n",
    "#     return float(window_dim * np.diff(spatial_data.min_max_of_coords(xa_ds, coord)) / len(list(xa_coral_climate_1_12[coord])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample[\"bottomT\"].isel(time=-1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-16,144), coord_range=(-4,2))[\"coral_algae_1-12_degree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_working = xa_coral_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [lat_lon_vals_dict.items() for key in [\"latitude\", \"longitude\"]]\n",
    "{key: lat_lon_vals_dict[key] for key in [\"latitude\", \"longitude\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_y_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(list(subsample.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_X = np.moveaxis(np.array(test_array), 2, 1)\n",
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mask = ~np.all(np.isnan(test_array), axis=(0,2))\n",
    "sub_X = test_array[:, col_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features = ds_man.get_dataset(\"monthly_climate_features\")\n",
    "xa_coral_climate_1_12 = ds_man.get_dataset(\"monthly_climate_1_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, lat_lon_dict = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "Xs_nonans = naive_X_nan_replacement(Xs)\n",
    "ys, _ = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5), variables = [\"coral_algae_1-12_degree\"])\n",
    "ys = ys[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subset\n",
    "# subsample, lat_lon_vals_dict = sample_spatial_batch(xa_coral_climate_1_12_features,lat_lon_starts=(-12,144), coord_range=(-4,2))\n",
    "# subsample, lat_lon_vals_dict = sample_spatial_batch(xa_coral_climate_1_12_features,lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "# convert to ndarray\n",
    "# # test_array = spatial_data.xa_ds_to_3d_numpy(subsample)\n",
    "# # subsample_all, _ = sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-16,144), coord_range=(-4,2))\n",
    "# subsample_all, _ = sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "# sub_y_nans = (np.array(subsample_all[\"coral_algae_1-12_degree\"].isel(time=-1))).reshape(-1, 1)\n",
    "# # remove nans\n",
    "# #sub_X, sub_y = filter_out_nans(test_array, sub_y_nans)\n",
    "# # testing, so replace nans with -1\n",
    "# # filter out columns that contain entirely NaN values\n",
    "# # col_mask = ~np.all(np.isnan(test_array), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "# # sub_X = test_array[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "\n",
    "# # sub_X = np.moveaxis(np.array(sub_X), 2, 1)\n",
    "# sub_y = sub_y_nans\n",
    "# # sub_X[np.isnan(sub_X)] = -10000\n",
    "# sub_y[np.isnan(sub_y)] = -10000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_dataset_var(xa_ds: xa.Dataset | xa.DataArray, subset_vals: np.ndarray, dims: list=['latitude', 'longitude', \"time\"]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_train = np.append(100*np.ones((50,50,5)), 1*np.ones((50,50,5)), 0)\n",
    "test_y_train = np.append(np.ones(50,), np.zeros(50,))\n",
    "\n",
    "\n",
    "print(\"test_x_train:\", test_x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"test_y_train:\", test_y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.normal(size = (32, 20, 1))    # shape: (num_samples, sequence_length, num_features)\n",
    "y_dud = np.random.choice([0, 1], size = 32)\n",
    "print(\"array shape:\", array.shape)\n",
    "print(\"y_dud shape:\", y_dud.shape)\n",
    "\n",
    "x_train, y_train = X[:500], y[:500].reshape(500,)\n",
    "# x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_model(test_x_train[:],training=False).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = g_model(X[:5610],training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30*187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(predicted.numpy().reshape(30,187))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(y_pred.numpy().reshape(20,25))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((y_pred > 0.5).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check log likelihood is computable\n",
    "negative_log_likelihood(y[:32], g_model(X[:32]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batcher function (by space and time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher_fun(X, y, batch_size, seq_len):\n",
    "    \"\"\"\n",
    "    A function to prepare the data for training the LSTM.\n",
    "    \n",
    "    :param data: The input data to the LSTM.\n",
    "    :param batch_size: The number of samples in each batch.\n",
    "    :param seq_len: The sequence length of each sample.\n",
    "    \n",
    "    :return: A tuple of (batch_x, batch_y), where batch_x is a numpy array of shape (batch_size, seq_len, num_features) \n",
    "             and batch_y is a numpy array of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(num_samples / batch_size)\n",
    "    num_features = spatial_data.shape[1]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        \n",
    "        batch_x = np.zeros((batch_size, seq_len, num_features))\n",
    "        batch_y = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            sample = data[j]\n",
    "            X = sample[:-1]\n",
    "            y = y[]\n",
    "            \n",
    "            batch_x[j - start_idx] = x.reshape((seq_len, num_features))\n",
    "            batch_y[j - start_idx, y] = 1\n",
    "            \n",
    "        yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 1\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            array, y  = batcher_fun(X, 32, 276 \n",
    "            #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "            )\n",
    "            y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "         ## validation set \n",
    "         ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copypasta\n",
    "\n",
    "[Source](https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam # https://stackoverflow.com/questions/75356826/attributeerror-adam-object-has-no-attribute-get-updates\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Model configuration\n",
    "additional_metrics = [\"accuracy\"]\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "# embedding_output_dims = 15\n",
    "# embedding_output_dims = 10\n",
    "loss_function = BinaryCrossentropy()\n",
    "# max_sequence_length = 300\n",
    "max_sequence_length = 276\n",
    "# num_distinct_words = 5000\n",
    "# num_distinct_words = 10000\n",
    "number_of_epochs = 5\n",
    "optimizer = Adam()\n",
    "validation_split = 0.20\n",
    "verbosity_mode = 1\n",
    "\n",
    "# Disable eager execution\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\n",
    "x_train, y_train = X[:5000], y[:5000].reshape((5000,))\n",
    "x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad all sequences: keras requires sequences of equal lengths. Should be handled in pre-processing, but here for now for security\n",
    "padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "\n",
    "# (number_samples, sequence_length, num_features)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = pad_sequences(x_train[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "padded_inputs_test = pad_sequences(x_test[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "\n",
    "# (number_samples, sequence_length)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        num_distinct_words+1, embedding_output_dims, input_length=max_sequence_length\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
    "\n",
    "# Give a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    padded_inputs,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=number_of_epochs,\n",
    "    verbose=verbosity_mode,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
    "print(f\"Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_timeseries(history) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history[\"accuracy\"])\n",
    "    ax.plot(history.history[\"val_accuracy\"])\n",
    "\n",
    "    ax.set_title(\"Model accuracy against epoch\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend(['train set', 'validation set'], loc='upper left')\n",
    "\n",
    "plot_score_timeseries(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate model\n",
    "\n",
    "[Source](https://medium.com/@canerkilinc/hands-on-multivariate-time-series-sequence-to-sequence-predictions-with-lstm-tensorflow-keras-ce86f2c0e4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = X[:32*10,:10,:3]\n",
    "print(\"X_toy:\", X_toy.shape)\n",
    "y_toy = y[:32*10]\n",
    "print(\"y_toy:\", y_toy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "#####################################\n",
    "#Before do anything else do not forget to reset the backend for the next iteration (rerun the model)\n",
    "tensorflow.keras.backend.clear_session()\n",
    "#####################################\n",
    "# Initialising the LSTM Model with MAE Loss-Function\n",
    "batch_size = 32\n",
    "epochs = 120\n",
    "timesteps = 10\n",
    "num_features = 3\n",
    "input_1 = Input(batch_shape=(batch_size,timesteps,num_features))\n",
    "#each layer is the input of the next layer\n",
    "lstm_hidden_layer_1 = LSTM(10, stateful=True, return_sequences=True)(input_1)\n",
    "lstm_hidden_layer_2 = LSTM(10, stateful=True, return_sequences=True)(lstm_hidden_layer_1)\n",
    "output_1 = Dense(units = 1)(lstm_hidden_layer_2)\n",
    "regressor_mae = Model(inputs=input_1, outputs = output_1)\n",
    "#adam is fast starting off and then gets slower and more precise\n",
    "#mae -> mean absolute error loss function\n",
    "regressor_mae.compile(optimizer='adam', loss = 'mae')\n",
    "#####################################\n",
    "#Summarize and observe the layers as well as paramter configurations\n",
    "regressor_mae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_mae.fit(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralshift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

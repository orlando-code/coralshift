{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "# TODO: hacky, shouldn't be necessary\n",
    "os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"lustre_scratch/coralshift/notebooks/rnn.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.interpolate import interp2d\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray as rio\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories\n",
    "from coralshift.plotting import spatial_plots\n",
    "from coralshift.dataloading import data_structure, climate_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting xarray Dataset to numpy arrays: 100%|██████████| 13/13 [00:00<00:00, 17583.34it/s]\n"
     ]
    }
   ],
   "source": [
    "ds_man = data_structure.MyDatasets()\n",
    "\n",
    "# add datasets\n",
    "ds_man.set_location(\"remote\")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12\", xa.open_dataset(\n",
    "        ds_man.get_location() / \"global_ocean_reanalysis/monthly_means/coral_climate_1_12.nc\")\n",
    ")\n",
    "\n",
    "coral_climate_feature_vars = list(\n",
    "    set(ds_man.get_dataset(\"monthly_climate_1_12\").data_vars) - {'spatial_ref', 'coral_algae_1-12_degree', 'output'})\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_features\", ds_man.get_dataset(\"monthly_climate_1_12\")[coral_climate_feature_vars]\n",
    ")\n",
    "\n",
    "# TODO: sort numpy assignment with new one-hot encoding and noormalisation\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12_X_y_np\", spatial_data.filter_out_nans(\n",
    "        spatial_data.xa_ds_to_3d_numpy(ds_man.get_dataset(\"monthly_climate_1_12\")), \n",
    "        np.array(ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1))\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12_X_np\", ds_man.get_dataset(\"monthly_climate_1_12_X_y_np\")[0]\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12_y_np\", ds_man.get_dataset(\"monthly_climate_1_12_X_y_np\")[1]\n",
    ")\n",
    "\n",
    "# TODO: handle depth\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12\", xa.open_dataset(\n",
    "        Path(ds_man.get_location() / \"global_ocean_reanalysis/daily_means/dailies_combined.nc\")).isel(depth=0)\n",
    ")\n",
    "\n",
    "# TODO: save as file\n",
    "daily_climate_1_12_X_file_path = ds_man.get_location() / \"global_ocean_reanalysis/daily_means/daily_climate_1_12_X.npy\"\n",
    "# if daily_climate_1_12_X numpy array doesn't exist, generate and save\n",
    "if not check_file_exists(filepath = daily_climate_1_12_X_file_path):\n",
    "    daily_climate_1_12_X = spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"daily_climate_1_12\"),\n",
    "        feature_vars = list((ds_man.get_dataset(\"daily_climate_1_12\").data_vars)))\n",
    "    np.save(daily_climate_1_12_X_file_path, daily_climate_1_12_X) \n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "else:\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "\n",
    "# same target as monthly\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12_y\", ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"bathymetry_A\", rio.open_rasterio(\n",
    "        rasterio.open(ds_man.get_location() / \"bathymetry/GBR_30m/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif\"),\n",
    "        ).rename(\"bathymetry_A\").rename({\"x\": \"longitude\", \"y\": \"latitude\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# ds_man.get_dataset(\"daily_climate_1_12_X\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds_man\u001b[39m.\u001b[39;49mget_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mdaily_climate_1_12_y\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# ds_man.get_dataset(\"daily_climate_1_12_X\")\n",
    "ds_man.get_dataset(\"daily_climate_1_12_y\").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def xa_ds_to_X_y(xa_ds: xa.Dataset, \n",
    "#     feature_vars: list[str], gt_var: str, normalise: bool=True, onehot: bool=True):\n",
    "#     \"\"\"Process xarray Dataset for machine learning\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     xa_ds (xa.Dataset): The input xarray dataset.\n",
    "#     lat_lon_starts (tuple): The starting latitude and longitude indices for sampling the patch.\n",
    "#     coord_range (tuple): The latitude and longitude range for sampling the patch.\n",
    "#     feature_vars (list[str], optional): List of variable names to be used as features.\n",
    "#         Default is [\"bottomT\", \"so\", \"mlotst\", \"uo\", \"vo\", \"zos\", \"thetao\"].\n",
    "#     gt_var (str, optional): The variable name for the ground truth. Default is \"coral_algae_1-12_degree\".\n",
    "#     normalise (bool, optional): Flag indicating whether to normalize each variable between 0 and 1. Default is True.\n",
    "#     onehot (bool, optional): Flag indicating whether to encode NaN values using the one-hot method. Default is True.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     tuple: A tuple containing the feature array and ground truth array\n",
    "#     \"\"\"\n",
    "#     # assign features\n",
    "#     Xs = xa_d_to_np_array(xa_ds[feature_vars])\n",
    "#     # assign ground truth\n",
    "#     ys = xa_d_to_np_array(xa_ds[gt_var])\n",
    "\n",
    "#     # convert to column vectors\n",
    "#     Xs, ys = spatial_array_to_column(Xs), spatial_array_to_column(ys)\n",
    "\n",
    "#     # if normalise = True, normalise each variable between 0 and 1\n",
    "#     if normalise:\n",
    "#         Xs = normalise_3d_array(Xs)\n",
    "\n",
    "#     # remove columns containing only nans. TODO: enable all nan dims\n",
    "#     nans_array = exclude_all_nan_dim(Xs, dim=1)\n",
    "\n",
    "#     # if encoding nans using onehot method\n",
    "#     if onehot:\n",
    "#         Xs = encode_nans_one_hot(nans_array)\n",
    "#     Xs = naive_nan_replacement(Xs)\n",
    "\n",
    "#     # this shouldn't ever be necessary\n",
    "#     ys = naive_nan_replacement(ys)\n",
    "#     # take single time slice (since broadcasted back through time)\n",
    "#     ys = ys[:, 0]\n",
    "\n",
    "#     return Xs, ys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (latitude: 85, time: 9863, longitude: 61)\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float32 -17.0 -16.92 -16.83 ... -10.17 -10.08 -10.0\n",
       "  * time       (time) datetime64[ns] 1993-12-31T12:00:00 ... 2020-12-31T12:00:00\n",
       "  * longitude  (longitude) float32 142.0 142.1 142.2 142.2 ... 146.8 146.9 147.0\n",
       "    depth      float32 0.494\n",
       "Data variables:\n",
       "    bottomT    (time, latitude, longitude) float32 ...\n",
       "    mlotst     (time, latitude, longitude) float32 ...\n",
       "    so         (time, latitude, longitude) float32 ...\n",
       "    thetao     (time, latitude, longitude) float32 ...\n",
       "    uo         (time, latitude, longitude) float32 ...\n",
       "    vo         (time, latitude, longitude) float32 ...\n",
       "    zos        (time, latitude, longitude) float32 ...\n",
       "Attributes: (12/25)\n",
       "    title:                              daily mean fields from Global Ocean P...\n",
       "    easting:                            longitude\n",
       "    northing:                           latitude\n",
       "    history:                            2022/05/25 21:54:07 MERCATOR OCEAN Ne...\n",
       "    source:                             MERCATOR GLORYS12V1\n",
       "    institution:                        MERCATOR OCEAN\n",
       "    ...                                 ...\n",
       "    FROM_ORIGINAL_FILE__longitude_max:  179.91667\n",
       "    FROM_ORIGINAL_FILE__latitude_min:   -80.0\n",
       "    FROM_ORIGINAL_FILE__latitude_max:   90.0\n",
       "    z_min:                              0.494025\n",
       "    z_max:                              5727.917\n",
       "    _CoordSysBuilder:                   ucar.nc2.dataset.conv.CF1Convention</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-3fa4e34a-dc4e-4823-9c6c-a50dcb207845' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-3fa4e34a-dc4e-4823-9c6c-a50dcb207845' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>latitude</span>: 85</li><li><span class='xr-has-index'>time</span>: 9863</li><li><span class='xr-has-index'>longitude</span>: 61</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-ec589add-0a45-467d-ae2c-24103a797d28' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ec589add-0a45-467d-ae2c-24103a797d28' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-17.0 -16.92 ... -10.08 -10.0</div><input id='attrs-ee376152-cdec-40fe-bf0e-675125904d6f' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ee376152-cdec-40fe-bf0e-675125904d6f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b7cd41a0-e8fd-46e5-8501-3301c2b934c6' class='xr-var-data-in' type='checkbox'><label for='data-b7cd41a0-e8fd-46e5-8501-3301c2b934c6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>valid_min :</span></dt><dd>-17.0</dd><dt><span>valid_max :</span></dt><dd>-10.0</dd><dt><span>step :</span></dt><dd>0.08333588</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>unit_long :</span></dt><dd>Degrees North</dd><dt><span>long_name :</span></dt><dd>Latitude</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>axis :</span></dt><dd>Y</dd><dt><span>_ChunkSizes :</span></dt><dd>2041</dd><dt><span>_CoordinateAxisType :</span></dt><dd>Lat</dd></dl></div><div class='xr-var-data'><pre>array([-17.      , -16.916666, -16.833334, -16.75    , -16.666666, -16.583334,\n",
       "       -16.5     , -16.416666, -16.333334, -16.25    , -16.166666, -16.083334,\n",
       "       -16.      , -15.916667, -15.833333, -15.75    , -15.666667, -15.583333,\n",
       "       -15.5     , -15.416667, -15.333333, -15.25    , -15.166667, -15.083333,\n",
       "       -15.      , -14.916667, -14.833333, -14.75    , -14.666667, -14.583333,\n",
       "       -14.5     , -14.416667, -14.333333, -14.25    , -14.166667, -14.083333,\n",
       "       -14.      , -13.916667, -13.833333, -13.75    , -13.666667, -13.583333,\n",
       "       -13.5     , -13.416667, -13.333333, -13.25    , -13.166667, -13.083333,\n",
       "       -13.      , -12.916667, -12.833333, -12.75    , -12.666667, -12.583333,\n",
       "       -12.5     , -12.416667, -12.333333, -12.25    , -12.166667, -12.083333,\n",
       "       -12.      , -11.916667, -11.833333, -11.75    , -11.666667, -11.583333,\n",
       "       -11.5     , -11.416667, -11.333333, -11.25    , -11.166667, -11.083333,\n",
       "       -11.      , -10.916667, -10.833333, -10.75    , -10.666667, -10.583333,\n",
       "       -10.5     , -10.416667, -10.333333, -10.25    , -10.166667, -10.083333,\n",
       "       -10.      ], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1993-12-31T12:00:00 ... 2020-12-...</div><input id='attrs-9e940efb-442f-4372-b281-a9f9fa7b1c09' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9e940efb-442f-4372-b281-a9f9fa7b1c09' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b89b44de-0d5b-4437-9754-e7b7b5e0c3b9' class='xr-var-data-in' type='checkbox'><label for='data-b89b44de-0d5b-4437-9754-e7b7b5e0c3b9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Time (hours since 1950-01-01)</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>_ChunkSizes :</span></dt><dd>1024</dd><dt><span>_CoordinateAxisType :</span></dt><dd>Time</dd><dt><span>valid_min :</span></dt><dd>385692.0</dd><dt><span>valid_max :</span></dt><dd>473316.0</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;1993-12-31T12:00:00.000000000&#x27;, &#x27;1994-01-01T12:00:00.000000000&#x27;,\n",
       "       &#x27;1994-01-02T12:00:00.000000000&#x27;, ..., &#x27;2020-12-29T12:00:00.000000000&#x27;,\n",
       "       &#x27;2020-12-30T12:00:00.000000000&#x27;, &#x27;2020-12-31T12:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>142.0 142.1 142.2 ... 146.9 147.0</div><input id='attrs-7ebec7da-0015-4abe-a034-0d569e471f3f' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-7ebec7da-0015-4abe-a034-0d569e471f3f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b3af942d-47a5-4e13-a2f6-e788efe58e97' class='xr-var-data-in' type='checkbox'><label for='data-b3af942d-47a5-4e13-a2f6-e788efe58e97' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>valid_min :</span></dt><dd>142.0</dd><dt><span>valid_max :</span></dt><dd>147.0</dd><dt><span>step :</span></dt><dd>0.08332825</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>unit_long :</span></dt><dd>Degrees East</dd><dt><span>long_name :</span></dt><dd>Longitude</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>axis :</span></dt><dd>X</dd><dt><span>_ChunkSizes :</span></dt><dd>4320</dd><dt><span>_CoordinateAxisType :</span></dt><dd>Lon</dd></dl></div><div class='xr-var-data'><pre>array([142.     , 142.08333, 142.16667, 142.25   , 142.33333, 142.41667,\n",
       "       142.5    , 142.58333, 142.66667, 142.75   , 142.83333, 142.91667,\n",
       "       143.     , 143.08333, 143.16667, 143.25   , 143.33333, 143.41667,\n",
       "       143.5    , 143.58333, 143.66667, 143.75   , 143.83333, 143.91667,\n",
       "       144.     , 144.08333, 144.16667, 144.25   , 144.33333, 144.41667,\n",
       "       144.5    , 144.58333, 144.66667, 144.75   , 144.83333, 144.91667,\n",
       "       145.     , 145.08333, 145.16667, 145.25   , 145.33333, 145.41667,\n",
       "       145.5    , 145.58333, 145.66667, 145.75   , 145.83333, 145.91667,\n",
       "       146.     , 146.08333, 146.16667, 146.25   , 146.33333, 146.41667,\n",
       "       146.5    , 146.58333, 146.66667, 146.75   , 146.83333, 146.91667,\n",
       "       147.     ], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>depth</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.494</div><input id='attrs-196415c4-7a1d-43b4-b168-8417f64c5b9a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-196415c4-7a1d-43b4-b168-8417f64c5b9a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-225cd988-e257-479f-8036-827c827b6bd0' class='xr-var-data-in' type='checkbox'><label for='data-225cd988-e257-479f-8036-827c827b6bd0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>valid_min :</span></dt><dd>0.494025</dd><dt><span>valid_max :</span></dt><dd>40.34405</dd><dt><span>units :</span></dt><dd>m</dd><dt><span>positive :</span></dt><dd>down</dd><dt><span>unit_long :</span></dt><dd>Meters</dd><dt><span>long_name :</span></dt><dd>Depth</dd><dt><span>standard_name :</span></dt><dd>depth</dd><dt><span>axis :</span></dt><dd>Z</dd><dt><span>_ChunkSizes :</span></dt><dd>50</dd><dt><span>_CoordinateAxisType :</span></dt><dd>Height</dd><dt><span>_CoordinateZisPositive :</span></dt><dd>down</dd></dl></div><div class='xr-var-data'><pre>array(0.494025, dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-756d38f8-863c-46b2-85f2-6bc6cebb0f04' class='xr-section-summary-in' type='checkbox'  checked><label for='section-756d38f8-863c-46b2-85f2-6bc6cebb0f04' class='xr-section-summary' >Data variables: <span>(7)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>bottomT</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-58023ecb-21a2-4fd6-a175-1ada0a0cfbd3' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-58023ecb-21a2-4fd6-a175-1ada0a0cfbd3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7d8e777f-f56c-466f-bcf8-f54d505ceb1e' class='xr-var-data-in' type='checkbox'><label for='data-7d8e777f-f56c-466f-bcf8-f54d505ceb1e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Sea floor potential temperature</dd><dt><span>standard_name :</span></dt><dd>sea_water_potential_temperature_at_sea_floor</dd><dt><span>units :</span></dt><dd>degrees_C</dd><dt><span>unit_long :</span></dt><dd>Degrees Celsius</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[   1  681 1440]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>mlotst</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b99335e7-265c-4cc6-947d-b4bd4ee70dc3' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b99335e7-265c-4cc6-947d-b4bd4ee70dc3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ccc5412a-8d55-4b02-9c9c-93f9581f462c' class='xr-var-data-in' type='checkbox'><label for='data-ccc5412a-8d55-4b02-9c9c-93f9581f462c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Density ocean mixed layer thickness</dd><dt><span>standard_name :</span></dt><dd>ocean_mixed_layer_thickness_defined_by_sigma_theta</dd><dt><span>units :</span></dt><dd>m</dd><dt><span>unit_long :</span></dt><dd>Meters</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[   1  681 1440]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>so</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-7d2eb172-bd8f-49d4-a86a-18b6564a4fb6' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-7d2eb172-bd8f-49d4-a86a-18b6564a4fb6' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5a3727f2-be2a-4279-b899-fef58fc7485c' class='xr-var-data-in' type='checkbox'><label for='data-5a3727f2-be2a-4279-b899-fef58fc7485c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Salinity</dd><dt><span>standard_name :</span></dt><dd>sea_water_salinity</dd><dt><span>units :</span></dt><dd>1e-3</dd><dt><span>unit_long :</span></dt><dd>Practical Salinity Unit</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[  1   7 341 720]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>thetao</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-561de67b-dbd8-48c2-adf9-4e72f48bf8fb' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-561de67b-dbd8-48c2-adf9-4e72f48bf8fb' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-819bdd3c-6d86-4155-8509-2b4b056a2794' class='xr-var-data-in' type='checkbox'><label for='data-819bdd3c-6d86-4155-8509-2b4b056a2794' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Temperature</dd><dt><span>standard_name :</span></dt><dd>sea_water_potential_temperature</dd><dt><span>units :</span></dt><dd>degrees_C</dd><dt><span>unit_long :</span></dt><dd>Degrees Celsius</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[  1   7 341 720]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>uo</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-30963560-08da-4f5d-b9df-1777d161083c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-30963560-08da-4f5d-b9df-1777d161083c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-53cd5453-8539-4f74-a31e-91171462174f' class='xr-var-data-in' type='checkbox'><label for='data-53cd5453-8539-4f74-a31e-91171462174f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Eastward velocity</dd><dt><span>standard_name :</span></dt><dd>eastward_sea_water_velocity</dd><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>unit_long :</span></dt><dd>Meters per second</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[  1   7 341 720]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>vo</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-7a367b5d-17c6-4da9-889a-bd07ccce0aac' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-7a367b5d-17c6-4da9-889a-bd07ccce0aac' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-886dbdfe-88c0-4977-a267-4d0743258f87' class='xr-var-data-in' type='checkbox'><label for='data-886dbdfe-88c0-4977-a267-4d0743258f87' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Northward velocity</dd><dt><span>standard_name :</span></dt><dd>northward_sea_water_velocity</dd><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>unit_long :</span></dt><dd>Meters per second</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[  1   7 341 720]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>zos</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-7c5c07ff-330b-45c4-a49e-5af233092057' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-7c5c07ff-330b-45c4-a49e-5af233092057' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f78eed3a-bbf2-45bb-ac14-1ef9e4bd2e43' class='xr-var-data-in' type='checkbox'><label for='data-f78eed3a-bbf2-45bb-ac14-1ef9e4bd2e43' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Sea surface height</dd><dt><span>standard_name :</span></dt><dd>sea_surface_height_above_geoid</dd><dt><span>units :</span></dt><dd>m</dd><dt><span>unit_long :</span></dt><dd>Meters</dd><dt><span>cell_methods :</span></dt><dd>area: mean</dd><dt><span>_ChunkSizes :</span></dt><dd>[   1  681 1440]</dd></dl></div><div class='xr-var-data'><pre>[51139655 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-d9b6abd2-9e6d-499a-9b0d-4503b50239c6' class='xr-section-summary-in' type='checkbox'  ><label for='section-d9b6abd2-9e6d-499a-9b0d-4503b50239c6' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-f36152c6-deec-42cb-acbe-3e881a3d32a6' class='xr-index-data-in' type='checkbox'/><label for='index-f36152c6-deec-42cb-acbe-3e881a3d32a6' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([              -17.0,  -16.91666603088379,  -16.83333396911621,\n",
       "                    -16.75,  -16.66666603088379,  -16.58333396911621,\n",
       "                     -16.5,  -16.41666603088379,  -16.33333396911621,\n",
       "                    -16.25,  -16.16666603088379,  -16.08333396911621,\n",
       "                     -16.0, -15.916666984558105, -15.833333015441895,\n",
       "                    -15.75, -15.666666984558105, -15.583333015441895,\n",
       "                     -15.5, -15.416666984558105, -15.333333015441895,\n",
       "                    -15.25, -15.166666984558105, -15.083333015441895,\n",
       "                     -15.0, -14.916666984558105, -14.833333015441895,\n",
       "                    -14.75, -14.666666984558105, -14.583333015441895,\n",
       "                     -14.5, -14.416666984558105, -14.333333015441895,\n",
       "                    -14.25, -14.166666984558105, -14.083333015441895,\n",
       "                     -14.0, -13.916666984558105, -13.833333015441895,\n",
       "                    -13.75, -13.666666984558105, -13.583333015441895,\n",
       "                     -13.5, -13.416666984558105, -13.333333015441895,\n",
       "                    -13.25, -13.166666984558105, -13.083333015441895,\n",
       "                     -13.0, -12.916666984558105, -12.833333015441895,\n",
       "                    -12.75, -12.666666984558105, -12.583333015441895,\n",
       "                     -12.5, -12.416666984558105, -12.333333015441895,\n",
       "                    -12.25, -12.166666984558105, -12.083333015441895,\n",
       "                     -12.0, -11.916666984558105, -11.833333015441895,\n",
       "                    -11.75, -11.666666984558105, -11.583333015441895,\n",
       "                     -11.5, -11.416666984558105, -11.333333015441895,\n",
       "                    -11.25, -11.166666984558105, -11.083333015441895,\n",
       "                     -11.0, -10.916666984558105, -10.833333015441895,\n",
       "                    -10.75, -10.666666984558105, -10.583333015441895,\n",
       "                     -10.5, -10.416666984558105, -10.333333015441895,\n",
       "                    -10.25, -10.166666984558105, -10.083333015441895,\n",
       "                     -10.0],\n",
       "      dtype=&#x27;float32&#x27;, name=&#x27;latitude&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-14693b1c-caba-4daa-b1d2-dd24fb9ae0c1' class='xr-index-data-in' type='checkbox'/><label for='index-14693b1c-caba-4daa-b1d2-dd24fb9ae0c1' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;1993-12-31 12:00:00&#x27;, &#x27;1994-01-01 12:00:00&#x27;,\n",
       "               &#x27;1994-01-02 12:00:00&#x27;, &#x27;1994-01-03 12:00:00&#x27;,\n",
       "               &#x27;1994-01-04 12:00:00&#x27;, &#x27;1994-01-05 12:00:00&#x27;,\n",
       "               &#x27;1994-01-06 12:00:00&#x27;, &#x27;1994-01-07 12:00:00&#x27;,\n",
       "               &#x27;1994-01-08 12:00:00&#x27;, &#x27;1994-01-09 12:00:00&#x27;,\n",
       "               ...\n",
       "               &#x27;2020-12-22 12:00:00&#x27;, &#x27;2020-12-23 12:00:00&#x27;,\n",
       "               &#x27;2020-12-24 12:00:00&#x27;, &#x27;2020-12-25 12:00:00&#x27;,\n",
       "               &#x27;2020-12-26 12:00:00&#x27;, &#x27;2020-12-27 12:00:00&#x27;,\n",
       "               &#x27;2020-12-28 12:00:00&#x27;, &#x27;2020-12-29 12:00:00&#x27;,\n",
       "               &#x27;2020-12-30 12:00:00&#x27;, &#x27;2020-12-31 12:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=9863, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-f33d1d15-a5f9-4e14-819b-9864d1aa9582' class='xr-index-data-in' type='checkbox'/><label for='index-f33d1d15-a5f9-4e14-819b-9864d1aa9582' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([            142.0, 142.0833282470703, 142.1666717529297,\n",
       "                  142.25, 142.3333282470703, 142.4166717529297,\n",
       "                   142.5, 142.5833282470703, 142.6666717529297,\n",
       "                  142.75, 142.8333282470703, 142.9166717529297,\n",
       "                   143.0, 143.0833282470703, 143.1666717529297,\n",
       "                  143.25, 143.3333282470703, 143.4166717529297,\n",
       "                   143.5, 143.5833282470703, 143.6666717529297,\n",
       "                  143.75, 143.8333282470703, 143.9166717529297,\n",
       "                   144.0, 144.0833282470703, 144.1666717529297,\n",
       "                  144.25, 144.3333282470703, 144.4166717529297,\n",
       "                   144.5, 144.5833282470703, 144.6666717529297,\n",
       "                  144.75, 144.8333282470703, 144.9166717529297,\n",
       "                   145.0, 145.0833282470703, 145.1666717529297,\n",
       "                  145.25, 145.3333282470703, 145.4166717529297,\n",
       "                   145.5, 145.5833282470703, 145.6666717529297,\n",
       "                  145.75, 145.8333282470703, 145.9166717529297,\n",
       "                   146.0, 146.0833282470703, 146.1666717529297,\n",
       "                  146.25, 146.3333282470703, 146.4166717529297,\n",
       "                   146.5, 146.5833282470703, 146.6666717529297,\n",
       "                  146.75, 146.8333282470703, 146.9166717529297,\n",
       "                   147.0],\n",
       "      dtype=&#x27;float32&#x27;, name=&#x27;longitude&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-c7e3dc0b-71f7-4d8e-a337-4a67ec0cbf44' class='xr-section-summary-in' type='checkbox'  ><label for='section-c7e3dc0b-71f7-4d8e-a337-4a67ec0cbf44' class='xr-section-summary' >Attributes: <span>(25)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>title :</span></dt><dd>daily mean fields from Global Ocean Physics Analysis and Forecast updated Daily</dd><dt><span>easting :</span></dt><dd>longitude</dd><dt><span>northing :</span></dt><dd>latitude</dd><dt><span>history :</span></dt><dd>2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creation</dd><dt><span>source :</span></dt><dd>MERCATOR GLORYS12V1</dd><dt><span>institution :</span></dt><dd>MERCATOR OCEAN</dd><dt><span>references :</span></dt><dd>http://www.mercator-ocean.fr</dd><dt><span>comment :</span></dt><dd>CMEMS product</dd><dt><span>Conventions :</span></dt><dd>CF-1.4</dd><dt><span>domain_name :</span></dt><dd>GL12</dd><dt><span>FROM_ORIGINAL_FILE__field_type :</span></dt><dd>mean</dd><dt><span>field_date :</span></dt><dd>2020-12-31 00:00:00</dd><dt><span>field_julian_date :</span></dt><dd>25932.0</dd><dt><span>julian_day_unit :</span></dt><dd>days since 1950-01-01 00:00:00</dd><dt><span>forecast_range :</span></dt><dd></dd><dt><span>forecast_type :</span></dt><dd></dd><dt><span>bulletin_date :</span></dt><dd>2021-01-06 00:00:00</dd><dt><span>bulletin_type :</span></dt><dd>operational</dd><dt><span>FROM_ORIGINAL_FILE__longitude_min :</span></dt><dd>-180.0</dd><dt><span>FROM_ORIGINAL_FILE__longitude_max :</span></dt><dd>179.91667</dd><dt><span>FROM_ORIGINAL_FILE__latitude_min :</span></dt><dd>-80.0</dd><dt><span>FROM_ORIGINAL_FILE__latitude_max :</span></dt><dd>90.0</dd><dt><span>z_min :</span></dt><dd>0.494025</dd><dt><span>z_max :</span></dt><dd>5727.917</dd><dt><span>_CoordSysBuilder :</span></dt><dd>ucar.nc2.dataset.conv.CF1Convention</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 85, time: 9863, longitude: 61)\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float32 -17.0 -16.92 -16.83 ... -10.17 -10.08 -10.0\n",
       "  * time       (time) datetime64[ns] 1993-12-31T12:00:00 ... 2020-12-31T12:00:00\n",
       "  * longitude  (longitude) float32 142.0 142.1 142.2 142.2 ... 146.8 146.9 147.0\n",
       "    depth      float32 0.494\n",
       "Data variables:\n",
       "    bottomT    (time, latitude, longitude) float32 ...\n",
       "    mlotst     (time, latitude, longitude) float32 ...\n",
       "    so         (time, latitude, longitude) float32 ...\n",
       "    thetao     (time, latitude, longitude) float32 ...\n",
       "    uo         (time, latitude, longitude) float32 ...\n",
       "    vo         (time, latitude, longitude) float32 ...\n",
       "    zos        (time, latitude, longitude) float32 ...\n",
       "Attributes: (12/25)\n",
       "    title:                              daily mean fields from Global Ocean P...\n",
       "    easting:                            longitude\n",
       "    northing:                           latitude\n",
       "    history:                            2022/05/25 21:54:07 MERCATOR OCEAN Ne...\n",
       "    source:                             MERCATOR GLORYS12V1\n",
       "    institution:                        MERCATOR OCEAN\n",
       "    ...                                 ...\n",
       "    FROM_ORIGINAL_FILE__longitude_max:  179.91667\n",
       "    FROM_ORIGINAL_FILE__latitude_min:   -80.0\n",
       "    FROM_ORIGINAL_FILE__latitude_max:   90.0\n",
       "    z_min:                              0.494025\n",
       "    z_max:                              5727.917\n",
       "    _CoordSysBuilder:                   ucar.nc2.dataset.conv.CF1Convention"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_daily_1_12 = ds_man.get_dataset(\"daily_climate_1_12\")\n",
    "ds_daily_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily climate to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_climate = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "nan_eg, _ = sample_spatial_batch(monthly_climate, window_dims=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_eg[\"bottomT\"].isel(time=-1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = spatial_data.xa_ds_to_3d_numpy(nan_eg, \n",
    "    exclude_vars = [\"spatial_ref\", \"coral_algae_1-12_degree\", \"latitude\", \"longitude\", \"depth\", \"time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_nans = spatial_data.encode_nans_one_hot(array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix boolean indexing\n",
    "# # For now, shallowest depth is taken (0.45)\n",
    "# # TODO: process this and export it to new file since takes a while to run\n",
    "# ds_man.add_dataset(\n",
    "#     \"daily_climate_1_12_X_np\", filter_out_nans(\n",
    "#         spatial_data.xa_ds_to_3d_numpy(ds_man.get_dataset(\"daily_climate_1_12\").isel(depth=0)), ds_man.get_dataset(\"daily_climate_1_12_y_np\"))[0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this merge into data processing pipeline\n",
    "# merge daily mean files\n",
    "# var_daily_dir = Path(\"lustre_scratch/datasets/global_ocean_reanalysis/daily_means\")\n",
    "# save_combined_dailies_path = Path(\"lustre_scratch/datasets/global_ocean_reanalysis/daily_means/dailies_combined.nc\")\n",
    "# daily_file_paths = file_ops.return_list_filepaths(var_daily_dir, \".nc\")\n",
    "# combined_dailies = xa.open_mfdataset(daily_file_paths)\n",
    "# combined_dailies.to_netcdf(save_combined_dailies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create 3D array from xarray dataset variables. Shape: (num_samples, num_parameters, sequence_len)\n",
    "# X_with_nans = spatial_data.xa_ds_to_3d_numpy(xa_coral_climate_1_12_features)\n",
    "# print(f'X_with_nans shape (num_samples: {X_with_nans.shape[0]}, total num_parameters (includes nans parameters): {X_with_nans.shape[1]}, sequence_len: {X_with_nans.shape[2]})')\n",
    "\n",
    "# for i, param in enumerate(xa_coral_climate_1_12_features.data_vars):\n",
    "#     print(f\"{i}: {param}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations for which there are nan values\n",
    "\n",
    "99% sure these are are just gridcells containing land. Would be a good thing to investigate, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_with_nans\n",
    "# # problem, probably with sea ice features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out columns that contain entirely NaN values\n",
    "# col_mask = ~np.all(np.isnan(X), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "# masked_cols = X[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "# print(\"masked_cols shape:\", masked_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out all rows which contain any NaN values\n",
    "# row_mask = ~np.any(np.isnan(masked_cols), axis=1) # boolean mask indicating which rows to keep\n",
    "# masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "# masked_cols_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out all depths which contain any NaN values\n",
    "# depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "# X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "# X = np.swapaxes(X, 1, 2)\n",
    "# print(f\"X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create target from coral ground truth. Shape: (num_samples, 1)\n",
    "# # TODO: not sure if this is shuffling the values when reshaping\n",
    "# y_with_nans = np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].sel(\n",
    "#     time=xa_coral_climate_1_12.time[-1])).reshape(-1, 1)\n",
    "# # remove ys with nan values in other variables\n",
    "# y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "# print(f\"y_with_nans shape: {y_with_nans.shape}\")\n",
    "# print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = filter_out_nans(X_with_nans, np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1))\n",
    "# print(f\"X shape: {X.shape}\")\n",
    "# print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features = ds_man.get_dataset(\"monthly_climate_features\")\n",
    "xa_coral_climate_1_12 = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "\n",
    "xa_coral_climate_1_12_working = xa_coral_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_Xs_onehot, all_lat_lon_dict_onehot = sample_spatial_batch(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=(-20,13))\n",
    "# all_Xs_onehot, all_lat_lon_dict_onehot = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=((-20,13)))\n",
    "# all_Xs_onehot = naive_X_nan_replacement(all_Xs_onehot)\n",
    "# all_ys_onehot, _ = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=(-20,13), variables = [\"coral_algae_1-12_degree\"])\n",
    "# all_ys_onehot = naive_y_nan_replacement(all_ys_onehot)\n",
    "# all_ys_onehot = all_ys_onehot[:,:,0]\n",
    "\n",
    "train_onehot_Xs, train_onehot_ys, train_onehot_subsample, train_onehot_lat_lons_vals_dict = generate_patch(xa_ds=xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-6,6), onehot=False)\n",
    "test_onehot_Xs, test_onehot_ys, test_onehot_subsample, test_onehot_lat_lons_vals_dict = generate_patch(xa_ds=xa_coral_climate_1_12, lat_lon_starts=(-16,148), coord_range=(-6,6))\n",
    "\n",
    "print(\"train_onehot_Xs shape: \", train_onehot_Xs.shape)\n",
    "print(\"train_onehot_ys shape: \", train_onehot_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bathymetry\n",
    "bath_A = ds_man.get_dataset(\"bathymetry_A\")\n",
    "bath_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m target_resolution \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      6\u001b[0m _,_,av_degrees \u001b[39m=\u001b[39m spatial_data\u001b[39m.\u001b[39mdistance_to_degrees(target_resolution)\n\u001b[0;32m----> 7\u001b[0m bath_A_1km \u001b[39m=\u001b[39m spatial_data\u001b[39m.\u001b[39;49mupsample_xarray_to_target(bath_A, av_degrees)\n\u001b[1;32m      8\u001b[0m \u001b[39m# im = bath_A_1km.plot(ax=ax)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spatial_plots\u001b[39m.\u001b[39mplot_DEM(bath_A_1km, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m DEM upsampled to \u001b[39m\u001b[39m{\u001b[39;00mtarget_resolution\u001b[39m}\u001b[39;00m\u001b[39m meters\u001b[39m\u001b[39m\"\u001b[39m, vmin\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m, vmax\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/lustre_scratch/orlando-code/coralshift/coralshift/processing/spatial_data.py:22\u001b[0m, in \u001b[0;36mupsample_xarray_to_target\u001b[0;34m(xa_array, target_resolution)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupsample_xarray_to_target\u001b[39m(\n\u001b[1;32m     18\u001b[0m     xa_array: xa\u001b[39m.\u001b[39mDataArray \u001b[39m|\u001b[39m xa\u001b[39m.\u001b[39mDataset, target_resolution: \u001b[39mfloat\u001b[39m\n\u001b[1;32m     19\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m xa\u001b[39m.\u001b[39mDataset:\n\u001b[1;32m     20\u001b[0m     \u001b[39m# N.B. not perfect at getting starts/ends matching up\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# TODO: enable flexible upsampling by time also\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     lat_lims \u001b[39m=\u001b[39m xarray_coord_limits(xa_array, \u001b[39m\"\u001b[39;49m\u001b[39mlatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m     lon_lims \u001b[39m=\u001b[39m xarray_coord_limits(xa_array, \u001b[39m\"\u001b[39m\u001b[39mlongitude\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[39m# get current degree resolution\u001b[39;00m\n",
      "File \u001b[0;32m/lustre_scratch/orlando-code/coralshift/coralshift/processing/spatial_data.py:347\u001b[0m, in \u001b[0;36mxarray_coord_limits\u001b[0;34m(xa_array, dim)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxarray_coord_limits\u001b[39m(xa_array: xa\u001b[39m.\u001b[39mDataset, dim: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    336\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the minimum and maximum values for a coordinate dimension of an xarray dataset.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mmin\u001b[39m, \u001b[39mmax\u001b[39m \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(xa_array[dim]\u001b[39m.\u001b[39mmin()), \u001b[39mfloat\u001b[39m(xa_array[dim]\u001b[39m.\u001b[39mmax())\n\u001b[1;32m    348\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mmin\u001b[39m, \u001b[39mmax\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 km. Struggles displaying/processing 100m, but have yet to try saving to this/inferring\n",
    "target_resolution = 1000\n",
    "_,_,av_degrees = spatial_data.distance_to_degrees(target_resolution)\n",
    "bath_A_1km = spatial_data.upsample_xarray_to_target(bath_A, av_degrees)\n",
    "# im = bath_A_1km.plot(ax=ax)\n",
    "\n",
    "spatial_plots.plot_DEM(bath_A_1km, f\" DEM upsampled to {target_resolution} meters\", vmin=-100, vmax=0)\n",
    "# spatial_plots.format_spatial_plot(im, fig, ax, f\"Upsampled to {target_resolution} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def min_max_index_extreme_values(array, min_val=-100, max_val=0):\n",
    "#     # Find indices of values above min and below max values\n",
    "#     above_max_indices = np.where(array > max_val)\n",
    "#     below_min_indices = np.where(array < min_val)\n",
    "\n",
    "#     # Find the minimum and maximum indices among the above criteria\n",
    "#     lon_min_index = np.min(np.concatenate((above_max_indices[1], below_min_indices[1])))\n",
    "#     lat_min_index = np.max(np.concatenate((above_max_indices[1], below_min_indices[1])))\n",
    "#     return lon_min_index, lat_min_index\n",
    "\n",
    "\n",
    "# def slice_coast(bath_ds, num_slices=10, bathymetric_range: tuple[float] = (-100,0)):\n",
    "\n",
    "#     # lat_lims = spatial_data.xarray_coord_limits(bath_ds, \"latitude\")\n",
    "#     # could use values or indices\n",
    "\n",
    "\n",
    "#     # slice_ranges = np.linspace(lat_lims[0], lat_lims[1], num_slices)\n",
    "#     lat_num = len(list(bath_ds[\"latitude\"]))\n",
    "#     slice_ranges = np.arange(0, lat_num, (lat_num // num_slices))\n",
    "#     # slice_height = int(np.floor(np.abs(np.diff(lat_lims)) / num_slices))\n",
    "#     # for vertical centre of each slice, find limits of relevant bathymetry\n",
    "#     for i in range(len(slice_ranges)-1):\n",
    "#         slice_ds = bath_ds.isel(latitude=slice(slice_ranges[i],slice_ranges[i+1]))\n",
    "#         #don't think I need values here\n",
    "#         lon_min_index, lat_min_index = min_max_index_extreme_values(slice_ds.values)\n",
    "#         print(lon_min_index, lat_min_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_coast(bath_1_12_degree)\n",
    "chunk_size = 20\n",
    "vmin, vmax = -100, 0\n",
    "threshold_percent = 10\n",
    "chunk_coords = spatial_data.find_chunks_with_percentage(bath_A_1km.values[0,:,:], -100, 0, chunk_size, threshold_percent)\n",
    "\n",
    "print(\"array_shape\", bath_A_1km.values[0,:,:].shape)\n",
    "print(\"extreme chunk\", chunk_coords[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_coords[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bath_A_1km.isel(band=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vars_from_ds_or_da(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_subsample_from_coord(xa_ds, chunk_coords):\n",
    "    lat_start, lat_end = chunk_coords[0][0], chunk_coords[0][1]\n",
    "    lon_start, lon_end = chunk_coords[1][0], chunk_coords[1][1]\n",
    "\n",
    "    return xa_ds.isel({\"latitude\": slice(lat_start, lat_end), \"longitude\": slice(lon_start, lon_end)})\n",
    "\n",
    "def get_vars_from_ds_or_da(xa_d: xa.DataArray | xa.Dataset) -> str | list[str]:\n",
    "    if type(xa_d) == xa.core.dataarray.DataArray:\n",
    "        vars = xa_d.name\n",
    "    elif type(xa_d) == xa.core.dataarray.Dataset:\n",
    "        vars = list(xa_d.data_vars)\n",
    "    else:\n",
    "        raise TypeError(\"Format was neither an xarray Dataset nor a DataArray\")\n",
    "\n",
    "    return vars\n",
    "\n",
    "\n",
    "def nc_chunk_files(dest_dir_path: Path | str, xa_ds: xa.Dataset, chunk_size: int = 20, \n",
    "    threshold_percent: float=10, vmin: float=-100, vmax: float=0):\n",
    "    \n",
    "    chunk_coord_pairs = spatial_data.find_chunks_with_percentage(\n",
    "        xa_ds, vmin, vmax, chunk_size, threshold_percent)\n",
    "    \n",
    "    for coord_pair in chunk_coord_pairs:\n",
    "        sub_ds = ds_subsample_from_coord(xa_ds, coord_pair)\n",
    "        # make filename\n",
    "        vars = get_vars_from_ds_or_da(xa_ds)\n",
    "        # convert coord indices to absolute coords\n",
    "        filename = climate_data.generate_spatiotemporal_var_filename_from_dict(\n",
    "            {\"vars\": vars, \"lats\": coord_pair[0], \"lons\": coord_pair[1]})\n",
    "\n",
    "        # save file\n",
    "    print(filename)\n",
    "    return sub_ds\n",
    "\n",
    "nc_chunk_files(\"asdf\", bath_A_1km.isel(band=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "da = bath_A_1km\n",
    "da.plot(ax=ax, vmin=vmin, vmax=vmax, cmap=\"BrBG\")\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "for coord in chunk_coords:\n",
    "    xy = index_to_coord(da, coord[0])\n",
    "    height, width = delta_index_to_distance(da, coord[1], coord[0])\n",
    "    rect = patches.Rectangle(xy, width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt downsampling climate in each square\n",
    "# save as individual nc file\n",
    "# set up tf dataloader: load each nc file and take batches from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_1_12_degree = spatial_data.upsample_xarray_to_target(bath_A, 1/12)\n",
    "\n",
    "bath_1_12_degree.values[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_index_extreme_values(bath_1_12_degree.sel(latitude=slice(-12,-15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_1_12_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_climate.sel(latitude=slice(-10,-17.05), longitude=slice(141.95,147.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_band_bath = bath_1_12_degree.isel(band=0)\n",
    "\n",
    "# downsample climate data to 1km\n",
    "monthly_climate = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "\n",
    "# get limits of bathymetry\n",
    "lat_lims = spatial_data.xarray_coord_limits(bath_1_12_degree, \"latitude\")\n",
    "lon_lims = spatial_data.xarray_coord_limits(bath_1_12_degree, \"longitude\")\n",
    "\n",
    "restricted_monthly_climate = monthly_climate.sel(latitude=slice(-10,-17), longitude=slice(142,147))\n",
    "\n",
    "\n",
    "# padded_restricted_monthly_climate = spatial_data.buffer_nans(restricted_monthly_climate, size=1\n",
    "km_monthly = restricted_monthly_climate.interp_like(bath_1_12_degree, method=\"linear\")\n",
    "\n",
    "coral_climate_1km = xa.combine_by_coords([km_monthly.drop(\"spatial_ref\"),no_band_bath], coords=[\"time\", \"latitude\", \"longitude\"])\n",
    "(coral_climate_1km,) = xa.broadcast(coral_climate_1km)\n",
    "coral_climate_1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_monthly_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (13,) + inhomogeneous part.\n",
    "\n",
    "# train_comb_Xs, train_comb_ys, train_comb_subsample, train_comb_lat_lons_vals_dict = generate_patch(xa_ds=coral_climate_1km, lat_lon_starts=(-10,142), coord_range=(-6,6))\n",
    "# test_comb_Xs, test_comb_ys, test_comb_subsample, test_comb_lat_lons_vals_dict = generate_patch(xa_ds=coral_climate_1km, lat_lon_starts=(-16,148), coord_range=(-6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all_Xs_onehot shape: \", all_Xs_onehot.shape)\n",
    "print(\"all_ys_onehot shape: \", all_ys_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalise along variable axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Xs, all_lat_lon_dict = sample_spatial_batch(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=(-20,13))\n",
    "all_Xs, all_lat_lon_dict = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-8,140), coord_range=((-20,13)))\n",
    "all_Xs = naive_X_nan_replacement(all_Xs)\n",
    "all_ys, _ = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5), variables = [\"coral_algae_1-12_degree\"])\n",
    "all_ys = naive_y_nan_replacement(all_ys)\n",
    "all_ys = all_ys[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xs shape: \", Xs.shape)\n",
    "print(\"ys shape: \", ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, ys, all_subsample, all_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "patch1_Xs, patch1_ys, patch1_subsample, patch1_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, (-10,142), (-7,5))\n",
    "patch2_Xs, patch2_ys, patch2_subsample, patch2_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, (-17,147), (-7,5))\n",
    "patch3_Xs, patch3_ys, patch3_subsample, patch3_lat_lons_vals_dict = generate_patch(xa_coral_climate_1_12, (-16,146), (-7,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"coralshift\",\n",
    "    entity=\"orlando-code\",\n",
    "    settings=wandb.Settings(start_method=\"fork\")\n",
    "    # config={    }\n",
    "    )\n",
    "\n",
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "# X = ds_man.get_dataset(\"monthly_climate_1_12_X_np\")\n",
    "# y = ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    "# # check that untrained model runs (should output array of non-nan values)\n",
    "# # why values change?\n",
    "# # g_model(X[:32])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     sub_X, sub_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class gru_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, rnn_units: list[int], num_layers: int, \n",
    "        # dff: int\n",
    "        ):\n",
    "        \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "        layers to a desired output shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "        num_layers (int): number of layers in GRU model\n",
    "        \"\"\"\n",
    "        super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "        # store values for later use\n",
    "        self.num_layers = num_layers    # number of layers in GRU model\n",
    "        self.rnn_units = rnn_units\n",
    "        # self.dff = dff\n",
    "        # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "        self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "                                   return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "                                   return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "        # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "        # self.w1 = tf.keras.layers.Dense(dff) # 10 units\n",
    "        self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "\n",
    "        self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "        # self.A = tf.keras.layers.Dense(30)\n",
    "        # self.B = tf.keras.layers.Dense(dff)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        # whole_seq, static_input = inputs\n",
    "        whole_seq = inputs\n",
    "\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwritting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        # static = self.B(tf.nn.gelu(self.A(static_input)))\n",
    "        # target = self.w1(final_s)  + static # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "\n",
    "# initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([500], 1) # N.B. [x] is number of hidden layers in GRU network\n",
    "\n",
    "\n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if class_weights is not None:\n",
    "        sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "        return bce(y, y_pred, sample_weight=sample_weights)\n",
    "\n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def training_batches(X: np.ndarray, y: np.ndarray, batch_num: int, batch_size: int=32):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "    X_batch = X[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "# https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "# should aim to delete the following to speed up training: but can't figure out a way to make wandb reporting work\n",
    "# without it\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: np.ndarray, y: np.ndarray, \n",
    "        training: bool=True, class_weights=class_weights, batch_num:int=None, batch_size: int=None) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            num_samples = X.shape[0]\n",
    "            num_batches = num_samples // batch_size\n",
    "            # num_batches = batch_num\n",
    "            total_epoch_loss = 0.0\n",
    "            for batch_num in tqdm(range(num_batches), desc=\"batches\", position=0, leave=True):\n",
    "                X_batch, y_batch = training_batches(X, y, batch_num=batch_num, batch_size=batch_size)\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = gru(X_batch, training) \n",
    "                    xent = negative_log_likelihood(y_batch, y_pred, class_weights)\n",
    "                    # y_pred = gru(X, training) # TO DELETE\n",
    "                    # xent = negative_log_likelihood(y, y_pred)\n",
    "                \n",
    "                gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "                # print(\"xent\", xent.numpy())\n",
    "                # print(\"total_epoch_loss\", total_epoch_loss)\n",
    "                total_epoch_loss += xent\n",
    "                # learning rate?\n",
    "                wandb.log({\"batch\": batch_num, \"loss\": xent, \"total_epoch_loss\": total_epoch_loss})\n",
    "\n",
    "            average_loss = total_epoch_loss / num_batches\n",
    "            # return predicted output values and total loss value\n",
    "            return y_pred, xent, total_epoch_loss\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    # TODO: this isn't assigned... What should it return otherwise?\n",
    "    return train_step\n",
    "\n",
    "\n",
    "X_train = train_onehot_Xs[:]\n",
    "y_train = train_onehot_ys[:]\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=np.reshape(y_train,-1))\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    num_epochs = 50\n",
    "    # will update so that subsamples are fed in from which batches are taken: will require recomputation\n",
    "    # of class_weight for each subsample\n",
    "    num_batches = 2\n",
    "    batch_size = 512\n",
    "    tr_step = build_graph()\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position = 0, leave=True):\n",
    "        y_pred, xent, total_epoch_loss = tr_step(\n",
    "            g_model, optimizer, X_train[:], y_train[:], class_weights=class_weights, \n",
    "            batch_size=batch_size, batch_num=num_batches, training=True)\n",
    "        wandb.log({\"epoch\": epoch, \"epoch_loss\": total_epoch_loss})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# check (with prints) that wandb is functioning\n",
    "# check against known timeseries task for correct implementation\n",
    "# find timeseries which get bad loss and debug why\n",
    "# log best loss which can be logged: save weights (and do a inference plot with best weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train/predict\n",
    "# if training:\n",
    "    # assign epoch output to dataset\n",
    "# if testing:\n",
    "# patch1_pred = g_model(patch1_Xs, training=False)\n",
    "# patch2_pred = g_model(patch2_Xs, training=False)\n",
    "# patch3_pred = g_model(patch3_Xs, training=False)\n",
    "pred = g_model(test_onehot_Xs, training=False)\n",
    "\n",
    "# all_predicted = g_model(all_Xs, training=False)\n",
    "    # assign output predictions to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pred_xa_ds_onehot_lim = reformat_prediction(xa_coral_climate_1_12_working, test_onehot_subsample, pred, test_onehot_lat_lons_vals_dict)\n",
    "\n",
    "mask = patch_pred_xa_ds_onehot_lim[\"output\"] > 0.6\n",
    "\n",
    "spatial_plots.plot_var(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pred_xa_ds_onehot_lim = reformat_prediction(xa_coral_climate_1_12_working, test_onehot_subsample, pred, test_onehot_lat_lons_vals_dict)\n",
    "\n",
    "spatial_plots.plot_var(patch_pred_xa_ds_onehot_lim[\"output\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_Xs_onehot, patch_ys_onehot, patch_subsample_onehot, patch_lat_lons_vals_dict_onehot = generate_patch(xa_coral_climate_1_12, (-10,142), (-7,5))\n",
    "\n",
    "patch_pred_xa_ds_onehot = reformat_prediction(xa_coral_climate_1_12_working, patch_subsample_onehot, pred, patch_lat_lons_vals_dict_onehot)\n",
    "\n",
    "spatial_plots.plot_var(patch_pred_xa_ds_onehot[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(patch1_pred_xa_ds[\"output\"].values,patch2_pred_xa_ds[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch1_pred_xa_ds = reformat_prediction(xa_coral_climate_1_12_working, patch1_subsample, patch1_pred, patch1_lat_lons_vals_dict)\n",
    "# patch2_pred_xa_ds = reformat_prediction(xa_coral_climate_1_12_working, patch2_subsample, patch2_pred, patch2_lat_lons_vals_dict)\n",
    "# patch3_pred_xa_ds = reformat_prediction(xa_coral_climate_1_12_working, patch3_subsample, patch3_pred, patch3_lat_lons_vals_dict)\n",
    "\n",
    "\n",
    "f, a0 = spatial_plots.plot_var_at_time(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"], \"2020-12-16\")\n",
    "# # visualise result\n",
    "# spatial_plots.plot_var(patch1_pred_xa_ds[\"output\"])\n",
    "# spatial_plots.plot_var(patch2_pred_xa_ds[\"output\"])\n",
    "# spatial_plots.plot_var(patch3_pred_xa_ds[\"output\"])\n",
    "\n",
    "# pred_xa_ds[\"coral_algae_1-12_degree\"].isel(time=-1).plot(ax=ax[0])\n",
    "# pred_xa_ds[\"output\"].plot(ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"bottomT\"].isel(time=-1).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample climate data to 1km\n",
    "monthly_climate = ds_man.get_dataset(\"monthly_climate_1_12\")\n",
    "\n",
    "# get limits of bathymetry\n",
    "lat_lims = spatial_data.xarray_coord_limits(coarsened_bath_A, \"latitude\")\n",
    "lon_lims = spatial_data.xarray_coord_limits(coarsened_bath_A, \"longitude\")\n",
    "\n",
    "restricted_monthly_climate = monthly_climate.sel(latitude=slice(-10,-17), longitude=slice(142,147))\n",
    "\n",
    "\n",
    "# padded_restricted_monthly_climate = spatial_data.buffer_nans(restricted_monthly_climate, size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 3\n",
    "exclude_vars = [\"spatial_ref\", \"coral_algae_1-12_degree\", \"siconc\", \"usi\", \"vsi\", \"sithick\"]\n",
    "buffered_ds = spatially_buffer_timeseries(monthly_climate, buffer_size=buffer_size, exclude_vars=exclude_vars)\n",
    "\n",
    "buffered_ds.to_netcdf(\n",
    "    ds_man.get_location() / f\"global_ocean_reanalysis/monthly_means/monthly_climate_{buffer_size}_buffer.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a = plt.subplots(1,2, figsize=[10,5])\n",
    "monthly_climate[\"mlotst\"].isel(time=99).plot(ax=a[0], cmap=\"jet\")\n",
    "buffer_attempt[\"mlotst\"].isel(time=99).plot(ax=a[1],cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_climate_1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_attempt.equals(monthly_climate.isel(time=slice(0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_attempt.isel(time=1)[\"mlotst\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsened_bath_A.isel(band=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_climate_1km[\"bathymetry_A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a = plt.subplots(1,2, figsize=[10,5])\n",
    "coral_climate_1km.isel(time=-1)[\"bathymetry_A\"].plot(ax=a[0], vmin=-100, vmax=0)\n",
    "coral_climate_1km.isel(time=-1)[\"mlotst\"].plot(ax=a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt.isel(time=-1)[\"mlotst\"].plot()\n",
    "eg_data = buffer_attempt.isel(time=-1)[\"mlotst\"]\n",
    "\n",
    "spatial_plots.plot_DEM(eg_data, f\" DEM upsampled to {target_resolution} meters\", \n",
    "    landmask=False, vmin=np.nanmin(eg_data.values), vmax=np.nanmax(eg_data.values), cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "\n",
    "# for longitude in array\n",
    "# get \n",
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: optionally replace batching with spatial batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GRU functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    num_epochs = 5\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position = 0):\n",
    "        y_pred, average_loss = tr_step(g_model, optimizer, X_train[:1000], y_train[:1000], batch_size=32, training=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for batch in range(num_batches):\n",
    "        #     array, y  = batcher_fun(X, 32, 276 \n",
    "        #     #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "        #     )\n",
    "        #     y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "        #  ## validation set \n",
    "        #  ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_log_likelihood(y_test,g_model(X_test))\n",
    "y_test=y_test[:1000]\n",
    "y_pred = g_model(X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made unnecssary due to isel indexing\n",
    "# def pixels_to_coord_diff(xa_ds: xa.Dataset | xa.DataArray, window_dim: int, coord: str) -> list[float, float]:\n",
    "#     return float(window_dim * np.diff(spatial_data.min_max_of_coords(xa_ds, coord)) / len(list(xa_coral_climate_1_12[coord])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample[\"bottomT\"].isel(time=-1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-16,144), coord_range=(-4,2))[\"coral_algae_1-12_degree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_working = xa_coral_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [lat_lon_vals_dict.items() for key in [\"latitude\", \"longitude\"]]\n",
    "{key: lat_lon_vals_dict[key] for key in [\"latitude\", \"longitude\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_y_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(list(subsample.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_X = np.moveaxis(np.array(test_array), 2, 1)\n",
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mask = ~np.all(np.isnan(test_array), axis=(0,2))\n",
    "sub_X = test_array[:, col_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features = ds_man.get_dataset(\"monthly_climate_features\")\n",
    "xa_coral_climate_1_12 = ds_man.get_dataset(\"monthly_climate_1_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, lat_lon_dict = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "Xs_nonans = naive_X_nan_replacement(Xs)\n",
    "ys, _ = subsample_to_array(xa_coral_climate_1_12, lat_lon_starts=(-10,142), coord_range=(-7,5), variables = [\"coral_algae_1-12_degree\"])\n",
    "ys = ys[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subset\n",
    "# subsample, lat_lon_vals_dict = sample_spatial_batch(xa_coral_climate_1_12_features,lat_lon_starts=(-12,144), coord_range=(-4,2))\n",
    "# subsample, lat_lon_vals_dict = sample_spatial_batch(xa_coral_climate_1_12_features,lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "# convert to ndarray\n",
    "# # test_array = spatial_data.xa_ds_to_3d_numpy(subsample)\n",
    "# # subsample_all, _ = sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-16,144), coord_range=(-4,2))\n",
    "# subsample_all, _ = sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-10,142), coord_range=(-7,5))\n",
    "# sub_y_nans = (np.array(subsample_all[\"coral_algae_1-12_degree\"].isel(time=-1))).reshape(-1, 1)\n",
    "# # remove nans\n",
    "# #sub_X, sub_y = filter_out_nans(test_array, sub_y_nans)\n",
    "# # testing, so replace nans with -1\n",
    "# # filter out columns that contain entirely NaN values\n",
    "# # col_mask = ~np.all(np.isnan(test_array), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "# # sub_X = test_array[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "\n",
    "# # sub_X = np.moveaxis(np.array(sub_X), 2, 1)\n",
    "# sub_y = sub_y_nans\n",
    "# # sub_X[np.isnan(sub_X)] = -10000\n",
    "# sub_y[np.isnan(sub_y)] = -10000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_dataset_var(xa_ds: xa.Dataset | xa.DataArray, subset_vals: np.ndarray, dims: list=['latitude', 'longitude', \"time\"]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_train = np.append(100*np.ones((50,50,5)), 1*np.ones((50,50,5)), 0)\n",
    "test_y_train = np.append(np.ones(50,), np.zeros(50,))\n",
    "\n",
    "\n",
    "print(\"test_x_train:\", test_x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"test_y_train:\", test_y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.normal(size = (32, 20, 1))    # shape: (num_samples, sequence_length, num_features)\n",
    "y_dud = np.random.choice([0, 1], size = 32)\n",
    "print(\"array shape:\", array.shape)\n",
    "print(\"y_dud shape:\", y_dud.shape)\n",
    "\n",
    "x_train, y_train = X[:500], y[:500].reshape(500,)\n",
    "# x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_model(test_x_train[:],training=False).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = g_model(X[:5610],training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30*187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(predicted.numpy().reshape(30,187))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(y_pred.numpy().reshape(20,25))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((y_pred > 0.5).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check log likelihood is computable\n",
    "negative_log_likelihood(y[:32], g_model(X[:32]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batcher function (by space and time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher_fun(X, y, batch_size, seq_len):\n",
    "    \"\"\"\n",
    "    A function to prepare the data for training the LSTM.\n",
    "    \n",
    "    :param data: The input data to the LSTM.\n",
    "    :param batch_size: The number of samples in each batch.\n",
    "    :param seq_len: The sequence length of each sample.\n",
    "    \n",
    "    :return: A tuple of (batch_x, batch_y), where batch_x is a numpy array of shape (batch_size, seq_len, num_features) \n",
    "             and batch_y is a numpy array of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(num_samples / batch_size)\n",
    "    num_features = spatial_data.shape[1]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        \n",
    "        batch_x = np.zeros((batch_size, seq_len, num_features))\n",
    "        batch_y = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            sample = data[j]\n",
    "            X = sample[:-1]\n",
    "            y = y[]\n",
    "            \n",
    "            batch_x[j - start_idx] = x.reshape((seq_len, num_features))\n",
    "            batch_y[j - start_idx, y] = 1\n",
    "            \n",
    "        yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 1\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            array, y  = batcher_fun(X, 32, 276 \n",
    "            #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "            )\n",
    "            y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "         ## validation set \n",
    "         ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copypasta\n",
    "\n",
    "[Source](https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam # https://stackoverflow.com/questions/75356826/attributeerror-adam-object-has-no-attribute-get-updates\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Model configuration\n",
    "additional_metrics = [\"accuracy\"]\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "# embedding_output_dims = 15\n",
    "# embedding_output_dims = 10\n",
    "loss_function = BinaryCrossentropy()\n",
    "# max_sequence_length = 300\n",
    "max_sequence_length = 276\n",
    "# num_distinct_words = 5000\n",
    "# num_distinct_words = 10000\n",
    "number_of_epochs = 5\n",
    "optimizer = Adam()\n",
    "validation_split = 0.20\n",
    "verbosity_mode = 1\n",
    "\n",
    "# Disable eager execution\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\n",
    "x_train, y_train = X[:5000], y[:5000].reshape((5000,))\n",
    "x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad all sequences: keras requires sequences of equal lengths. Should be handled in pre-processing, but here for now for security\n",
    "padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "\n",
    "# (number_samples, sequence_length, num_features)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = pad_sequences(x_train[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "padded_inputs_test = pad_sequences(x_test[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "\n",
    "# (number_samples, sequence_length)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        num_distinct_words+1, embedding_output_dims, input_length=max_sequence_length\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
    "\n",
    "# Give a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    padded_inputs,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=number_of_epochs,\n",
    "    verbose=verbosity_mode,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
    "print(f\"Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_timeseries(history) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history[\"accuracy\"])\n",
    "    ax.plot(history.history[\"val_accuracy\"])\n",
    "\n",
    "    ax.set_title(\"Model accuracy against epoch\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend(['train set', 'validation set'], loc='upper left')\n",
    "\n",
    "plot_score_timeseries(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate model\n",
    "\n",
    "[Source](https://medium.com/@canerkilinc/hands-on-multivariate-time-series-sequence-to-sequence-predictions-with-lstm-tensorflow-keras-ce86f2c0e4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = X[:32*10,:10,:3]\n",
    "print(\"X_toy:\", X_toy.shape)\n",
    "y_toy = y[:32*10]\n",
    "print(\"y_toy:\", y_toy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "#####################################\n",
    "#Before do anything else do not forget to reset the backend for the next iteration (rerun the model)\n",
    "tensorflow.keras.backend.clear_session()\n",
    "#####################################\n",
    "# Initialising the LSTM Model with MAE Loss-Function\n",
    "batch_size = 32\n",
    "epochs = 120\n",
    "timesteps = 10\n",
    "num_features = 3\n",
    "input_1 = Input(batch_shape=(batch_size,timesteps,num_features))\n",
    "#each layer is the input of the next layer\n",
    "lstm_hidden_layer_1 = LSTM(10, stateful=True, return_sequences=True)(input_1)\n",
    "lstm_hidden_layer_2 = LSTM(10, stateful=True, return_sequences=True)(lstm_hidden_layer_1)\n",
    "output_1 = Dense(units = 1)(lstm_hidden_layer_2)\n",
    "regressor_mae = Model(inputs=input_1, outputs = output_1)\n",
    "#adam is fast starting off and then gets slower and more precise\n",
    "#mae -> mean absolute error loss function\n",
    "regressor_mae.compile(optimizer='adam', loss = 'mae')\n",
    "#####################################\n",
    "#Summarize and observe the layers as well as paramter configurations\n",
    "regressor_mae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_mae.fit(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "new-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

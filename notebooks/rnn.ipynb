{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import data\n",
    "from coralshift.utils  import file_ops, directories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \"\"\"Handle the variety of datasets required to test and train model\"\"\"\n",
    "    # TODO: add in declaration of filepath root\n",
    "    def __init__(self):\n",
    "        self.datasets = {}\n",
    "        self.files_location = \"\"\n",
    "        # fetching external functions\n",
    "\n",
    "    def set_location(self, location=\"remote\"):\n",
    "        if location == \"remote\":\n",
    "            # change directory to home. TODO: make less hacky\n",
    "            os.chdir(\"/home/jovyan\")\n",
    "            self.files_location = Path(\"lustre_scratch/datasets/\")\n",
    "        elif location == \"local\":\n",
    "            self.files_location = directories.get_volume_dir()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def get_location(self):\n",
    "        return self.files_location\n",
    "\n",
    "\n",
    "    def add_dataset(self, name, data):\n",
    "        self.datasets[name] = data\n",
    "\n",
    "    def get_dataset(self, name):\n",
    "        return self.datasets.get(name, None)\n",
    "\n",
    "    def remove_dataset(self, name):\n",
    "        if name in self.datasets:\n",
    "            del self.datasets[name]\n",
    "\n",
    "    def list_datasets(self):\n",
    "        return list(self.datasets.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_strings(\n",
    "    str_list: list[str], exclude: list[str] = [\"latitude\", \"longitude\", \"depth\", \"time\"]\n",
    "):\n",
    "    \"\"\"Filters a list of strings to exclude those contained in a second list of excluded strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    str_list (list[str]): A list of strings to filter.\n",
    "    exclude (list[str], optional): A list of strings to exclude. Defaults to [\"latitude\", \"longitude\", \"depth\", \"time\"].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]: A list of the filtered strings.\n",
    "    \"\"\"\n",
    "    # Filter strings using list comprehension\n",
    "    filtered_strings = [s for s in str_list if s not in exclude]\n",
    "    # Return filtered strings\n",
    "    return filtered_strings\n",
    "\n",
    "def xa_ds_to_3d_numpy(\n",
    "    xa_ds: xa.Dataset,\n",
    "    exclude_vars: list[str] = [\"latitude\", \"longitude\", \"depth\", \"time\"],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert an xarray dataset to a 3D numpy array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xa_ds (xarray.Dataset): The xarray dataset to convert.\n",
    "    exclude_vars (list[str], optional): A list of variable names to exclude from the conversion.\n",
    "        Default is [\"latitude\", \"longitude\", \"depth\", \"time\"].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: The converted 3D numpy array.\n",
    "    \"\"\"\n",
    "    # stack the dataset\n",
    "    ds_stacked = xa_ds.stack(location=(\"latitude\", \"longitude\"))\n",
    "\n",
    "    array_list = []\n",
    "    variables_to_read = filter_strings(list(xa_ds.variables), exclude_vars)\n",
    "    for var in tqdm(variables_to_read):\n",
    "        vals = ds_stacked[var].values\n",
    "        array_list.append(vals)\n",
    "\n",
    "    # swap first and third columns. New shape: grid_cell_val x var x time\n",
    "    return np.moveaxis(np.array(array_list), 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 17337.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12223.20it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 25619 but corresponding boolean dimension is 5185",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m ds_man\u001b[39m.\u001b[39madd_dataset(\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdaily_climate_1_12_y_np\u001b[39m\u001b[39m\"\u001b[39m, ds_man\u001b[39m.\u001b[39mget_dataset(\u001b[39m\"\u001b[39m\u001b[39mmonthly_climate_1_12_y_np\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[39m# TODO: sort out depth dimension\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# TODO: process this and export it to new file\u001b[39;00m\n\u001b[1;32m     38\u001b[0m ds_man\u001b[39m.\u001b[39madd_dataset(\n\u001b[0;32m---> 39\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdaily_climate_1_12_X_np\u001b[39m\u001b[39m\"\u001b[39m, filter_out_nans(\n\u001b[1;32m     40\u001b[0m         data\u001b[39m.\u001b[39;49mxa_ds_to_3d_numpy(ds_man\u001b[39m.\u001b[39;49mget_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mdaily_climate_1_12\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49misel(depth\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)), ds_man\u001b[39m.\u001b[39;49mget_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mdaily_climate_1_12_y_np\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39m]\n\u001b[1;32m     41\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mfilter_out_nans\u001b[0;34m(X_with_nans, y_with_nans)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# swap axes so shape (num_samples, seq_length, num_params)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mswapaxes(X, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m y \u001b[39m=\u001b[39m y_with_nans[row_mask[:,\u001b[39m0\u001b[39;49m]]\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 25619 but corresponding boolean dimension is 5185"
     ]
    }
   ],
   "source": [
    "ds_man = MyDataset()\n",
    "\n",
    "# add datasets\n",
    "ds_man.set_location(\"remote\")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12\", xa.open_dataset(\n",
    "        Path(ds_man.get_location() / \"global_ocean_reanalysis/monthly_means/coral_climate_1_12.nc\"))\n",
    ")\n",
    "\n",
    "coral_climate_feature_vars = list(\n",
    "    set(ds_man.get_dataset(\"monthly_climate_1_12\").data_vars) - {'spatial_ref', 'coral_algae_1-12_degree', 'output'})\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_features\", ds_man.get_dataset(\"monthly_climate_1_12\")[coral_climate_feature_vars]\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12_y_np\", np.array(ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1)\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12_X_np\", filter_out_nans(\n",
    "        data.xa_ds_to_3d_numpy(ds_man.get_dataset(\"monthly_climate_1_12\")), ds_man.get_dataset(\"monthly_climate_1_12_y_np\"))[0]\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12\", xa.open_dataset(\n",
    "        Path(ds_man.get_location() / \"global_ocean_reanalysis/daily_means/dailies_combined.nc\"))\n",
    ")\n",
    "\n",
    "# same target as monthly\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12_y_np\", ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 1812.13it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 25619 but corresponding boolean dimension is 5185",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# For now, shallowest depth is taken (0.45)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# TODO: process this and export it to new file since takes a while to proceess\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ds_man\u001b[39m.\u001b[39madd_dataset(\n\u001b[0;32m----> 4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdaily_climate_1_12_X_np\u001b[39m\u001b[39m\"\u001b[39m, filter_out_nans(\n\u001b[1;32m      5\u001b[0m         data\u001b[39m.\u001b[39;49mxa_ds_to_3d_numpy(ds_man\u001b[39m.\u001b[39;49mget_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mdaily_climate_1_12\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49misel(depth\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)), ds_man\u001b[39m.\u001b[39;49mget_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mdaily_climate_1_12_y_np\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mfilter_out_nans\u001b[0;34m(X_with_nans, y_with_nans)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# swap axes so shape (num_samples, seq_length, num_params)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mswapaxes(X, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m y \u001b[39m=\u001b[39m y_with_nans[row_mask[:,\u001b[39m0\u001b[39;49m]]\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 25619 but corresponding boolean dimension is 5185"
     ]
    }
   ],
   "source": [
    "# For now, shallowest depth is taken (0.45)\n",
    "# TODO: process this and export it to new file since takes a while to proceess\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12_X_np\", filter_out_nans(\n",
    "        data.xa_ds_to_3d_numpy(ds_man.get_dataset(\"daily_climate_1_12\").isel(depth=0)), ds_man.get_dataset(\"daily_climate_1_12_y_np\"))[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(ds_man.get_dataset(\"monthly_climate_1_12_y_np\"), ds_man.get_dataset(\"monthly_climate_1_12_y_np\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this merge into data processing pipeline\n",
    "# merge daily mean files\n",
    "# var_daily_dir = Path(\"lustre_scratch/datasets/global_ocean_reanalysis/daily_means\")\n",
    "# save_combined_dailies_path = Path(\"lustre_scratch/datasets/global_ocean_reanalysis/daily_means/dailies_combined.nc\")\n",
    "# daily_file_paths = file_ops.return_list_filepaths(var_daily_dir, \".nc\")\n",
    "# combined_dailies = xa.open_mfdataset(daily_file_paths)\n",
    "# combined_dailies.to_netcdf(save_combined_dailies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3D array from xarray dataset variables. Shape: (num_samples, num_parameters, sequence_len)\n",
    "X_with_nans = data.xa_ds_to_3d_numpy(xa_coral_climate_1_12_features)\n",
    "print(f'X_with_nans shape (num_samples: {X_with_nans.shape[0]}, total num_parameters (includes nans parameters): {X_with_nans.shape[1]}, sequence_len: {X_with_nans.shape[2]})')\n",
    "\n",
    "for i, param in enumerate(xa_coral_climate_1_12_features.data_vars):\n",
    "    print(f\"{i}: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_nans.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations for which there are nan values\n",
    "\n",
    "99% sure these are are just gridcells containing land. Would be a good thing to investigate, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_nans(X_with_nans, y_with_nans):\n",
    "    # must be in shape (num_samples, num_params, seq_length)\n",
    "\n",
    "    # filter out columns that contain entirely NaN values\n",
    "    col_mask = ~np.all(np.isnan(X_with_nans), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "    masked_cols = X_with_nans[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "\n",
    "    # filter out all rows which contain any NaN values\n",
    "    row_mask = ~np.any(np.isnan(masked_cols), axis=1) # boolean mask indicating which rows to keep\n",
    "    masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "\n",
    "    # filter out all depths which contain any NaN values\n",
    "    depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "    X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "    # swap axes so shape (num_samples, seq_length, num_params)\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "\n",
    "    y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_1d(array):\n",
    "    \"\"\"Reshape a 2D array to a 1D array.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    array : numpy.ndarray\n",
    "        2D array to be reshaped.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        1D array representing the reshaped input array.\n",
    "    \"\"\"\n",
    "    return array.flatten()\n",
    "\n",
    "    \n",
    "def map_to_2d(array_1d, shape):\n",
    "    \"\"\"Map a 1D array back to the original 2D shape.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    array_1d : numpy.ndarray\n",
    "        1D array to be mapped.\n",
    "\n",
    "    shape : tuple\n",
    "        Shape of the original 2D array.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        2D array with values from the 1D array mapped to their correct coordinate points.\n",
    "    \"\"\"\n",
    "    return array_1d.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_with_nans\n",
    "# problem, probably with sea ice features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out columns that contain entirely NaN values\n",
    "col_mask = ~np.all(np.isnan(X), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "masked_cols = X[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "print(\"masked_cols shape:\", masked_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all rows which contain any NaN values\n",
    "row_mask = ~np.any(np.isnan(masked_cols), axis=1) # boolean mask indicating which rows to keep\n",
    "masked_cols_rows = masked_cols[row_mask[:,0], :, :] # keep only the rows that don't contain any NaN values\n",
    "masked_cols_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all depths which contain any NaN values\n",
    "depth_mask = ~np.any(np.isnan(masked_cols_rows), axis=(0,1)) # boolean mask indicating which depths to keep\n",
    "X = masked_cols_rows[:, :, depth_mask] # keep only the depths that don't contain any NaN values\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "print(f\"X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target from coral ground truth. Shape: (num_samples, 1)\n",
    "# TODO: not sure if this is shuffling the values when reshaping\n",
    "y_with_nans = np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].sel(\n",
    "    time=xa_coral_climate_1_12.time[-1])).reshape(-1, 1)\n",
    "# remove ys with nan values in other variables\n",
    "y = y_with_nans[row_mask[:,0]]\n",
    "\n",
    "print(f\"y_with_nans shape: {y_with_nans.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = filter_out_nans(X_with_nans, np.array(xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)).reshape(-1, 1))\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gated Recurrent Unit model class in TensorFlow\n",
    "class gru_model(tf.keras.Model):\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, rnn_units: list[int], num_layers: int):\n",
    "        \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "        layers to a desired output shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "        num_layers (int): number of layers in GRU model\n",
    "        \"\"\"\n",
    "        super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "        # store values for later use\n",
    "        self.num_layers = num_layers    # number of layers in GRU model\n",
    "        self.rnn_units = rnn_units\n",
    "        # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "        self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "                                   return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "                                   return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "        # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "        self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "        self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "\n",
    "\n",
    "    def call(self, inputs: np.ndarray, training: bool=False):\n",
    "        \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "        fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "            batch_size - defines the size of the sample drawn from datapoints\n",
    "            seq_length - number of timesteps in sequence\n",
    "            features - number of features associated with each datapoint\n",
    "        training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target: probability of an event occuring, with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "        assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "        # check that input tensor has correct shape\n",
    "        if (len(inputs.shape) != 3):\n",
    "            print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "        # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "        whole_seq = inputs\n",
    "\n",
    "        # iteratively passes input tensor to GRU layers, overwritting preceding sequence 'whole_seq'\n",
    "        for layer_num in range(self.num_layers):\n",
    "            whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "        # adding extra layers\n",
    "        target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "        target = tf.nn.relu(target) # via ReLU activation function\n",
    "        target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "        # obtain a probability value between 0 and 1\n",
    "        target = tf.nn.sigmoid(target)\n",
    "        \n",
    "        return target\n",
    "\n",
    "    \n",
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities.\n",
    "    N.B. Binary cross-entropy loss defined as the negative log likelihood of the binary labels given the predicted\n",
    "    probabilities\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()    \n",
    "    return bce(y, y_pred)\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "    # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: np.ndarray, y: np.ndarray, \n",
    "        training: bool=True, batch_size: int=None) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "        negative_log_likelihood)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "        y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            num_samples = X.shape[0]\n",
    "            num_batches = num_samples // batch_size\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for batch in tqdm(range(num_batches), desc=\"batches\", position=1, leave=False):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = (batch + 1) * batch_size\n",
    "\n",
    "                X_batch = X[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = gru(X, training) \n",
    "                    xent = negative_log_likelihood(y, y_pred)\n",
    "                \n",
    "                gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "\n",
    "                total_loss += xent\n",
    "\n",
    "            # TODO: return iterative loss\n",
    "            average_loss = total_loss / num_batches\n",
    "            # return predicted output values and total loss value\n",
    "            return y_pred, xent\n",
    "\n",
    "    # set default float type\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: optionally replace batching with spatial batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GRU functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "g_model = gru_model([500], 1) # N.B. [x] is number of hidden layers in GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model(X[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create latitude, longitude, and time coordinates\n",
    "# latitude = np.linspace(0, 30, 15)\n",
    "# longitude = np.linspace(0, 10, 10)\n",
    "# time = pd.date_range(start='1/1/2018', end='1/08/2018')\n",
    "\n",
    "# # Create target array\n",
    "# target = np.zeros((len(latitude), len(longitude), len(time)))\n",
    "# target[:len(latitude) // 3, :, :] = 1\n",
    "\n",
    "# # Create variables var1, var2, var3\n",
    "# var1 = np.where(target == 1, 100, 10)\n",
    "# var2 = np.where(target == 1, 100, 10)\n",
    "# var3 = np.where(target == 1, 100, 10)\n",
    "\n",
    "# # Create xarray dataset\n",
    "# dataset = xa.Dataset(\n",
    "#     {\n",
    "#         'var1': (['latitude', 'longitude', 'time'], var1),\n",
    "#         'var2': (['latitude', 'longitude', 'time'], var2),\n",
    "#         'var3': (['latitude', 'longitude', 'time'], var3),\n",
    "#         'target': (['latitude', 'longitude', 'time'], target),\n",
    "#     },\n",
    "#     coords={\n",
    "#         'latitude': latitude,\n",
    "#         'longitude': longitude,\n",
    "#         'time': time,\n",
    "#     },\n",
    "# )\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made unnecssary due to isel indexing\n",
    "# def pixels_to_coord_diff(xa_ds: xa.Dataset | xa.DataArray, window_dim: int, coord: str) -> list[float, float]:\n",
    "#     return float(window_dim * np.diff(data.min_max_of_coords(xa_ds, coord)) / len(list(xa_coral_climate_1_12[coord])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spatial_batch(xa_ds: xa.Dataset, lat_lon_starts: tuple=(0,0), window_dims: tuple[int,int] = (6,6), \n",
    "    coord_range: tuple[float]=None, variables: list[str] = None) -> np.ndarray:\n",
    "    \"\"\"Sample a spatial batch from an xarray Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        xa_ds (xa.Dataset): The input xarray Dataset.\n",
    "        lat_lon_starts (tuple): Tuple specifying the starting latitude and longitude indices of the batch.\n",
    "        window_dims (tuple[int, int]): Tuple specifying the dimensions (number of cells) of the spatial window.\n",
    "        coord_range (tuple[float]): Tuple specifying the latitude and longitude range (in degrees) of the spatial \n",
    "            window. If provided, it overrides the window_dims parameter.\n",
    "        variables (list[str]): List of variable names to include in the spatial batch. If None, includes all variables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        np.ndarray: The sampled spatial batch as a NumPy array.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "        - The function selects a subset of the input dataset based on the provided latitude, longitude indices, and window dimensions.\n",
    "        - If a coord_range is provided, it is used to compute the latitude and longitude indices of the spatial window.\n",
    "        - The function returns the selected subset as a NumPy array.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        # Sample a spatial batch from an xarray Dataset\n",
    "        dataset = ...\n",
    "        lat_lon_starts = (2, 3)\n",
    "        window_dims = (6, 6)\n",
    "        coord_range = (2.5, 3.5)\n",
    "        variables = ['var1', 'var2', 'var3']\n",
    "        spatial_batch = sample_spatial_batch(dataset, lat_lon_starts, window_dims, coord_range, variables)\n",
    "    \"\"\"\n",
    "    # N.B. have to be careful when providing coordinate ranges for areas with negative coords. TODO: make universal\n",
    "    lat_start, lon_start = lat_lon_starts[0], lat_lon_starts[1]\n",
    "    if not coord_range:\n",
    "        subset = xa_ds.isel({\"latitude\": slice(lat_start,window_dims[0]), \n",
    "                            \"longitude\": slice(lon_start,window_dims[1])})\n",
    "    else:\n",
    "        lat_cells, lon_cells = coord_range[0], coord_range[1]\n",
    "        subset = xa_ds.sel({\"latitude\": slice(lat_start,lat_start+lat_cells), \n",
    "                            \"longitude\": slice(lon_start,lon_start+lon_cells)})\n",
    "\n",
    "    lat_slice = subset[\"latitude\"].values\n",
    "    lon_slice = subset[\"longitude\"].values\n",
    "    time_slice = subset[\"time\"].values\n",
    "\n",
    "    return subset, {\"latitude\": lat_slice, \"longitude\": lon_slice, \"time\": time_slice}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample[\"bottomT\"].isel(time=-1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-16,144), coord_range=(-4,2))[\"coral_algae_1-12_degree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_working = xa_coral_climate_1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [lat_lon_vals_dict.items() for key in [\"latitude\", \"longitude\"]]\n",
    "{key: lat_lon_vals_dict[key] for key in [\"latitude\", \"longitude\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_y_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(list(subsample.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_X = np.moveaxis(np.array(test_array), 2, 1)\n",
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mask = ~np.all(np.isnan(test_array), axis=(0,2))\n",
    "sub_X = test_array[:, col_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subset\n",
    "subsample, lat_lon_vals_dict = sample_spatial_batch(xa_coral_climate_1_12_features,lat_lon_starts=(-12,144), coord_range=(-4,2))\n",
    "# convert to ndarray\n",
    "test_array = data.xa_ds_to_3d_numpy(subsample)\n",
    "subsample_all, _ = sample_spatial_batch(xa_coral_climate_1_12,lat_lon_starts=(-16,144), coord_range=(-4,2))\n",
    "sub_y_nans = (np.array(subsample_all[\"coral_algae_1-12_degree\"].isel(time=-1))).reshape(-1, 1)\n",
    "# remove nans\n",
    "#sub_X, sub_y = filter_out_nans(test_array, sub_y_nans)\n",
    "# testing, so replace nans with -1\n",
    "# filter out columns that contain entirely NaN values\n",
    "col_mask = ~np.all(np.isnan(test_array), axis=(0,2)) # boolean mask indicating which columns to keep\n",
    "sub_X = test_array[:, col_mask, :] # keep only the columns that don't contain entirely NaN values\n",
    "\n",
    "sub_X = np.moveaxis(np.array(sub_X), 2, 1)\n",
    "sub_y = sub_y_nans\n",
    "sub_X[np.isnan(sub_X)] = -10000\n",
    "sub_y[np.isnan(sub_y)] = -10000\n",
    "\n",
    "### train/predict\n",
    "# if training:\n",
    "    # assign epoch output to dataset\n",
    "# if testing:\n",
    "predicted = g_model(sub_X, training=False)\n",
    "    # assign output predictions to dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape to original dimensions\n",
    "original_shape = [subsample.dims[d] for d in ['latitude', 'longitude', 'time']]\n",
    "original_shape += [len(list(subsample.data_vars))]\n",
    "original_shape = tuple(original_shape)\n",
    "\n",
    "output = predicted.numpy().reshape(original_shape[:2])\n",
    "\n",
    "# Create a new variable in the dataset using the output subset\n",
    "xa_coral_climate_1_12_working['output'] = xa.DataArray(output, dims=['latitude', 'longitude'], coords={key: lat_lon_vals_dict[key] for key in [\"latitude\", \"longitude\"]})\n",
    "\n",
    "lat_start, lat_end = lat_lon_vals_dict[\"latitude\"].min(), lat_lon_vals_dict[\"latitude\"].max()\n",
    "lon_start, lon_end = lat_lon_vals_dict[\"longitude\"].min(), lat_lon_vals_dict[\"longitude\"].max()\n",
    "\n",
    "# Set values outside the subset range to NaN\n",
    "xa_coral_climate_1_12_working['output'] = xa_coral_climate_1_12_working['output'].where((xa_coral_climate_1_12_working['output'].latitude >= lat_start) & (xa_coral_climate_1_12_working['output'].latitude <= lat_end) &\n",
    "                                      (xa_coral_climate_1_12_working['output'].longitude >= lon_start) & (xa_coral_climate_1_12_working['output'].longitude <= lon_end), np.nan)\n",
    "\n",
    "\n",
    "                                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise result\n",
    "fig, ax = plt.subplots(1,2, figsize=[15,5])\n",
    "\n",
    "xa_coral_climate_1_12_working[\"coral_algae_1-12_degree\"].isel(time=-1).plot(ax=ax[0])\n",
    "xa_coral_climate_1_12_working[\"output\"].plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_dataset_var(xa_ds: xa.Dataset | xa.DataArray, subset_vals: np.ndarray, dims: list=['latitude', 'longitude', \"time\"]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_train = np.append(100*np.ones((50,50,5)), 1*np.ones((50,50,5)), 0)\n",
    "test_y_train = np.append(np.ones(50,), np.zeros(50,))\n",
    "\n",
    "\n",
    "print(\"test_x_train:\", test_x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"test_y_train:\", test_y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.normal(size = (32, 20, 1))    # shape: (num_samples, sequence_length, num_features)\n",
    "y_dud = np.random.choice([0, 1], size = 32)\n",
    "print(\"array shape:\", array.shape)\n",
    "print(\"y_dud shape:\", y_dud.shape)\n",
    "\n",
    "x_train, y_train = X[:500], y[:500].reshape(500,)\n",
    "# x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "# print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that untrained model runs (should output array of non-nan values)\n",
    "# g_model(x_train)\n",
    "g_model(test_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 5\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in tqdm(range(num_epochs), desc= \" epochs\", position = 0):\n",
    "        # y_pred, average_loss = tr_step(g_model, optimizer, x_train, y_train, batch_size=32, training=True)\n",
    "        y_pred, average_loss = tr_step(g_model, optimizer, test_x_train, test_y_train, batch_size=32, training=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for batch in range(num_batches):\n",
    "        #     array, y  = batcher_fun(X, 32, 276 \n",
    "        #     #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "        #     )\n",
    "        #     y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "        #  ## validation set \n",
    "        #  ## test_set \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_model(test_x_train[:],training=False).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = g_model(X[:5610],training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30*187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(predicted.numpy().reshape(30,187))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_coral_climate_1_12[\"coral_algae_1-12_degree\"].isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.imshow(y_pred.numpy().reshape(20,25))\n",
    "fig.colorbar(out, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((y_pred > 0.5).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check log likelihood is computable\n",
    "negative_log_likelihood(y[:32], g_model(X[:32]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batcher function (by space and time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher_fun(X, y, batch_size, seq_len):\n",
    "    \"\"\"\n",
    "    A function to prepare the data for training the LSTM.\n",
    "    \n",
    "    :param data: The input data to the LSTM.\n",
    "    :param batch_size: The number of samples in each batch.\n",
    "    :param seq_len: The sequence length of each sample.\n",
    "    \n",
    "    :return: A tuple of (batch_x, batch_y), where batch_x is a numpy array of shape (batch_size, seq_len, num_features) \n",
    "             and batch_y is a numpy array of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(num_samples / batch_size)\n",
    "    num_features = data.shape[1]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        \n",
    "        batch_x = np.zeros((batch_size, seq_len, num_features))\n",
    "        batch_y = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            sample = data[j]\n",
    "            X = sample[:-1]\n",
    "            y = y[]\n",
    "            \n",
    "            batch_x[j - start_idx] = x.reshape((seq_len, num_features))\n",
    "            batch_y[j - start_idx, y] = 1\n",
    "            \n",
    "        yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    num_epochs = 1\n",
    "    num_batches = 100\n",
    "    tr_step = build_graph()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            array, y  = batcher_fun(X, 32, 276 \n",
    "            #training = True)# shapes: (batch_s, seq_l, features), (batch_s, 1)\n",
    "            )\n",
    "            y_pred, xent = tr_step(g_model, optimizer, X[:32], y, training=True)\n",
    "            \n",
    "         ## validation set \n",
    "         ## test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copypasta\n",
    "\n",
    "[Source](https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam # https://stackoverflow.com/questions/75356826/attributeerror-adam-object-has-no-attribute-get-updates\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Model configuration\n",
    "additional_metrics = [\"accuracy\"]\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "# embedding_output_dims = 15\n",
    "# embedding_output_dims = 10\n",
    "loss_function = BinaryCrossentropy()\n",
    "# max_sequence_length = 300\n",
    "max_sequence_length = 276\n",
    "# num_distinct_words = 5000\n",
    "# num_distinct_words = 10000\n",
    "number_of_epochs = 5\n",
    "optimizer = Adam()\n",
    "validation_split = 0.20\n",
    "verbosity_mode = 1\n",
    "\n",
    "# Disable eager execution\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\n",
    "x_train, y_train = X[:5000], y[:5000].reshape((5000,))\n",
    "x_test, y_test = X[5000:6000], y[5000:6000].reshape((1000,))\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad all sequences: keras requires sequences of equal lengths. Should be handled in pre-processing, but here for now for security\n",
    "padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n",
    "\n",
    "# (number_samples, sequence_length, num_features)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = pad_sequences(x_train[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "padded_inputs_test = pad_sequences(x_test[:,:,0], maxlen=max_sequence_length, value = 0.0)\n",
    "\n",
    "# (number_samples, sequence_length)\n",
    "print(\"padded_inputs:\", padded_inputs.shape)\n",
    "print(\"padded_inputs_test:\", padded_inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        num_distinct_words+1, embedding_output_dims, input_length=max_sequence_length\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
    "\n",
    "# Give a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    padded_inputs,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=number_of_epochs,\n",
    "    verbose=verbosity_mode,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
    "print(f\"Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_timeseries(history) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history[\"accuracy\"])\n",
    "    ax.plot(history.history[\"val_accuracy\"])\n",
    "\n",
    "    ax.set_title(\"Model accuracy against epoch\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend(['train set', 'validation set'], loc='upper left')\n",
    "\n",
    "plot_score_timeseries(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate model\n",
    "\n",
    "[Source](https://medium.com/@canerkilinc/hands-on-multivariate-time-series-sequence-to-sequence-predictions-with-lstm-tensorflow-keras-ce86f2c0e4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = X[:32*10,:10,:3]\n",
    "print(\"X_toy:\", X_toy.shape)\n",
    "y_toy = y[:32*10]\n",
    "print(\"y_toy:\", y_toy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "#####################################\n",
    "#Before do anything else do not forget to reset the backend for the next iteration (rerun the model)\n",
    "tensorflow.keras.backend.clear_session()\n",
    "#####################################\n",
    "# Initialising the LSTM Model with MAE Loss-Function\n",
    "batch_size = 32\n",
    "epochs = 120\n",
    "timesteps = 10\n",
    "num_features = 3\n",
    "input_1 = Input(batch_shape=(batch_size,timesteps,num_features))\n",
    "#each layer is the input of the next layer\n",
    "lstm_hidden_layer_1 = LSTM(10, stateful=True, return_sequences=True)(input_1)\n",
    "lstm_hidden_layer_2 = LSTM(10, stateful=True, return_sequences=True)(lstm_hidden_layer_1)\n",
    "output_1 = Dense(units = 1)(lstm_hidden_layer_2)\n",
    "regressor_mae = Model(inputs=input_1, outputs = output_1)\n",
    "#adam is fast starting off and then gets slower and more precise\n",
    "#mae -> mean absolute error loss function\n",
    "regressor_mae.compile(optimizer='adam', loss = 'mae')\n",
    "#####################################\n",
    "#Summarize and observe the layers as well as paramter configurations\n",
    "regressor_mae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_mae.fit(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "new-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose whether to work on a remote machine\n",
    "location = \"remote\"\n",
    "# location = \"local\"\n",
    "\n",
    "if location == \"remote\":\n",
    "    # change this line to the where the GitHub repository is located\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data storage setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "# import math as m\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.interpolate import interp2d\n",
    "from sklearn.utils import class_weight\n",
    "from scipy.ndimage import gaussian_gradient_magnitude\n",
    "# import xbatcher\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray as rio\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories, utils\n",
    "# from coralshift.plotting import spatial_plots, model_results\n",
    "from coralshift.dataloading import data_structure, climate_data, bathymetry, reef_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_man = data_structure.MyDatasets()\n",
    "ds_man.set_location(location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify your area of interest\n",
    "\n",
    "The availability of high-resolution (30m) bathymetric data means that areas of interest are currently confided to 4 areas on the Great Barrier Reef (GBR). The following code generates a geoJSON file specifying which area (A-D) you'd like to investigate:\n",
    "\n",
    "| Reef Area Name                \t| Latitudes \t| Longitudes \t|\n",
    "|-------------------------------\t|-----------\t|------------\t|\n",
    "| Great Barrier Reef A 2020 30m \t| 10-17°S   \t| 143-147°E  \t|\n",
    "| Great Barrier Reef B 2020 30m \t| 16-23°S   \t| 144-149°E  \t|\n",
    "| Great Barrier Reef C 2020 30m \t| 18-24°S   \t| 148-154°E  \t|\n",
    "| Great Barrier Reef D 2020 30m \t| 23-29°S   \t| 150-156°E  \t|\n",
    "\n",
    "\n",
    "Download your required area from here: https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/115066\n",
    "\n",
    "Due to the computational load required to run ML models on such a high resolution data, bathymetric data is currently upsampled to 4km grid cells and areas are limited to a quarter of the GBR's total area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be downsampled to 0.00923 degrees (~1000m).\n"
     ]
    }
   ],
   "source": [
    "# choose resolution (should be above 1000m for processing in decent time)\n",
    "target_resolution_m, target_resolution_d = spatial_data.choose_resolution(\n",
    "    resolution=1000, unit=\"m\")\n",
    "\n",
    "# convert distance to degrees:\n",
    "# _,_,av_degrees = spatial_data.distance_to_degrees(target_resolution)\n",
    "print(f\"Data will be downsampled to {target_resolution_d:.05f} degrees (~{target_resolution_m}m).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: /lustre_scratch/orlando-code/datasets/bathymetry/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif\n",
      "bathymetry_A already exists in /lustre_scratch/orlando-code/datasets/bathymetry.\n",
      "bathymetry_A_0-00923_upsampled already exists in /lustre_scratch/orlando-code/datasets/bathymetry.\n"
     ]
    }
   ],
   "source": [
    "# select your area\n",
    "area_name = \"A\"\n",
    "\n",
    "# download .tif if not downloaded aready\n",
    "reef_areas = bathymetry.ensure_bathymetry_downloaded(area_name)\n",
    "# cast tif to processed xarray\n",
    "xa_bath = spatial_data.tif_to_xarray(\n",
    "    directories.get_bathymetry_datasets_dir() / reef_areas.get_filename(area_name), reef_areas.get_xarray_name(area_name)\n",
    "    )\n",
    "# bath_name = f\"{reef_areas.get_xarray_name(area_name)}\"\n",
    "# save to files\n",
    "bath_name = file_ops.save_nc(\n",
    "    directories.get_bathymetry_datasets_dir(),\n",
    "    f\"{reef_areas.get_xarray_name(area_name)}\", \n",
    "    xa_bath)\n",
    "\n",
    "\n",
    "# upsample to specified resolution\n",
    "xa_bath_upsampled = spatial_data.upsample_xarray_to_target(xa_bath, target_resolution=target_resolution_d)\n",
    "# upsampled_bath_name = file_ops.replace_dot_with_dash(\n",
    "#     f\"{reef_areas.get_xarray_name(area_name)}_{target_resolution_d:.05f}_upsampled\")\n",
    "# save file TODO: return name, move replacedot to utils\n",
    "upsampled_bath_name = file_ops.save_nc(\n",
    "    directories.get_bathymetry_datasets_dir(), \n",
    "    f\"{reef_areas.get_xarray_name(area_name)}_{target_resolution_d:.05f}_upsampled\", \n",
    "    xa_bath_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: better visualisation, investigate speed of plotting. Cell seems not to load when calling values...\n",
    "# xa_bath_upsampled.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_gradient_magnitude(dataarray, sigma, output_path):\n",
    "    # Calculate the gradient magnitude using gaussian_gradient_magnitude\n",
    "    gradient_magnitude = gaussian_gradient_magnitude(dataarray.values, sigma=sigma)\n",
    "\n",
    "    # Create a new DataArray with the gradient magnitude\n",
    "    grad_magnitude_dataarray = xr.DataArray(\n",
    "        gradient_magnitude,\n",
    "        coords=dataarray.coords,\n",
    "        dims=dataarray.dims,\n",
    "        attrs=dataarray.attrs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_save_gradient_magnitude(xa_bath_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradient_nc(\n",
    "    bathymetry_name,\n",
    "    kernel_size: int = 1,\n",
    "    return_array: bool = False,\n",
    ") -> xa.DataArray:\n",
    "    gradient_dir = directories.get_gradients_dir()\n",
    "    gradient_path = gradient_dir / f\"{bathymetry_name}_gradients\"\n",
    "\n",
    "    if not gradient_path.is_file():\n",
    "        xa_bath = xa.open_dataset(\n",
    "            (directories.get_bathymetry_datasets_dir() / bathymetry_name).with_suffix(\n",
    "                \".nc\"\n",
    "            )\n",
    "        )\n",
    "        bath_vals = xa_bath.compute()\n",
    "        gradients = gaussian_gradient_magnitude(bath_vals, sigma=kernel_size)\n",
    "        xa_gradients = xa_bath.copy(data={\"gradients\": gradients})\n",
    "        file_ops.save_nc(gradient_dir, gradient_path.stem, xa_gradients)\n",
    "    else:\n",
    "        gradients = xa.open_dataset(gradient_path)\n",
    "\n",
    "    if return_array:\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xa_bath.values\n",
    "# xa_bath_upsampled.values\n",
    "xa_bath_upsampled_vals = xa_bath_upsampled.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_cell_slopes(values, sigma: int = 1):\n",
    "#     grad_func = gaussian_gradient_magnitude(values, sigma=sigma)\n",
    "\n",
    "#     return xa.apply_ufunc(grad_func, values, kwargs={\"sigma\": sigma}, dask=\"parallelized\", vectorize=True)\n",
    "\n",
    "# out = calculate_cell_slopes(xa_bath_upsampled)\n",
    "\n",
    "def apply_gaussian_gradient_magnitude(da: xr.DataArray, sigma: float) -> xr.DataArray:\n",
    "    # Convert DataArray to numpy array\n",
    "    array = da.values\n",
    "    \n",
    "    # Apply gaussian_gradient_magnitude to the array\n",
    "    gradient_array = gaussian_gradient_magnitude(array, sigma=sigma)\n",
    "    \n",
    "    # Create a new DataArray with the updated values\n",
    "    gradient_da = xa.DataArray(gradient_array, coords=da.coords, dims=da.dims, attrs=da.attrs)\n",
    "    \n",
    "    return gradient_da\n",
    "\n",
    "# def apply_gaussian_gradient_magnitude(da: xr.DataArray, sigma: float) -> xr.DataArray:\n",
    "#     # Convert DataArray to Dask array\n",
    "#     dask_array = da.data\n",
    "    \n",
    "#     # Apply gaussian_gradient_magnitude to the Dask array\n",
    "#     gradient_array = da.map_blocks(lambda block: gaussian_gradient_magnitude(block, sigma=sigma))\n",
    "    \n",
    "#     # Create a new DataArray with the Dask array\n",
    "#     gradient_da = xr.DataArray(gradient_array, coords=da.coords, dims=da.dims, attrs=da.attrs)\n",
    "    \n",
    "#     return gradient_da\n",
    "\n",
    "result = apply_gaussian_gradient_magnitude(xa_bath_upsampled, sigma=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a =plt.subplots(1,2, figsize=[14,5])\n",
    "result.plot(ax=a[0])\n",
    "xa_bath_upsampled_vals.plot(ax=a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = gaussian_gradient_magnitude(xa_bath_upsampled_vals, sigma=1)\n",
    "xa_gradients = xa_bath_upsampled.copy(data={\"gradients\": gradients})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slopes of upsampled bathymetry and save to nc file\n",
    "generate_gradient_nc(xa_bath_upsampled_vals, return_array=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coral ground truth: Allen Coral Atlas\n",
    "\n",
    "\n",
    "There is currently no API for accessing data directly from your local machine. Please follow the instructions* below:\n",
    "1. Make an account on the [Allen Coral Atlas](https://allencoralatlas.org/atlas/#6.00/-13.5257/144.5000) webpage\n",
    "2. Generate a geojson file using the code cell below (generated in the `reef_baseline` directory)\n",
    "\n",
    "*Instructions correct as of 30.06.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate geojson file in reef_baseline directory for download from the Allen Coral Atlas\n",
    "geojson_path = reef_extent.generate_area_geojson(\n",
    "    area_class = reef_areas, area_name=area_name, save_dir=directories.get_reef_baseline_dir())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Upload the geojson file via:\n",
    "\n",
    "    \\> My Areas > Upload a GeoJSON or KML file\n",
    "4. Wait for the area to be processed, and select \"Benthic Map (OGC GeoPackage (.gpkg))\". Sign the terms and conditions \n",
    "and select \"Prepare Download\". After ~two minutes a sequence of emails will arrive notifying you that your download is ready.\n",
    "5. Download the file and unzip it using a unzipping utility. Then,\n",
    "    - add the `benthic.geojson` to the `reef_baseline` directory\n",
    "    \n",
    "    \n",
    "6. Continue with the subsequent code cells.\n",
    "\n",
    "----\n",
    "\n",
    "You have now downloaded:\n",
    "\n",
    "**`benthic.gpkg`**\n",
    "\n",
    "This is a dataframe of Shapely objects (\"geometry\" polygons) defining the boundaries of different benthic classes:\n",
    "\n",
    "| Class           \t| Number of polygons \t|\n",
    "|-----------------\t|--------------------\t|\n",
    "| Coral/Algae     \t| 877787             \t|\n",
    "| Rock            \t| 766391             \t|\n",
    "| Rubble          \t| 568041             \t|\n",
    "| Sand            \t| 518805             \t|\n",
    "| Microalgal Mats \t| 27569              \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benthic_df = file_ops.check_pkl_else_read_gpkg(directories.get_reef_baseline_dir(), filename = \"benthic.pkl\")\n",
    "benthic_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterize polygons\n",
    "\n",
    "Rasterized arrays are necessary to process the geospatial data e.g. to align different gridcells. Doing this locally through rasterio requires such significant compute that clouds computing is the only reasonable option. A JavaScript file for use with Google Earth Engine (GEE) is provided in the `coralshift` repo. Visit [this page](https://developers.google.com/earth-engine/guides/getstarted) for information regarding setting up a GEE account and getting started.\n",
    "\n",
    "GEE requires shapefile (.shp) to ingest data. This is generated in the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process df to gpd.GeoDataFrame. We are interested only in the \"Coral/Algae\" class, so gdf is limited to these rows\n",
    "gdf_coral = reef_extent.process_benthic_pd(benthic_df)\n",
    "# save as shapely file for rasterisation in GEE, if not already present\n",
    "reef_extent.generate_coral_shp(gdf_coral)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ingest the shapefile (and all accompanying files: .cpg, .dbf, .prj, .shx) as an asset.\n",
    "2. Import the subsequent `Table` into the script.\n",
    "3. Run the script, and submit the `coral_A_north` and `coral_A_south` tasks. Sit back and wait! After ~1 hour the rasters will be available to download from your Google Drive as GeoTIFFS: after this, please add them to the reef_baseline directory and carry on with the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "gbr_30_files = file_ops.return_list_filepaths(gbr_30_dir, \".tif\")\n",
    "# generate dictionary of file names and arrays: {filename: xarray.DataArray, ...}\n",
    "gbr_30_dict_preprocess = spatial_data.tifs_to_xa_array_dict(gbr_30_files)\n",
    "# process xa_arrays\n",
    "# gbr_30_dict = spatial_data.process_xa_arrays_in_dict(gbr_30_dict_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_eg = rio.open_rasterio(rasterio.open(directories.get_reef_baseline_dir() / \"gt_tifs/coral_raster_1000m.tif\")).squeeze(\"band\")\n",
    "gt_4km = xa.open_dataarray(directories.get_reef_baseline_dir() / \"gt_tifs/gt_nc_dir/concatenated_0-03691_degree.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_4km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_4km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2, figsize=[14,8])\n",
    "tif_eg.sel({\"x\":slice(142,142.4), \"y\":slice(-10.6,-11)}).plot(ax=ax[0])\n",
    "xa_4km[\"__xarray_dataarray_variable__\"].sel({\"longitude\":slice(142,142.4), \"latitude\":slice(-11,-10.6)}).plot(ax=ax[1])\n",
    "ax[0].set_aspect(\"equal\")\n",
    "ax[1].set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=xa.open_dataarray(directories.get_reef_baseline_dir() / \"coral_A_south-0000065536-0000065536.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa.where(test>0, 1, test).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_coral_gt_tifs(tif_dir_name=None, target_resolution_d:float=None):\n",
    "#     if not tif_dir_name:\n",
    "#         tif_dir = directories.get_reef_baseline_dir()\n",
    "#     else:\n",
    "#         tif_dir = directories.get_reef_baseline_dir() / tif_dir_name\n",
    "    \n",
    "#     nc_dir = file_ops.guarantee_existence(tif_dir / \"gt_nc_dir\")\n",
    "#     # save tifs to ncs in new dir\n",
    "#     tif_paths = tifs_to_ncs(nc_dir, target_resolution_d)\n",
    "#     # get list of nc paths in dir\n",
    "#     xa_arrays_list = [tif_to_xa_array(tif_path) for tif_path in tif_paths]\n",
    "#     # merge ncs into one mega nc file\n",
    "#     if len(xa_arrays_list) > 1:\n",
    "#         concatted = xa.concat(xa_arrays_list, dim=[\"latitude\",\"longitude\"])\n",
    "#     else:\n",
    "#         concatted = xa_arrays_list[0]\n",
    "#     file_ops.save_nc(nc_dir, f\"concatenated_{target_resolution_d:.05f}_degree\", concatted)\n",
    "\n",
    "\n",
    "# def tifs_to_ncs(nc_dir: Path | list[str], target_resolution_d: float=None) -> None:\n",
    "\n",
    "#     tif_dir = nc_dir.parent\n",
    "#     tif_paths = file_ops.return_list_filepaths(tif_dir, \".tif\")\n",
    "#     xa_array_dict = {}\n",
    "#     for tif_path in tqdm(tif_paths, total=len(tif_paths), desc=\"Writing tifs to nc files\"):\n",
    "#         # filename = str(file_ops.get_n_last_subparts_path(tif, 1))\n",
    "#         filename = tif_path.stem\n",
    "#         tif_array = tif_to_xa_array(tif_path)\n",
    "#         # xa_array_dict[filename] = tif_array.rename(filename)\n",
    "#         if target_resolution_d:\n",
    "#             tif_array = spatial_data.upsample_xarray_to_target(xa_array=tif_array, target_resolution=target_resolution_d)\n",
    "#         # save array to nc file\n",
    "#         file_ops.save_nc(tif_dir, filename, tif_array)\n",
    "\n",
    "#     return tif_paths\n",
    "#     print(f\"All tifs converted to xarrays and stored as .nc files in {nc_dir}.\")\n",
    "\n",
    "\n",
    "# def tif_to_xa_array(tif_path) -> xa.DataArray:\n",
    "#     return spatial_data.process_xa_d(rio.open_rasterio(rasterio.open(tif_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef_extent.process_coral_gt_tifs(tif_dir_name=\"gt_tifs\", target_resolution_d=target_resolution_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifs_to_ncs(gbr_30_dir, target_resolution_d=target_resolution_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_path = gbr_30_dir / \"coral_A_south-0000000000-0000000000.tif\"\n",
    "check = tif_to_xa_array(tif_path)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.sel({\"longitude\":slice(143.3,143.6), \"latitude\":slice(-13.7,-13.5)}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = combine_tifs_to_xarray(gbr_30_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"coral_A_south-0000065536-0000032768.tif\"].isel({\"x\":slice(0,100),\"y\":slice(0,100)}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tif_eg.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_eg.sel({\"x\":slice(143.5,143.7), \"y\":slice(-13.6,-13.7)}).plot(cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_30_dict_preprocess['coral_A_south-0000065536-0000032768.tif'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Ocean Physics Reanalysis\n",
    "\n",
    "The dataset metadata can be accessed [here](https://doi.org/10.48670/moi-00021)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "You're required to set up an account with the [Copernicus Marine Service](https://marine.copernicus.eu/). \n",
    "\n",
    "\n",
    "**Warning:**  this is a large amount of data for which the only way to gather it is to query the copernicus API via motu. Requests are queued, and request sizes are floated to the top of the queue. The following functions take advantage of this by splitting a single request up by date adn variable before amalgamating the files, but this can still take a **very long time**, and vary significantly depending on overall website traffic. For those who aren't interested in the entire database, it's highly recommended that you use the toy dataset provided as a `.npy` file in the GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file already exists at /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/cmems_gopr_monthly.nc\n"
     ]
    }
   ],
   "source": [
    "# download monthly data. Can be adjusted to specify subset of variables, dates, and depths to download.\n",
    "# Values generated here are those reported in the accompanying paper.\n",
    "xa_cmems_monthly, cmems_monthly_path = climate_data.download_reanalysis(download_dir=directories.get_monthly_cmems_dir(), final_filename = \"cmems_gopr_monthly\",\n",
    "    lat_lims = reef_areas.get_lat_lon_limits(area_name)[0], lon_lims = reef_areas.get_lat_lon_limits(area_name)[1], \n",
    "    product_id = \"cmems_mod_glo_phy_my_0.083_P1M-m\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file already exists at /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/daily_means/cmems_gopr_daily.nc\n"
     ]
    }
   ],
   "source": [
    "# download daily data\n",
    "xa_cmems_daily, cmems_daily_path = climate_data.download_reanalysis(download_dir=directories.get_daily_cmems_dir(), final_filename = \"cmems_gopr_daily.nc\",\n",
    "    lat_lims = reef_areas.get_lat_lon_limits(area_name)[0], lon_lims = reef_areas.get_lat_lon_limits(area_name)[1], \n",
    "    product_id = \"cmems_mod_glo_phy_my_0.083_P1D-m\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially pad the data\n",
    "\n",
    "TODO: add visual explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatially_buffer_timeseries(\n",
    "    xa_ds: xa.Dataset,\n",
    "    buffer_size: int = 3,\n",
    "    exclude_vars: list[str] = [\"spatial_ref\", \"coral_algae_1-12_degree\"],\n",
    ") -> xa.Dataset:\n",
    "    \"\"\"Applies a spatial buffer to each data variable in the xarray dataset.\n",
    "\n",
    "    Parameters\n",
    "        xa_ds (xarray.Dataset): Input xarray dataset.\n",
    "        buffer_size (int): Buffer size in grid cells.\n",
    "        exclude_vars (list[str]): List of variable names to exclude from buffering.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: Xarray dataset with buffered data variables.\n",
    "    \"\"\"\n",
    "    filtered_vars = [var for var in xa_ds.data_vars if var not in exclude_vars]\n",
    "\n",
    "    buffered_ds = xa.Dataset()\n",
    "    for data_var in tqdm(\n",
    "        filtered_vars, desc=f\"Buffering variables by {buffer_size} pixel(s)\"\n",
    "    ):\n",
    "        buffered = xa.apply_ufunc(\n",
    "            spatial_data.buffer_nans,\n",
    "            xa_ds[data_var],\n",
    "            input_core_dims=[[]],\n",
    "            output_core_dims=[[]],\n",
    "            kwargs={\"size\": buffer_size},\n",
    "            dask=\"parallelized\",\n",
    "        )\n",
    "        buffered_ds[data_var] = buffered\n",
    "\n",
    "    return buffered_ds\n",
    "\n",
    "\n",
    "def spatially_buffer_nc_file(nc_path: Path | str, buffer_size: int = 3):\n",
    "    # TODO: specify distance buffer\n",
    "    nc_path = Path(nc_path)\n",
    "    buffered_name = nc_path.stem + f\"_buffered_{buffer_size}_pixel\"\n",
    "    buffered_path = (nc_path.parent / buffered_name).with_suffix(\".nc\")\n",
    "\n",
    "    # if buffered file doesn't already exist\n",
    "    if not buffered_path.is_file():\n",
    "        nc_file = xa.open_dataset(nc_path)\n",
    "        buffered_ds = spatially_buffer_timeseries(\n",
    "            nc_file, buffer_size=buffer_size\n",
    "        )\n",
    "        buffered_ds.to_netcdf(buffered_path)\n",
    "    else:\n",
    "        buffered_ds = xa.open_dataset(buffered_path)\n",
    "        print(\n",
    "            f\"Area buffered by {buffer_size} pixel(s) already exists at {buffered_path}.\"\n",
    "        )\n",
    "\n",
    "    return buffered_ds, buffered_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Buffering variables by 5 pixel(s):   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# xa_cmems_monthly_buffered, _ = spatial_data.spatially_buffer_nc_file(cmems_monthly_path, buffer_size=5)\n",
    "xa_cmems_daily_buffered, _ = spatially_buffer_nc_file(cmems_daily_path, buffer_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_plots.plot_DEM(xa.open_dataset(directories.get_monthly_cmems_dir() / \"cmems_gopr_monthly_buffered_5_pixel\")[\"mlotst\"].isel(time=0), \"\")\n",
    "# spatial_plots.plot_DEM(buffered[\"mlotst\"].isel(time=0), \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets into single netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "gt_xa = xa.open_dataset(directories.get_reef_baseline_dir() / \"gt_tifs/coral_raster_1000m.nc\")\n",
    "# gradient\n",
    "# bath_xa = xa.open_dataset(directories.get_bathymetry_datasets_dir() / \"gt_tifs/coral_raster_1000m.nc\")\n",
    "xa_bath_upsampled\n",
    "# slopes\n",
    "# cmems global ocean reanalysis\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_man = data_structure.MyDatasets()\n",
    "ds_man.set_location(location)\n",
    "\n",
    "noaa_features = ['mlotst', 'bottomT', 'uo', 'so', 'zos', 'thetao', 'vo']\n",
    "\n",
    "# TODO: transparency in preprocessing to get to this (probably split into separate gt datarray)\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12\", xa.open_dataset(\n",
    "        ds_man.get_location() / \"global_ocean_reanalysis/monthly_means/coral_climate_1_12.nc\")\n",
    ")\n",
    "\n",
    "ds_man.add_datasets(\n",
    "    [\"monthly_climate_1_12_X\", \"monthly_climate_1_12_y\"], \n",
    "        spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"monthly_climate_1_12\"), \n",
    "        feature_vars=noaa_features, gt_var=\"coral_algae_1-12_degree\")\n",
    ")\n",
    "\n",
    "# TODO: handle depth\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12\", spatial_data.generate_and_add_gt_to_xa_d(xa.open_dataset(\n",
    "        Path(ds_man.get_location() / \"global_ocean_reanalysis/daily_means/dailies_combined.nc\")).isel(depth=0),\n",
    "        ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"])\n",
    ")\n",
    "\n",
    "# TODO: streamline checking and saving process\n",
    "daily_climate_1_12_X_file_path = ds_man.get_location() / \"global_ocean_reanalysis/daily_means/daily_climate_1_12_X.npy\"\n",
    "# if daily_climate_1_12_X numpy array doesn't exist, generate and save\n",
    "if not file_ops.check_file_exists(filepath = daily_climate_1_12_X_file_path):\n",
    "    daily_climate_1_12_X = spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"daily_climate_1_12\"),\n",
    "        feature_vars = noaa_features)\n",
    "    np.save(daily_climate_1_12_X_file_path, daily_climate_1_12_X) \n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "else:\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "\n",
    "daily_climate_1_12_padded_1_file_path = ds_man.get_location() / \"global_ocean_reanalysis/daily_means/daily_climate_1_12_padded_1.nc\"\n",
    "# if daily_climate_1_12_padded_1 .nc file doesn't exist, generate and save\n",
    "if not file_ops.check_file_exists(filepath = daily_climate_1_12_padded_1_file_path):\n",
    "    daily_climate_1_12_padded_1 = spatial_data.spatially_buffer_timeseries(\n",
    "        ds_man.get_dataset(\"daily_climate_1_12\"), buffer_size=1, exclude_vars = [\"spatial_ref\", \"coral_algae_gt\"])\n",
    "    daily_climate_1_12_padded_1.to_netcdf(filepath = daily_climate_1_12_padded_1_file_path)\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_padded_1\", xa.open_dataset(daily_climate_1_12_padded_1_file_path))\n",
    "else:\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_padded_1\", xa.open_dataset(daily_climate_1_12_padded_1_file_path))\n",
    "\n",
    "# add in ground truth to padded\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12_padded_1_gt\", spatial_data.generate_and_add_gt_to_xa_d(\n",
    "        ds_man.get_dataset(\"daily_climate_1_12_padded_1\"),\n",
    "        ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"])\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"bathymetry_A\", rio.open_rasterio(\n",
    "        rasterio.open(ds_man.get_location() / \"bathymetry/GBR_30m/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif\"),\n",
    "        ).rename(\"bathymetry_A\").rename({\"x\": \"longitude\", \"y\": \"latitude\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_man_ml = data_structure.MyDatasets()\n",
    "ds_man_ml.set_location(location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

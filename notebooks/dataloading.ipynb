{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose whether to work on a remote machine\n",
    "location = \"remote\"\n",
    "# location = \"local\"\n",
    "\n",
    "if location == \"remote\":\n",
    "    # change this line to the where the GitHub repository is located\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data storage setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/jovyan/lustre_scratch/conda-envs/coralshift/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "\n",
    "# from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray as rio\n",
    "\n",
    "\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories, utils\n",
    "from coralshift.dataloading import data_structure, climate_data, bathymetry, reef_extent\n",
    "from coralshift.plotting import spatial_plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_man = data_structure.MyDatasets()\n",
    "# ds_man.set_location(location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify your area of interest\n",
    "\n",
    "The availability of high-resolution (30m) bathymetric data means that areas of interest are currently confided to 4 areas on the Great Barrier Reef (GBR). The following code generates a geoJSON file specifying which area (A-D) you'd like to investigate:\n",
    "\n",
    "| Reef Area Name                \t| Latitudes \t| Longitudes \t|\n",
    "|-------------------------------\t|-----------\t|------------\t|\n",
    "| Great Barrier Reef A 2020 30m \t| 10-17°S   \t| 143-147°E  \t|\n",
    "| Great Barrier Reef B 2020 30m \t| 16-23°S   \t| 144-149°E  \t|\n",
    "| Great Barrier Reef C 2020 30m \t| 18-24°S   \t| 148-154°E  \t|\n",
    "| Great Barrier Reef D 2020 30m \t| 23-29°S   \t| 150-156°E  \t|\n",
    "\n",
    "\n",
    "Download your required area from here: https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/115066\n",
    "\n",
    "Due to the computational load required to run ML models on such a high resolution data, bathymetric data is currently upsampled to 4km grid cells and areas are limited to a quarter of the GBR's total area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be downsampled to 0.00923 degrees (~1000m).\n"
     ]
    }
   ],
   "source": [
    "# choose resolution (should be above 1000m for processing in decent time)\n",
    "target_resolution_m, target_resolution_d = spatial_data.choose_resolution(\n",
    "    resolution=1000, unit=\"m\")\n",
    "\n",
    "# convert distance to degrees:\n",
    "# _,_,av_degrees = spatial_data.distance_to_degrees(target_resolution)\n",
    "print(f\"Data will be downsampled to {target_resolution_d:.05f} degrees (~{target_resolution_m}m).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great_Barrier_Reef_A_2020_30m_MSL_cog already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n",
      "bathymetry_A_0-00030d already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n",
      "bathymetry_A_0-0092276d already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n"
     ]
    }
   ],
   "source": [
    "# select your area\n",
    "area_name = \"A\"\n",
    "reef_areas = bathymetry.ReefAreas()\n",
    "\n",
    "# TODO: plot with different areas to choose from\n",
    "\n",
    "_, xa_bath = bathymetry.generate_bathymetry_xa_da(\"A\")\n",
    "_, xa_bath_upsampled = spatial_data.upsample_and_save_xa_a(\n",
    "    directories.get_bathymetry_datasets_dir(), xa_d= xa_bath, name=\"bathymetry_A\", target_resolution_d=target_resolution_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_bath.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great_Barrier_Reef_A_2020_30m_MSL_cog already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n"
     ]
    }
   ],
   "source": [
    "def generate_bathymetry_xa_da(area_name: str):\n",
    "    \"\"\"\n",
    "    Generate bathymetry data for a specified area.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        area_name (str): The name of the area.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tuple[str, xa.DataArray]: A tuple containing the filepath and the processed xarray of the generated bathymetry\n",
    "        data.\n",
    "    \"\"\"\n",
    "    # download .tif if not downloaded aready\n",
    "    reef_areas = bathymetry.ensure_bathymetry_downloaded(area_name)\n",
    "    # cast tif to processed xarray with correct crs\n",
    "    xa_bath = spatial_data.tif_to_xarray(\n",
    "        directories.get_bathymetry_datasets_dir() / reef_areas.get_filename(area_name),\n",
    "        reef_areas.get_xarray_name(area_name),\n",
    "    )\n",
    "\n",
    "    resolution = np.mean(spatial_data.calculate_spatial_resolution(xa_bath))\n",
    "    dir = directories.get_bathymetry_datasets_dir()\n",
    "    bath_name = f\"{reef_areas.get_xarray_name(area_name)}_{resolution:.05f}d\"\n",
    "    filepath, xa_da = save_nc(\n",
    "        dir, bath_name, xa_bath, return_array=True\n",
    "    )\n",
    "\n",
    "    return filepath, xa_da\n",
    "\n",
    "\n",
    "def save_nc(\n",
    "    save_dir: Path | str,\n",
    "    filename: str,\n",
    "    xa_d: xa.DataArray | xa.Dataset,\n",
    "    return_array: bool = False,\n",
    ") -> xa.DataArray | xa.Dataset:\n",
    "    \"\"\"\n",
    "    Save the given xarray DataArray or Dataset to a NetCDF file iff no file with the same\n",
    "    name already exists in the directory.\n",
    "    # TODO: issues when suffix provided\n",
    "    Parameters\n",
    "    ----------\n",
    "        save_dir (Path or str): The directory path to save the NetCDF file.\n",
    "        filename (str): The name of the NetCDF file.\n",
    "        xa_d (xarray.DataArray or xarray.Dataset): The xarray DataArray or Dataset to be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        xarray.DataArray or xarray.Dataset: The input xarray object.\n",
    "    \"\"\"\n",
    "    filename = file_ops.remove_suffix(utils.replace_dot_with_dash(filename))\n",
    "    save_path = (Path(save_dir) / filename).with_suffix(\".nc\")\n",
    "    if save_path.is_file():\n",
    "        print(f\"Writing {filename} to file at {save_path}\")\n",
    "        test = process_xa_d(xa_d).to_netcdf(save_path)\n",
    "    else:\n",
    "        print(f\"{filename} already exists in {save_dir}\")\n",
    "\n",
    "    if return_array:\n",
    "        if isinstance(xa_d, xa.DataArray):\n",
    "            return save_path, xa.open_dataarray(save_path)\n",
    "        elif isinstance(xa_d, xa.Dataset):\n",
    "            return save_path, xa.open_dataset(save_path)\n",
    "    else:\n",
    "        return save_path\n",
    "\n",
    "\n",
    "def process_xa_d(\n",
    "    xa_d: xa.Dataset | xa.DataArray,\n",
    "    rename_mapping: dict = {\n",
    "        \"lat\": \"latitude\",\n",
    "        \"lon\": \"longitude\",\n",
    "        \"x\": \"longitude\",\n",
    "        \"y\": \"latitude\",\n",
    "    },\n",
    "    squeeze_coords: str | list[str] = None,\n",
    "    chunk_dict: dict = {\"latitude\": 100, \"longitude\": 100, \"time\": 100},\n",
    "    crs: str = \"EPSG:4326\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the input xarray Dataset or DataArray by standardizing coordinate names, squeezing dimensions,\n",
    "    chunking along specified dimensions, and sorting coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        xa_d (xa.Dataset or xa.DataArray): The xarray Dataset or DataArray to be processed.\n",
    "        rename_mapping (dict, optional): A dictionary specifying the mapping for coordinate renaming.\n",
    "            The keys are the existing coordinate names, and the values are the desired names.\n",
    "            Defaults to a mapping that standardizes common coordinate names.\n",
    "        squeeze_coords (str or list of str, optional): The coordinates to squeeze by removing size-1 dimensions.\n",
    "                                                      Defaults to ['band'].\n",
    "        chunk_dict (dict, optional): A dictionary specifying the chunk size for each dimension.\n",
    "                                     The keys are the dimension names, and the values are the desired chunk sizes.\n",
    "                                     Defaults to {'latitude': 100, 'longitude': 100, 'time': 100}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        xa.Dataset or xa.DataArray: The processed xarray Dataset or DataArray.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # standardise coordinate names\n",
    "    # temp_xa_d = xa_d.rename(\n",
    "    #     {\"lat\": \"latitude\", \"lon\": \"longitude\", \"x\": \"longitude\", \"y\": \"latitude\"}\n",
    "    # )\n",
    "\n",
    "    temp_xa_d = xa_d.copy()\n",
    "\n",
    "    for coord, new_coord in rename_mapping.items():\n",
    "        if new_coord not in temp_xa_d.coords and coord in temp_xa_d.coords:\n",
    "            temp_xa_d = temp_xa_d.rename({coord: new_coord})\n",
    "    # temp_xa_d = xa_d.rename(\n",
    "    #     {coord: rename_mapping.get(coord, coord) for coord in xa_d.coords}\n",
    "    # )\n",
    "    if \"band\" in temp_xa_d.dims:\n",
    "        temp_xa_d = temp_xa_d.squeeze(\"band\")\n",
    "    if squeeze_coords:\n",
    "        temp_xa_d = temp_xa_d.squeeze(squeeze_coords)\n",
    "\n",
    "    # add crs\n",
    "    temp_xa_d.rio.write_crs(crs, inplace=True)\n",
    "    chunked_xa_d = chunk_as_necessary(temp_xa_d, chunk_dict)\n",
    "    # sort coords by ascending values\n",
    "    return chunked_xa_d.sortby(list(temp_xa_d.dims))\n",
    "\n",
    "\n",
    "_, xa_bath = generate_bathymetry_xa_da(\"A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dasked arrays for plotting\n",
    "xa_bath_upsampled = xa_bath_upsampled.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa_bath.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset.plot cannot be called directly. Use an explicit plot method, e.g. ds.plot.scatter(...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# N.B. native resolution not plotted since so high (takes ~10 minutes)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spatial_plots\u001b[39m.\u001b[39;49mplot_spatial(xa_bath_upsampled, val_lims\u001b[39m=\u001b[39;49m(\u001b[39m-\u001b[39;49m\u001b[39m50\u001b[39;49m,\u001b[39m0\u001b[39;49m), name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdepth\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      3\u001b[0m     title\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBathymetry at \u001b[39;49m\u001b[39m{\u001b[39;49;00mtarget_resolution_m\u001b[39m}\u001b[39;49;00m\u001b[39mm resolution\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/lustre_scratch/orlando-code/coralshift/coralshift/plotting/spatial_plots.py:176\u001b[0m, in \u001b[0;36mplot_spatial\u001b[0;34m(xa_da, fax, title, name, figsize, val_lims, cmap_type, symmetric, edgecolor, orient_colorbar)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m symmetric:\n\u001b[1;32m    174\u001b[0m     vmin, vmax \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mvmax, vmax) \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(vmin) \u001b[39m>\u001b[39m \u001b[39mabs\u001b[39m(vmax) \u001b[39melse\u001b[39;00m (vmin, \u001b[39m-\u001b[39mvmin)\n\u001b[0;32m--> 176\u001b[0m im \u001b[39m=\u001b[39m xa_da\u001b[39m.\u001b[39;49mplot(ax\u001b[39m=\u001b[39;49max, cmap\u001b[39m=\u001b[39;49mcmap, vmin\u001b[39m=\u001b[39;49mvmin, vmax\u001b[39m=\u001b[39;49mvmax, add_colorbar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    177\u001b[0m \u001b[39m# nicely format spatial plot\u001b[39;00m\n\u001b[1;32m    178\u001b[0m format_spatial_plot(im, fig, ax, title, name, orient_colorbar, edgecolor)\n",
      "File \u001b[0;32m~/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/xarray/plot/accessor.py:942\u001b[0m, in \u001b[0;36mDatasetPlotAccessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[0;32m--> 942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    943\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataset.plot cannot be called directly. Use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man explicit plot method, e.g. ds.plot.scatter(...)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset.plot cannot be called directly. Use an explicit plot method, e.g. ds.plot.scatter(...)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N.B. native resolution not plotted since so high (takes ~10 minutes)\n",
    "spatial_plots.plot_spatial(xa_bath_upsampled, val_lims=(-50,0), name=\"depth\", \n",
    "    title=f\"Bathymetry at {target_resolution_m}m resolution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate absolute gradients from bathymetry and save to file\n",
    "grads, grads_path = bathymetry.generate_gradient_magnitude_nc(xa_bath_up_vals, sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dasked array for plotting\n",
    "grads = grads.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display different resolutions\n",
    "fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(16,9), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "ax1 = spatial_plots.plot_spatial(xa_bath_upsampled, \n",
    "    fax= (fig,ax_left), val_lims=(-50,0), name=\"depth\", title=f\"Bathymetry at {target_resolution_m}m resolution\")\n",
    "ax2 = spatial_plots.plot_spatial(grads, \n",
    "    fax=(fig, ax_right), val_lims=(0,10), name=\"gradient magnitude\", \n",
    "    title=f\"Absolute seafloor gradients at {target_resolution_m}m resolution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coral ground truth: Allen Coral Atlas\n",
    "\n",
    "\n",
    "There is currently no API for accessing data directly from your local machine. Please follow the instructions* below:\n",
    "1. Make an account on the [Allen Coral Atlas](https://allencoralatlas.org/atlas/#6.00/-13.5257/144.5000) webpage\n",
    "2. Generate a geojson file using the code cell below (generated in the `reef_baseline` directory)\n",
    "\n",
    "*Instructions correct as of 30.06.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate geojson file in reef_baseline directory for download from the Allen Coral Atlas\n",
    "geojson_path = reef_extent.generate_area_geojson(\n",
    "    area_class = reef_areas, area_name=area_name, save_dir=directories.get_reef_baseline_dir())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Upload the geojson file via:\n",
    "\n",
    "    \\> My Areas > Upload a GeoJSON or KML file\n",
    "4. Wait for the area to be processed, and select \"Benthic Map (OGC GeoPackage (.gpkg))\". Sign the terms and conditions \n",
    "and select \"Prepare Download\". After ~two minutes a sequence of emails will arrive notifying you that your download is ready.\n",
    "5. Download the file and unzip it using a unzipping utility. Then, add the `benthic.gpkg` file to the `reef_baseline` directory.\n",
    "6. Continue with the subsequent code cells.\n",
    "\n",
    "----\n",
    "\n",
    "You have now downloaded:\n",
    "\n",
    "**`benthic.gpkg`**\n",
    "\n",
    "This is a dataframe of Shapely objects (\"geometry\" polygons) defining the boundaries of different benthic classes:\n",
    "\n",
    "| Class           \t| Number of polygons \t|\n",
    "|-----------------\t|--------------------\t|\n",
    "| Coral/Algae     \t| 877787             \t|\n",
    "| Rock            \t| 766391             \t|\n",
    "| Rubble          \t| 568041             \t|\n",
    "| Sand            \t| 518805             \t|\n",
    "| Microalgal Mats \t| 27569              \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .gpkg file and save to .pkl format for faster reading later\n",
    "benthic_df = file_ops.check_pkl_else_read_gpkg(directories.get_reef_baseline_dir(), filename = \"benthic.pkl\")\n",
    "benthic_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterize polygons\n",
    "\n",
    "Rasterized arrays are necessary to process the geospatial data e.g. to align different gridcells. Doing this locally through rasterio requires such significant compute that cloud computing is the only reasonable option. A JavaScript file (`rasterization.js`) for use in Google Earth Engine (GEE) is provided in the `coralshift` repo. Visit [this page](https://developers.google.com/earth-engine/guides/getstarted) for information regarding setting up a GEE account and getting started.\n",
    "\n",
    "GEE requires shapefile (.shp) format to ingest data. This is generated in the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process df to gpd.GeoDataFrame. \n",
    "# We are interested only in the \"Coral/Algae\" class, so gdf is limited to these rows by default\n",
    "gdf_coral = reef_extent.process_benthic_pd(benthic_df)\n",
    "# save as shapely file (if not already present) for rasterisation in GEE\n",
    "reef_extent.generate_coral_shp(gdf_coral)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ingest the shapefile (and all accompanying files: .cpg, .dbf, .prj, .shx) as a GEE asset.\n",
    "2. Import the subsequent `Table` into the script.\n",
    "3. Update the `resolution` variable as desired (usually that matching the target resolution specified above).\n",
    "3. Run the script, and submit the `coral_raster_Xm` task. Sit back and wait! After ~1 hour the rasters will be available to download from your Google Drive as GeoTIFFS: after this, add them to the `reef_baseline` directory and carry on with the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "gt_tif_files = file_ops.return_list_filepaths(directories.get_reef_baseline_dir() / \"gt_tifs\", \".tif\")\n",
    "# generate dictionary of file names and arrays: {filename: xarray.DataArray, ...}\n",
    "gt_tif_dict_preprocess = spatial_data.tifs_to_xa_array_dict(gt_tif_files)\n",
    "# process xa_arrays\n",
    "# gbr_30_dict = spatial_data.process_xa_arrays_in_dict(gbr_30_dict_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: processsing, saving to nc, visualisation\n",
    "\n",
    "gt_tif_dict_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_eg = rio.open_rasterio(rasterio.open(directories.get_reef_baseline_dir() / \"gt_tifs/coral_raster_1000m.tif\")).squeeze(\"band\")\n",
    "gt_4km = xa.open_dataarray(directories.get_reef_baseline_dir() / \"gt_tifs/gt_nc_dir/concatenated_0-03691_degree.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2, figsize=[14,8])\n",
    "tif_eg.sel({\"x\":slice(142,142.4), \"y\":slice(-10.6,-11)}).plot(ax=ax[0])\n",
    "xa_4km[\"__xarray_dataarray_variable__\"].sel({\"longitude\":slice(142,142.4), \"latitude\":slice(-11,-10.6)}).plot(ax=ax[1])\n",
    "ax[0].set_aspect(\"equal\")\n",
    "ax[1].set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_coral_gt_tifs(tif_dir_name=None, target_resolution_d:float=None):\n",
    "#     if not tif_dir_name:\n",
    "#         tif_dir = directories.get_reef_baseline_dir()\n",
    "#     else:\n",
    "#         tif_dir = directories.get_reef_baseline_dir() / tif_dir_name\n",
    "    \n",
    "#     nc_dir = file_ops.guarantee_existence(tif_dir / \"gt_nc_dir\")\n",
    "#     # save tifs to ncs in new dir\n",
    "#     tif_paths = tifs_to_ncs(nc_dir, target_resolution_d)\n",
    "#     # get list of nc paths in dir\n",
    "#     xa_arrays_list = [tif_to_xa_array(tif_path) for tif_path in tif_paths]\n",
    "#     # merge ncs into one mega nc file\n",
    "#     if len(xa_arrays_list) > 1:\n",
    "#         concatted = xa.concat(xa_arrays_list, dim=[\"latitude\",\"longitude\"])\n",
    "#     else:\n",
    "#         concatted = xa_arrays_list[0]\n",
    "#     file_ops.save_nc(nc_dir, f\"concatenated_{target_resolution_d:.05f}_degree\", concatted)\n",
    "\n",
    "\n",
    "# def tifs_to_ncs(nc_dir: Path | list[str], target_resolution_d: float=None) -> None:\n",
    "\n",
    "#     tif_dir = nc_dir.parent\n",
    "#     tif_paths = file_ops.return_list_filepaths(tif_dir, \".tif\")\n",
    "#     xa_array_dict = {}\n",
    "#     for tif_path in tqdm(tif_paths, total=len(tif_paths), desc=\"Writing tifs to nc files\"):\n",
    "#         # filename = str(file_ops.get_n_last_subparts_path(tif, 1))\n",
    "#         filename = tif_path.stem\n",
    "#         tif_array = tif_to_xa_array(tif_path)\n",
    "#         # xa_array_dict[filename] = tif_array.rename(filename)\n",
    "#         if target_resolution_d:\n",
    "#             tif_array = spatial_data.upsample_xarray_to_target(xa_array=tif_array, target_resolution=target_resolution_d)\n",
    "#         # save array to nc file\n",
    "#         file_ops.save_nc(tif_dir, filename, tif_array)\n",
    "\n",
    "#     return tif_paths\n",
    "#     print(f\"All tifs converted to xarrays and stored as .nc files in {nc_dir}.\")\n",
    "\n",
    "\n",
    "# def tif_to_xa_array(tif_path) -> xa.DataArray:\n",
    "#     return spatial_data.process_xa_d(rio.open_rasterio(rasterio.open(tif_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef_extent.process_coral_gt_tifs(tif_dir_name=\"gt_tifs\", target_resolution_d=target_resolution_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Ocean Physics Reanalysis\n",
    "\n",
    "The dataset metadata can be accessed [here](https://doi.org/10.48670/moi-00021)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "You're required to set up an account with the [Copernicus Marine Service](https://marine.copernicus.eu/). \n",
    "\n",
    "\n",
    "**Warning:**  this is a large amount of data for which the only way to gather it is to query the copernicus API via motu. Requests are queued, and request sizes are floated to the top of the queue. The following functions take advantage of this by splitting a single request up by date adn variable before amalgamating the files, but this can still take a **very long time**, and vary significantly depending on overall website traffic. For those who aren't interested in the entire database, it's highly recommended that you use the toy dataset provided as a `.npy` file in the GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download monthly data. Can be adjusted to specify subset of variables, dates, and depths to download.\n",
    "# Values generated here are those reported in the accompanying paper.\n",
    "xa_cmems_monthly, cmems_monthly_path = climate_data.download_reanalysis(download_dir=directories.get_monthly_cmems_dir(), \n",
    "    final_filename = \"cmems_gopr_monthly\",\n",
    "    lat_lims = reef_areas.get_lat_lon_limits(area_name)[0], lon_lims = reef_areas.get_lat_lon_limits(area_name)[1], \n",
    "    product_id = \"cmems_mod_glo_phy_my_0.083_P1M-m\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download daily data\n",
    "xa_cmems_daily, cmems_daily_path = climate_data.download_reanalysis(download_dir=directories.get_daily_cmems_dir(), \n",
    "    final_filename = \"cmems_gopr_daily.nc\",\n",
    "    lat_lims = reef_areas.get_lat_lon_limits(area_name)[0], lon_lims = reef_areas.get_lat_lon_limits(area_name)[1], \n",
    "    product_id = \"cmems_mod_glo_phy_my_0.083_P1D-m\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially pad the data\n",
    "\n",
    "TODO: add visual explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatially_buffer_timeseries(\n",
    "    xa_ds: xa.Dataset,\n",
    "    buffer_size: int = 3,\n",
    "    exclude_vars: list[str] = [\"spatial_ref\", \"coral_algae_1-12_degree\"],\n",
    ") -> xa.Dataset:\n",
    "    \"\"\"Applies a spatial buffer to each data variable in the xarray dataset.\n",
    "\n",
    "    Parameters\n",
    "        xa_ds (xarray.Dataset): Input xarray dataset.\n",
    "        buffer_size (int): Buffer size in grid cells.\n",
    "        exclude_vars (list[str]): List of variable names to exclude from buffering.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: Xarray dataset with buffered data variables.\n",
    "    \"\"\"\n",
    "    filtered_vars = [var for var in xa_ds.data_vars if var not in exclude_vars]\n",
    "\n",
    "    buffered_ds = xa.Dataset()\n",
    "    for data_var in tqdm(\n",
    "        filtered_vars, desc=f\"Buffering variables by {buffer_size} pixel(s)\"\n",
    "    ):\n",
    "        buffered = xa.apply_ufunc(\n",
    "            spatial_data.buffer_nans,\n",
    "            xa_ds[data_var],\n",
    "            input_core_dims=[[]],\n",
    "            output_core_dims=[[]],\n",
    "            kwargs={\"size\": buffer_size},\n",
    "            dask=\"parallelized\",\n",
    "        )\n",
    "        buffered_ds[data_var] = buffered\n",
    "\n",
    "    return buffered_ds\n",
    "\n",
    "\n",
    "def spatially_buffer_nc_file(nc_path: Path | str, buffer_size: int = 3):\n",
    "    # TODO: specify distance buffer\n",
    "    nc_path = Path(nc_path)\n",
    "    buffered_name = nc_path.stem + f\"_buffered_{buffer_size}_pixel\"\n",
    "    buffered_path = (nc_path.parent / buffered_name).with_suffix(\".nc\")\n",
    "\n",
    "    # if buffered file doesn't already exist\n",
    "    if not buffered_path.is_file():\n",
    "        nc_file = xa.open_dataset(nc_path)\n",
    "        buffered_ds = spatially_buffer_timeseries(\n",
    "            nc_file, buffer_size=buffer_size\n",
    "        )\n",
    "        buffered_ds.to_netcdf(buffered_path)\n",
    "    else:\n",
    "        buffered_ds = xa.open_dataset(buffered_path)\n",
    "        print(\n",
    "            f\"Area buffered by {buffer_size} pixel(s) already exists at {buffered_path}.\"\n",
    "        )\n",
    "\n",
    "    return buffered_ds, buffered_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xa_cmems_monthly_buffered, _ = spatial_data.spatially_buffer_nc_file(cmems_monthly_path, buffer_size=5)\n",
    "xa_cmems_daily_buffered, _ = spatially_buffer_nc_file(cmems_daily_path, buffer_size=5)\n",
    "# TODO: this taking forever (seemingly getting stuck after second variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_plots.plot_DEM(xa.open_dataset(directories.get_monthly_cmems_dir() / \"cmems_gopr_monthly_buffered_5_pixel\")[\"mlotst\"].isel(time=0), \"\")\n",
    "# spatial_plots.plot_DEM(buffered[\"mlotst\"].isel(time=0), \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_data.generate_era5_data(\n",
    "    lat_lims=reef_areas.get_lat_lon_limits(area_name)[0], lon_lims=reef_areas.get_lat_lon_limits(area_name)[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets into single netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "gt_xa = xa.open_dataset(directories.get_reef_baseline_dir() / \"gt_tifs/coral_raster_1000m.nc\")\n",
    "# gradient\n",
    "# bath_xa = xa.open_dataset(directories.get_bathymetry_datasets_dir() / \"gt_tifs/coral_raster_1000m.nc\")\n",
    "xa_bath_upsampled\n",
    "# slopes\n",
    "# cmems global ocean reanalysis\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_man = data_structure.MyDatasets()\n",
    "ds_man.set_location(location)\n",
    "\n",
    "noaa_features = ['mlotst', 'bottomT', 'uo', 'so', 'zos', 'thetao', 'vo']\n",
    "\n",
    "# TODO: transparency in preprocessing to get to this (probably split into separate gt datarray)\n",
    "ds_man.add_dataset(\n",
    "    \"monthly_climate_1_12\", xa.open_dataset(\n",
    "        ds_man.get_location() / \"global_ocean_reanalysis/monthly_means/coral_climate_1_12.nc\")\n",
    ")\n",
    "\n",
    "ds_man.add_datasets(\n",
    "    [\"monthly_climate_1_12_X\", \"monthly_climate_1_12_y\"], \n",
    "        spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"monthly_climate_1_12\"), \n",
    "        feature_vars=noaa_features, gt_var=\"coral_algae_1-12_degree\")\n",
    ")\n",
    "\n",
    "# TODO: handle depth\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12\", spatial_data.generate_and_add_gt_to_xa_d(xa.open_dataset(\n",
    "        Path(ds_man.get_location() / \"global_ocean_reanalysis/daily_means/dailies_combined.nc\")).isel(depth=0),\n",
    "        ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"])\n",
    ")\n",
    "\n",
    "# TODO: streamline checking and saving process\n",
    "daily_climate_1_12_X_file_path = ds_man.get_location() / \"global_ocean_reanalysis/daily_means/daily_climate_1_12_X.npy\"\n",
    "# if daily_climate_1_12_X numpy array doesn't exist, generate and save\n",
    "if not file_ops.check_file_exists(filepath = daily_climate_1_12_X_file_path):\n",
    "    daily_climate_1_12_X = spatial_data.process_xa_ds_for_ml(ds_man.get_dataset(\"daily_climate_1_12\"),\n",
    "        feature_vars = noaa_features)\n",
    "    np.save(daily_climate_1_12_X_file_path, daily_climate_1_12_X) \n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "else:\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_X\", np.load(daily_climate_1_12_X_file_path))\n",
    "\n",
    "daily_climate_1_12_padded_1_file_path = ds_man.get_location() / \"global_ocean_reanalysis/daily_means/daily_climate_1_12_padded_1.nc\"\n",
    "# if daily_climate_1_12_padded_1 .nc file doesn't exist, generate and save\n",
    "if not file_ops.check_file_exists(filepath = daily_climate_1_12_padded_1_file_path):\n",
    "    daily_climate_1_12_padded_1 = spatial_data.spatially_buffer_timeseries(\n",
    "        ds_man.get_dataset(\"daily_climate_1_12\"), buffer_size=1, exclude_vars = [\"spatial_ref\", \"coral_algae_gt\"])\n",
    "    daily_climate_1_12_padded_1.to_netcdf(filepath = daily_climate_1_12_padded_1_file_path)\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_padded_1\", xa.open_dataset(daily_climate_1_12_padded_1_file_path))\n",
    "else:\n",
    "    ds_man.add_dataset(\"daily_climate_1_12_padded_1\", xa.open_dataset(daily_climate_1_12_padded_1_file_path))\n",
    "\n",
    "# add in ground truth to padded\n",
    "ds_man.add_dataset(\n",
    "    \"daily_climate_1_12_padded_1_gt\", spatial_data.generate_and_add_gt_to_xa_d(\n",
    "        ds_man.get_dataset(\"daily_climate_1_12_padded_1\"),\n",
    "        ds_man.get_dataset(\"monthly_climate_1_12\")[\"coral_algae_1-12_degree\"])\n",
    ")\n",
    "\n",
    "ds_man.add_dataset(\n",
    "    \"bathymetry_A\", rio.open_rasterio(\n",
    "        rasterio.open(ds_man.get_location() / \"bathymetry/GBR_30m/Great_Barrier_Reef_A_2020_30m_MSL_cog.tif\"),\n",
    "        ).rename(\"bathymetry_A\").rename({\"x\": \"longitude\", \"y\": \"latitude\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_man_ml = data_structure.MyDatasets()\n",
    "ds_man_ml.set_location(location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

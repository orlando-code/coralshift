{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose whether to work on a remote machine\n",
    "location = \"remote\"\n",
    "# location = \"local\"\n",
    "\n",
    "if location == \"remote\":\n",
    "    # change this line to the where the GitHub repository is located\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data storage setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/jovyan/lustre_scratch/conda-envs/coralshift/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# # import necessary packages\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from rasterio.plot import show\n",
    "import rioxarray as rio\n",
    "import rasterio\n",
    "\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories, utils\n",
    "from coralshift.dataloading import data_structure, climate_data, bathymetry, reef_extent\n",
    "from coralshift.utils import directories\n",
    "# from coralshift.plotting import spatial_plots\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify your area of interest\n",
    "\n",
    "The availability of high-resolution (30m) bathymetric data means that areas of interest are currently confided to 4 areas on the Great Barrier Reef (GBR). The following code downloads the specified area of bathymetry data:\n",
    "\n",
    "| Reef Area Name                \t| Latitudes \t| Longitudes \t|\n",
    "|-------------------------------\t|-----------\t|------------\t|\n",
    "| Great Barrier Reef A 2020 30m \t| 10-17°S   \t| 142-147°E  \t|\n",
    "| Great Barrier Reef B 2020 30m \t| 16-23°S   \t| 144-149°E  \t|\n",
    "| Great Barrier Reef C 2020 30m \t| 18-24°S   \t| 148-154°E  \t|\n",
    "| Great Barrier Reef D 2020 30m \t| 23-29°S   \t| 150-156°E  \t|\n",
    "\n",
    "\n",
    "![bathymetry_regions.png](https://github.com/orlando-code/coralshift/blob/dev-setup/bathymetry_regions.png?raw=true)\n",
    "\n",
    "\n",
    "Due to the computational load required to run ML models on such high resolution data, bathymetric data is currently upsampled to 4km grid cells and areas are limited to a quarter of the GBR's total area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be resampled to 0.03691 degrees (~4000m).\n"
     ]
    }
   ],
   "source": [
    "# choose resolution (should be above 1000m for processing in decent time)\n",
    "# native resolutions are 1 (1, \"m\") or 1/12 degrees (1/12, \"d\"), or 4000m (4000, \"m\") or 1000m (1000, \"m\")\n",
    "target_resolution_m, target_resolution_d = spatial_data.choose_resolution(\n",
    "    resolution=4000, unit=\"m\")\n",
    "\n",
    "print(f\"Data will be resampled to {target_resolution_d:.05f} degrees (~{target_resolution_m}m).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great_Barrier_Reef_C_2020_30m_MSL_cog already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n",
      "bathymetry_C_0-00030d already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great_Barrier_Reef_C_0-036912d already exists in /lustre_scratch/orlando-code/datasets/bathymetry\n"
     ]
    }
   ],
   "source": [
    "# select your area. Region \"A\" is used in the report\n",
    "area_name = \"C\"\n",
    "\n",
    "reef_areas = bathymetry.ReefAreas()\n",
    "file_name = reef_areas.get_short_filename(area_name)\n",
    "\n",
    "\n",
    "bath_dir = directories.get_bathymetry_datasets_dir()\n",
    "_, xa_bath = bathymetry.generate_bathymetry_xa_da(area_name)\n",
    "_, xa_bath_upsampled = spatial_data.upsample_and_save_xa_a(\n",
    "    bath_dir, xa_d=xa_bath, name=file_name, target_resolution_d=target_resolution_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute dasked arrays for plotting\n",
    "# xa_bath_upsampled = xa_bath_upsampled.compute()\n",
    "\n",
    "# # N.B. native resolution not plotted since so high (takes ~10 minutes)\n",
    "# spatial_plots.plot_spatial(xa_bath_upsampled, val_lims=(-50,0), name=\"depth\", \n",
    "#     title=f\"Bathymetry at {target_resolution_m}m resolution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate absolute gradients from bathymetry and save to file\n",
    "grads, grads_path = bathymetry.generate_gradient_magnitude_nc(extract_variable(xa_bath_upsampled), sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display different resolutions\n",
    "fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(16,9), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "ax1 = spatial_plots.plot_spatial(xa_bath_upsampled, \n",
    "    fax= (fig,ax_left), val_lims=(-50,0), name=\"depth\", title=f\"Bathymetry at {target_resolution_m}m resolution\")\n",
    "ax2 = spatial_plots.plot_spatial(grads, \n",
    "    fax=(fig, ax_right), val_lims=(0,10), name=\"gradient magnitude\", \n",
    "    title=f\"Absolute seafloor gradients at {target_resolution_m}m resolution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coral ground truth: Allen Coral Atlas\n",
    "\n",
    "\n",
    "There is currently no API for accessing data directly from your local machine. Please follow the instructions* below:\n",
    "1. Make an account on the [Allen Coral Atlas](https://allencoralatlas.org/atlas/#6.00/-13.5257/144.5000) webpage\n",
    "2. Generate a geojson file using the code cell below (generated in the `reef_baseline` directory)\n",
    "\n",
    "*Instructions correct as of 30.06.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate geojson file in reef_baseline directory for download from the Allen Coral Atlas\n",
    "geojson_path = reef_extent.generate_area_geojson(\n",
    "    area_class = reef_areas, area_name=file_name, save_dir=directories.get_reef_baseline_dir())\n",
    "\n",
    "print(f\"geoJSON file saved at {geojson_path} for upload to GEE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Upload the geojson file via:\n",
    "\n",
    "    \\> My Areas > Upload a GeoJSON or KML file\n",
    "4. Specify a region name and navigate to the \"Download data\" tab when it becomes available.\n",
    "4. Select \"Benthic Map (OGC GeoPackage (.gpkg))\". Sign the terms and conditions \n",
    "and select \"Prepare Download\". After ~two minutes a sequence of emails will arrive notifying you that your download is ready.\n",
    "5. Download the file and unzip it using a unzipping utility. Then, add the `benthic.gpkg` file to the `reef_baseline` directory.\n",
    "6. Continue with the subsequent code cells.\n",
    "\n",
    "----\n",
    "\n",
    "You have now downloaded:\n",
    "\n",
    "**`benthic.gpkg`**\n",
    "\n",
    "This is a dataframe of Shapely objects (\"geometry\" polygons) defining the boundaries of different benthic classes:\n",
    "\n",
    "| Class           \t| Number of polygons \t|\n",
    "|-----------------\t|--------------------\t|\n",
    "| Coral/Algae     \t| 877787             \t|\n",
    "| Rock            \t| 766391             \t|\n",
    "| Rubble          \t| 568041             \t|\n",
    "| Sand            \t| 518805             \t|\n",
    "| Microalgal Mats \t| 27569              \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .gpkg file and save to .pkl format for faster reading later\n",
    "benthic_df = file_ops.check_pkl_else_read_gpkg(directories.get_reef_baseline_dir(), filename = \"benthic.pkl\")\n",
    "benthic_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterize polygons\n",
    "\n",
    "Rasterized arrays are necessary to process the geospatial data e.g. to align different gridcells. Doing this locally through rasterio requires such significant compute that cloud computing is the only reasonable option. A JavaScript file (`rasterization.js`) for use in Google Earth Engine (GEE) is provided in the `coralshift` repo. Visit [this page](https://developers.google.com/earth-engine/guides/getstarted) for information regarding setting up a GEE account and getting started.\n",
    "\n",
    "GEE requires shapefile (.shp) format to ingest data. This is generated in the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process df to gpd.GeoDataFrame. \n",
    "# We are interested only in the \"Coral/Algae\" class, so gdf is limited to these rows by default\n",
    "gdf_coral = reef_extent.process_benthic_pd(benthic_df)\n",
    "# save as shapely file (if not already present) for rasterisation in GEE\n",
    "reef_extent.generate_coral_shp(gdf_coral)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ingest the shapefile (and all accompanying files: .cpg, .dbf, .prj, .shx) as a GEE asset.\n",
    "2. Import the subsequent `Table` into the script.\n",
    "3. Update the `resolution` variable as desired (usually that matching the target resolution specified above).\n",
    "3. Run the script, and submit the `coral_raster_Xm` task. Sit back and wait! After ~1 hour (depending on the chosen resolution) the rasters will be available to download from your Google Drive as GeoTIFFS: after this, add them to the `reef_baseline` directory and carry on with the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all tifs in folder, casting to nc files for future use\n",
    "reef_extent.process_reef_extent_tifs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: automated visualisation\n",
    "\n",
    "gt_tif_dict['coral_raster_1000m'].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Ocean Physics Reanalysis\n",
    "\n",
    "The dataset metadata can be accessed [here](https://doi.org/10.48670/moi-00021)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "You're required to set up an account with the [Copernicus Marine Service](https://marine.copernicus.eu/). \n",
    "\n",
    "\n",
    "**Warning:**  this is a large amount of data for which the only way to gather it is to query the copernicus API via motu. Requests are queued, and request sizes are floated to the top of the queue. The following functions take advantage of this by splitting a single request up by date adn variable before amalgamating the files, but this can still take a **very long time**, and vary significantly depending on overall website traffic. For those who aren't interested in the entire database, it's highly recommended that you use the toy dataset provided as a `.npy` file in the GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xa\n",
    "import os\n",
    "import cdsapi\n",
    "\n",
    "# import getpass\n",
    "\n",
    "# import cdsapi\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pandas._libs.tslibs.timestamps import Timestamp\n",
    "\n",
    "from coralshift.utils import utils, file_ops, directories\n",
    "from coralshift.processing import spatial_data\n",
    "\n",
    "def generate_spatiotemporal_var_filename_from_dict(\n",
    "    info_dict: dict,\n",
    ") -> str:\n",
    "    \"\"\"Generate a filename based on variable, date, and coordinate limits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info_dict (dict): A dictionary containing information about the variable, date, and coordinate limits.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str: The generated filename.\n",
    "    \"\"\"\n",
    "    filename_list = []\n",
    "    for k, v in info_dict.items():\n",
    "        # strings (variables)\n",
    "        if utils.is_type_or_list_of_type(v, str):\n",
    "            filename_list.extend(\n",
    "                [\n",
    "                    k.upper(),\n",
    "                    utils.replace_dot_with_dash(utils.underscore_str_of_strings(v)),\n",
    "                ]\n",
    "            )\n",
    "        # np.datetime64 (dates)\n",
    "        elif utils.is_type_or_list_of_type(\n",
    "            v, np.datetime64\n",
    "        ) or utils.is_type_or_list_of_type(v, Timestamp):\n",
    "            filename_list.extend(\n",
    "                [\n",
    "                    k.upper(),\n",
    "                    utils.replace_dot_with_dash(utils.underscore_str_of_dates(v)),\n",
    "                ]\n",
    "            )\n",
    "        # tuples (coordinates limits)\n",
    "        elif utils.is_type_or_list_of_type(v, tuple):\n",
    "            filename_list.extend(\n",
    "                [\n",
    "                    k.upper(),\n",
    "                    utils.replace_dot_with_dash(\n",
    "                        utils.underscore_list_of_tuples(utils.round_list_tuples(v))\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    return \"_\".join(filename_list)\n",
    "\n",
    "\n",
    "def generate_metadata(\n",
    "    download_dir: str,\n",
    "    filename: str,\n",
    "    variables: list[str],\n",
    "    date_lims: tuple[str, str],\n",
    "    lon_lims: list[float],\n",
    "    lat_lims: list[float],\n",
    "    depth_lims: list[float],\n",
    "    query: str = \"n/a\",\n",
    ") -> None:\n",
    "    \"\"\"Generate metadata for the downloaded file and save it as a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    download_dir (str): The directory where the file is downloaded.\n",
    "    filename (str): The name of the file.\n",
    "    variable (str): The variable acronym.\n",
    "    date_lims (tuple[str, str]): A tuple containing the start and end dates.\n",
    "    lon_lims (list[float]): A list containing the longitude limits.\n",
    "    lat_lims (list[float]): A list containing the latitude limits.\n",
    "    depth_lims (list[float]): A list containing the depth limits.\n",
    "    query (str): The MOTU query used for downloading the file.\n",
    "    \"\"\"\n",
    "    filepath = (Path(download_dir) / filename).with_suffix(\".json\")\n",
    "\n",
    "    var_dict = {\n",
    "        \"mlotst\": \"ocean mixed layer thickness (sigma theta)\",\n",
    "        \"siconc\": \"sea ice area fraction\",\n",
    "        \"thetao\": \"sea water potential temperature\",\n",
    "        \"usi\": \"eastward sea ice velocity\",\n",
    "        \"sithick\": \"sea ice thickness\",\n",
    "        \"bottomT\": \"sea water potential temperature at sea floor\",\n",
    "        \"vsi\": \"northward sea ice velocity\",\n",
    "        \"usi\": \"eastward sea ice velocity\",\n",
    "        \"vo\": \"northward sea water velocity\",\n",
    "        \"uo\": \"eastward sea water velocity\",\n",
    "        \"so\": \"sea water salinity\",\n",
    "        \"zos\": \"sea surface height above geoid\",\n",
    "    }\n",
    "\n",
    "    # if list of variables, iterate through dict to get long names\n",
    "    if len(variables) > 1:\n",
    "        variable_names = str([var_dict[var] for var in variables])\n",
    "    else:\n",
    "        variable_names = var_dict[variables[0]]\n",
    "    # send list to a string for json\n",
    "    variables = str(variables)\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": filename,\n",
    "        \"download directory\": str(download_dir),\n",
    "        \"variable acronym(s)\": variables,\n",
    "        \"variable name(s)\": variable_names,\n",
    "        \"longitude-min\": lon_lims[0],\n",
    "        \"longitude-max\": lon_lims[1],\n",
    "        \"latitude-min\": lat_lims[0],\n",
    "        \"latitude-max\": lat_lims[1],\n",
    "        \"date-min\": str(date_lims[0]),\n",
    "        \"date-max\": str(date_lims[1]),\n",
    "        \"depth-min\": depth_lims[0],\n",
    "        \"depth-max\": depth_lims[1],\n",
    "        \"motu_query\": query,\n",
    "    }\n",
    "    # save metadata as json file at filepath\n",
    "    file_ops.save_json(metadata, filepath)\n",
    "\n",
    "\n",
    "def generate_name_dict(\n",
    "    variables: list[str],\n",
    "    date_lims: tuple[str, str],\n",
    "    lon_lims: tuple[str, str],\n",
    "    lat_lims: tuple[str, str],\n",
    "    depth_lims: tuple[str, str],\n",
    ") -> dict:\n",
    "    return {\n",
    "        \"vars\": variables,\n",
    "        \"dates\": date_lims,\n",
    "        \"lons\": lon_lims,\n",
    "        \"lats\": lat_lims,\n",
    "        \"depths\": depth_lims,\n",
    "    }\n",
    "\n",
    "\n",
    "def download_reanalysis(\n",
    "    download_dir: str | Path,\n",
    "    region: str,\n",
    "    final_filename: str = None,\n",
    "    variables: list[str] = [\"mlotst\", \"bottomT\", \"uo\", \"so\", \"zos\", \"thetao\", \"vo\"],\n",
    "    date_lims: tuple[str, str] = (\"1992-12-31\", \"2021-12-31\"),\n",
    "    depth_lims: tuple[str, str] = (0.3, 1),\n",
    "    lon_lims: tuple[str, str] = (142, 147),\n",
    "    lat_lims: tuple[str, str] = (-17, -10),\n",
    "    product_type: str = \"my\",\n",
    "    service_id: str = \"GLOBAL_MULTIYEAR_PHY_001_030\",\n",
    "    product_id: str = \"cmems_mod_glo_phy_my_0.083_P1D-m\",\n",
    ") -> xa.Dataset:\n",
    "    \"\"\"\n",
    "    Download reanalysis data for multiple variables and save them to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    download_dir (str | Path): Directory to save the downloaded files.\n",
    "    variables (list[str]): List of variables to download.\n",
    "    date_lims (tuple[str, str]): Date limits as a tuple of strings in the format \"YYYY-MM-DD\".\n",
    "    lon_lims (tuple[str, str]): Longitude limits as a tuple of strings in the format \"lon_min, lon_max\".\n",
    "    lat_lims (tuple[str, str]): Latitude limits as a tuple of strings in the format \"lat_min, lat_max\".\n",
    "    depth_lims (tuple[str, str]): Depth limits as a tuple of strings in the format \"depth_min, depth_max\".\n",
    "    product_type (str, optional): Product type. Defaults to \"my\".\n",
    "    service_id (str, optional): Product ID. Defaults to \"GLOBAL_MULTIYEAR_PHY_001_030\".\n",
    "    product_id (str, optional): Dataset ID. Defaults to \"cmems_mod_glo_phy_my_0.083_P1D-m\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xa.Dataset: dataset merged from individual files\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Currently taking only topmost depth (TODO: make full use of profile)\n",
    "\n",
    "    \"\"\"\n",
    "    download_dir = file_ops.guarantee_existence(Path(download_dir) / region)\n",
    "    merged_download_dir = download_dir / \"merged_vars\"\n",
    "\n",
    "    # User credentials\n",
    "    # username = input(\"Enter your username: \")\n",
    "    # password = getpass.getpass(\"Enter your password: \")\n",
    "    username = \"otimmerman\"\n",
    "    password = \"Fgg0N$tUUuL3\"\n",
    "\n",
    "    # generate name of combined file\n",
    "    name_dict = generate_name_dict(variables, date_lims, lon_lims, lat_lims, depth_lims)\n",
    "    main_filename = generate_spatiotemporal_var_filename_from_dict(name_dict)\n",
    "    save_path = (Path(download_dir) / main_filename).with_suffix(\".nc\")\n",
    "\n",
    "    # if particular filename specified\n",
    "    if final_filename:\n",
    "        save_path = (\n",
    "            Path(download_dir) / file_ops.remove_suffix(final_filename)\n",
    "        ).with_suffix(\".nc\")\n",
    "\n",
    "    if save_path.is_file():\n",
    "        print(f\"Merged file already exists at {save_path}\")\n",
    "        return xa.open_dataset(save_path), save_path\n",
    "\n",
    "    date_merged_xas = []\n",
    "    # split request by variable\n",
    "    for var in tqdm(variables, desc=\" variable loop\", position=0, leave=True):\n",
    "        print(f\"Downloading {var} data...\")\n",
    "        # split request by time\n",
    "        date_pairs = utils.generate_date_pairs(date_lims)\n",
    "        # create download folder for each variable (if not already existing)\n",
    "        save_dir = Path(download_dir) / var\n",
    "        file_ops.guarantee_existence(save_dir)\n",
    "        for sub_date_lims in tqdm(date_pairs, leave=False):\n",
    "            # generate name info dictionary\n",
    "            name_dict = generate_name_dict(\n",
    "                var, sub_date_lims, lon_lims, lat_lims, depth_lims\n",
    "            )\n",
    "            filename = generate_spatiotemporal_var_filename_from_dict(name_dict)\n",
    "            # if file doesn't already exist, generate and execute API query\n",
    "            # print((Path(save_dir) / filename).with_suffix(\".nc\"))\n",
    "            if not (Path(save_dir) / filename).with_suffix(\".nc\").is_file():\n",
    "                query = generate_motu_query(\n",
    "                    save_dir,\n",
    "                    filename,\n",
    "                    var,\n",
    "                    sub_date_lims,\n",
    "                    lon_lims,\n",
    "                    lat_lims,\n",
    "                    depth_lims,\n",
    "                    product_type,\n",
    "                    service_id,\n",
    "                    product_id,\n",
    "                    username,\n",
    "                    password,\n",
    "                )\n",
    "                execute_motu_query(\n",
    "                    save_dir,\n",
    "                    filename,\n",
    "                    [var],\n",
    "                    sub_date_lims,\n",
    "                    lon_lims,\n",
    "                    lat_lims,\n",
    "                    depth_lims,\n",
    "                    query,\n",
    "                )\n",
    "            else:\n",
    "                print(f\"{filename} already exists in {save_dir}.\")\n",
    "\n",
    "        var_name_dict = generate_name_dict(\n",
    "            var, date_lims, lon_lims, lat_lims, depth_lims\n",
    "        )\n",
    "        date_merged_name = generate_spatiotemporal_var_filename_from_dict(var_name_dict)\n",
    "        # merge files by time\n",
    "        merged_path = merge_nc_files_in_dir(\n",
    "            merged_download_dir, date_merged_name, merged_save_path=(\n",
    "                merged_download_dir/date_merged_name).with_suffix(\".nc\"))\n",
    "        # generate metadata\n",
    "        generate_metadata(\n",
    "            merged_download_dir,\n",
    "            date_merged_name,\n",
    "            var,\n",
    "            date_lims,\n",
    "            lon_lims,\n",
    "            lat_lims,\n",
    "            depth_lims,\n",
    "        )\n",
    "        \n",
    "        date_merged_xas.append(merged_path)\n",
    "\n",
    "    # concatenate variables\n",
    "    arrays = [\n",
    "        (spatial_data.process_xa_d(date_merged_xa))\n",
    "        for date_merged_xa in date_merged_xas\n",
    "    ]\n",
    "    all_merged = xa.merge(arrays)\n",
    "\n",
    "    all_merged.to_netcdf(save_path)\n",
    "    # generate accompanying metadata\n",
    "    generate_metadata(\n",
    "        download_dir,\n",
    "        final_filename,\n",
    "        variables,\n",
    "        date_lims,\n",
    "        lon_lims,\n",
    "        lat_lims,\n",
    "        depth_lims,\n",
    "    )\n",
    "    print(f\"Combined nc file written to {save_path}.\")\n",
    "\n",
    "    return all_merged, save_path\n",
    "\n",
    "\n",
    "def execute_motu_query(\n",
    "    download_dir: str | Path,\n",
    "    filename: str,\n",
    "    var: list[str] | str,\n",
    "    sub_date_lims: tuple[np.datetime64, np.datetime64],\n",
    "    lon_lims: tuple[float, float],\n",
    "    lat_lims: tuple[float, float],\n",
    "    depth_lims: tuple[float, float],\n",
    "    query: str,\n",
    ") -> None:\n",
    "    \"\"\"Execute the MOTU query to download the data slice and generate metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    download_dir (str | Path): The directory where the file will be downloaded.\n",
    "    filename (str): The name of the file.\n",
    "    var (list[str] | str): The variable(s) to be downloaded.\n",
    "    sub_date_lims (tuple[np.datetime64, np.datetime64]): A tuple containing the start and end dates.\n",
    "    lon_lims (tuple[float, float]): A tuple containing the longitude limits.\n",
    "    lat_lims (tuple[float, float]): A tuple containing the latitude limits.\n",
    "    depth_lims (tuple[float, float]): A tuple containing the depth limits.\n",
    "    query (str): The MOTU query used for downloading the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    outcome = download_data_slice(query)\n",
    "    # if download successful\n",
    "    if outcome:\n",
    "        generate_metadata(\n",
    "            download_dir,\n",
    "            filename,\n",
    "            var,\n",
    "            sub_date_lims,\n",
    "            lon_lims,\n",
    "            lat_lims,\n",
    "            depth_lims,\n",
    "            query,\n",
    "        )\n",
    "        print(f\"{filename} written to {download_dir} and metadata generated.\")\n",
    "\n",
    "\n",
    "def generate_motu_query(\n",
    "    download_dir: str | Path,\n",
    "    filename: str,\n",
    "    variable: list[str] | str,\n",
    "    date_lims: tuple[np.datetime64, np.datetime64],\n",
    "    lon_lims: tuple[float, float],\n",
    "    lat_lims: tuple[float, float],\n",
    "    depth_lims: tuple[float, float],\n",
    "    product_type: str,\n",
    "    service_id: str,\n",
    "    product_id: str,\n",
    "    username: str,\n",
    "    password: str,\n",
    ") -> str:\n",
    "    \"\"\"Generate the MOTU query for downloading climate data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    download_dir (str | Path): The directory where the file will be downloaded.\n",
    "    filename (str): The name of the file.\n",
    "    variable (list[str] | str): The variable(s) to be downloaded.\n",
    "    date_lims (tuple[np.datetime64, np.datetime64]): A tuple containing the start and end dates.\n",
    "    lon_lims (tuple[float, float]): A tuple containing the longitude limits.\n",
    "    lat_lims (tuple[float, float]): A tuple containing the latitude limits.\n",
    "    depth_lims (tuple[float, float]): A tuple containing the depth limits.\n",
    "    product_type (str): The type of product.\n",
    "    service_id (str): The product ID.\n",
    "    product_id (str): The dataset ID.\n",
    "    username (str): The username for authentication.\n",
    "    password (str): The password for authentication.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query (str): The MOTU query.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    lon_min, lon_max = min(lon_lims), max(lon_lims)\n",
    "    lat_min, lat_max = min(lat_lims), max(lat_lims)\n",
    "    date_min, date_max = min(date_lims), max(date_lims)\n",
    "    depth_min, depth_max = min(depth_lims), max(depth_lims)\n",
    "\n",
    "    # generate motuclient command line\n",
    "    # specifying environment to conda environment makes sure that `motuclient` module is found\n",
    "    query = f\"conda run -n coralshift python -m motuclient \\\n",
    "    --motu https://{product_type}.cmems-du.eu/motu-web/Motu --service-id {service_id}-TDS --product-id {product_id} \\\n",
    "    --longitude-min {lon_min} --longitude-max {lon_max} --latitude-min {lat_min} --latitude-max {lat_max} \\\n",
    "    --date-min '{date_min}' --date-max '{date_max}' --depth-min {depth_min} --depth-max {depth_max} \\\n",
    "    --variable {variable} --out-dir '{download_dir}' --out-name '{filename}.nc' --user '{username}' --pwd '{password}'\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def download_data_slice(query):\n",
    "    \"\"\"Download the data slice using the MOTU query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query (str): The MOTU query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool: True if the download is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.system(query)\n",
    "        return True\n",
    "    except ConnectionAbortedError():\n",
    "        print(\"Data download failed.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def ecmwf_api_call(\n",
    "    c,\n",
    "    filepath: str,\n",
    "    parameter: str,\n",
    "    time_info_dict: dict,\n",
    "    area: list[tuple[float]],\n",
    "    dataset_tag: str = \"reanalysis-era5-single-levels\",\n",
    "    format: str = \"nc\",\n",
    "):\n",
    "    api_call_dict = generate_ecmwf_api_dict(parameter, time_info_dict, area, format)\n",
    "    # make api call\n",
    "    try:\n",
    "        c.retrieve(dataset_tag, api_call_dict, filepath)\n",
    "    # if error in fetching, limit the parameter\n",
    "    except ConnectionAbortedError():\n",
    "        print(f\"API call failed for {parameter}.\")\n",
    "\n",
    "\n",
    "def generate_ecmwf_api_dict(\n",
    "    weather_params: list[str], time_info_dict: dict, area: list[float], format: str\n",
    ") -> dict:\n",
    "    \"\"\"Generate api dictionary format for single month of event\"\"\"\n",
    "\n",
    "    # if weather_params\n",
    "\n",
    "    api_call_dict = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [weather_params],\n",
    "        \"area\": area,\n",
    "        \"format\": format,\n",
    "    } | time_info_dict\n",
    "\n",
    "    return api_call_dict\n",
    "\n",
    "\n",
    "def return_full_ecmwf_weather_param_strings(dict_keys: list[str]):\n",
    "    \"\"\"Look up weather parameters in a dictionary so they can be entered as short strings rather than typed out in full.\n",
    "    Key:value pairs ordered in expected importance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_keys : list[str]\n",
    "        list of shorthand keys for longhand weather parameters. See accompanying documentation on GitHub\n",
    "    \"\"\"\n",
    "\n",
    "    weather_dict = {\n",
    "        \"d2m\": \"2m_dewpoint_temperature\",\n",
    "        \"t2m\": \"2m_temperature\",\n",
    "        \"skt\": \"skin_temperature\",\n",
    "        \"tp\": \"total_precipitation\",\n",
    "        \"sp\": \"surface_pressure\",\n",
    "        \"src\": \"skin_reservoir_content\",\n",
    "        \"swvl1\": \"volumetric_soil_water_layer_1\",\n",
    "        \"swvl2\": \"volumetric_soil_water_layer_2\",\n",
    "        \"swvl3\": \"volumetric_soil_water_layer_3\",\n",
    "        \"swvl4\": \"volumetric_soil_water_layer_4\",\n",
    "        \"slhf\": \"surface_latent_heat_flux\",\n",
    "        \"sshf\": \"surface_sensible_heat_flux\",\n",
    "        \"ssr\": \"surface_net_solar_radiation\",\n",
    "        \"str\": \"surface_net_thermal_radiation\",\n",
    "        \"ssrd\": \"surface_solar_radiation_downwards\",\n",
    "        \"strd\": \"surface_thermal_radiation_downwards\",\n",
    "        \"e\": \"total_evaporation\",\n",
    "        \"pev\": \"potential_evaporation\",\n",
    "        \"ro\": \"runoff\",\n",
    "        \"ssro\": \"sub-surface_runoff\",\n",
    "        \"sro\": \"surface_runoff\",\n",
    "        \"u10\": \"10m_u_component_of_wind\",\n",
    "        \"v10\": \"10m_v_component_of_wind\",\n",
    "    }\n",
    "\n",
    "    weather_params = []\n",
    "    for key in dict_keys:\n",
    "        weather_params.append(weather_dict.get(key))\n",
    "\n",
    "    return weather_params\n",
    "\n",
    "\n",
    "def hourly_means_to_daily(hourly_dir: Path | str, suffix: str = \"netcdf\"):\n",
    "    filepaths = file_ops.return_list_filepaths(hourly_dir, suffix, incl_subdirs=True)\n",
    "    # create subdirectory to store averaged files\n",
    "    daily_means_dir = file_ops.guarantee_existence(Path(hourly_dir) / \"daily_means\")\n",
    "    for filepath in tqdm(filepaths, desc=\"Converting hourly means to daily means\"):\n",
    "        filename = \"_\".join((str(filepath.stem), \"daily\"))\n",
    "        save_path = (daily_means_dir / filename).with_suffix(\n",
    "            file_ops.pad_suffix(suffix)\n",
    "        )\n",
    "        # open dataset\n",
    "        hourly = xa.open_dataset(filepath, chunks={\"time\": 100})\n",
    "        daily = hourly.resample(time=\"1D\").mean()\n",
    "        # take average means\n",
    "        daily.to_netcdf(save_path)\n",
    "\n",
    "\n",
    "def generate_month_day_hour_list(items_range):\n",
    "    items = []\n",
    "\n",
    "    if isinstance(items_range, (int, np.integer)):\n",
    "        items_range = [items_range]\n",
    "    elif isinstance(items_range, np.ndarray):\n",
    "        items_range = items_range.tolist()\n",
    "    elif not isinstance(items_range, list):\n",
    "        raise ValueError(\n",
    "            \"Invalid input format. Please provide an integer, a list, or a NumPy array.\"\n",
    "        )\n",
    "\n",
    "    for item in items_range:\n",
    "        if isinstance(item, (int, np.integer)):\n",
    "            if item < 0 or item > 31:\n",
    "                raise ValueError(\"Invalid items value: {}.\".format(item))\n",
    "            items.append(item)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid input format. Please provide an integer, a list, or a NumPy array.\"\n",
    "            )\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def return_times_info(\n",
    "    year: int,\n",
    "    months: list[int] | int = np.arange(1, 13),\n",
    "    days: list[int] | int = np.arange(1, 32),\n",
    "    hours: list[int] | int = np.arange(0, 24),\n",
    "):\n",
    "    year = str(year)\n",
    "    months = [\n",
    "        utils.pad_number_with_zeros(month)\n",
    "        for month in generate_month_day_hour_list(months)\n",
    "    ]\n",
    "    days = [\n",
    "        utils.pad_number_with_zeros(day) for day in generate_month_day_hour_list(days)\n",
    "    ]\n",
    "\n",
    "    hours = [\n",
    "        utils.pad_number_with_zeros(hour)\n",
    "        for hour in generate_month_day_hour_list(hours)\n",
    "    ]\n",
    "    for h, hour in enumerate(hours):\n",
    "        hours[h] = f\"{hour}:00\"\n",
    "\n",
    "    return {\"year\": year, \"month\": months, \"day\": days, \"time\": hours}\n",
    "\n",
    "\n",
    "def fetch_weather_data(\n",
    "    download_dest_dir,\n",
    "    weather_params,\n",
    "    years,\n",
    "    months: list[int] | int = np.arange(1, 13),\n",
    "    days: list[int] | int = np.arange(1, 32),\n",
    "    hours: list[int] | int = np.arange(0, 24),\n",
    "    lat_lims=(-10, -17),\n",
    "    lon_lims=(142, 147),\n",
    "    dataset_tag: str = \"reanalysis-era5-single-levels\",\n",
    "    format: str = \"netcdf\",\n",
    "):\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    area = [max(lat_lims), min(lon_lims), min(lat_lims), max(lon_lims)]\n",
    "\n",
    "    for param in weather_params:\n",
    "        param_download_dest = file_ops.guarantee_existence(\n",
    "            Path(download_dest_dir) / param\n",
    "        )\n",
    "        for year in years:\n",
    "            filename = generate_spatiotemporal_var_filename_from_dict(\n",
    "                {\"var\": param, \"lats\": lat_lims, \"lons\": lon_lims, \"year\": str(year)}\n",
    "            )\n",
    "            # filename = str(file_ops.generate_filepath(param_download_dest, filename, format))\n",
    "            filepath = str(\n",
    "                file_ops.generate_filepath(param_download_dest, filename, format)\n",
    "            )\n",
    "\n",
    "            if not Path(filepath).is_file():\n",
    "                time_info_dict = return_times_info(year, months, days)\n",
    "                # filename = str(file_ops.generate_filepath(param_download_dest, f\"{param}_{year}\", format))\n",
    "                # filename = str((param_download_dest / param / str(year)).with_suffix(format))\n",
    "                ecmwf_api_call(\n",
    "                    c, filepath, param, time_info_dict, area, dataset_tag, format\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Filepath already exists: {filepath}\")\n",
    "        # TODO: more descriptive filename\n",
    "\n",
    "\n",
    "def generate_era5_data(\n",
    "    weather_params: list[float] = [\n",
    "        \"evaporation\",\n",
    "        \"significant_height_of_combined_wind_waves_and_swell\",\n",
    "        \"surface_net_solar_radiation\",\n",
    "        \"surface_pressure\",\n",
    "    ],\n",
    "    years: list[int] = np.arange(1993, 2021),\n",
    "    lat_lims: tuple[float] = (-10, -17),\n",
    "    lon_lims: tuple = (142, 147),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates and merges ERA5 weather data files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        weather_params (list[str]): A list of weather parameters to download and merge.\n",
    "        years (list[int]): A list of years for which to download and merge the data.\n",
    "        lat_lims (tuple[float, float]): A tuple specifying the latitude limits.\n",
    "        lon_lims (tuple[float, float]): A tuple specifying the longitude limits.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # fetch era5 diirectory for saving\n",
    "    save_dir = directories.get_era5_data_dir()\n",
    "\n",
    "    # download data to appropriate folder(s)\n",
    "    fetch_weather_data(\n",
    "        download_dest_dir=save_dir,\n",
    "        weather_params=weather_params,\n",
    "        years=years,\n",
    "        format=\"netcdf\",\n",
    "    )\n",
    "\n",
    "    # combine files in folder to single folder\n",
    "    combined_save_dir = file_ops.guarantee_existence(save_dir / \"weather_parameters\")\n",
    "    print(\"\\n\")\n",
    "    for param in weather_params:\n",
    "        # get path to unmerged files\n",
    "        param_dir = save_dir / param\n",
    "        merged_name = generate_spatiotemporal_var_filename_from_dict(\n",
    "            {\n",
    "                \"var\": param,\n",
    "                \"lats\": lat_lims,\n",
    "                \"lons\": lon_lims,\n",
    "                \"year\": f\"{str(years[0])}-{str(years[-1])}\",\n",
    "            }\n",
    "        )\n",
    "        # generate combined save path\n",
    "        combined_save_path = (combined_save_dir / merged_name).with_suffix(\".nc\")\n",
    "\n",
    "        file_ops.merge_nc_files_in_dir(\n",
    "            nc_dir=param_dir, merged_save_path=combined_save_path, format=\".netcdf\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"\\nAll ERA5 weather files downloaded by year and merged into {combined_save_dir}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_nc_files_in_dir(\n",
    "    nc_dir: Path | str,\n",
    "    filename: str = None,\n",
    "    merged_save_path: Path | str = None,\n",
    "    incl_subdirs: bool = False,\n",
    "    concat_dim: str = \"time\",\n",
    "    format: str = \".nc\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and merge all netCDF files in a directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        nc_dir (Path or str): The directory containing the netCDF files to be merged.\n",
    "        filename (str, optional): The desired filename for the merged netCDF file. If not specified, a default filename\n",
    "            is used.\n",
    "        merged_save_path (Path or str, optional): The path to save the merged netCDF file. If not specified, it will be\n",
    "            saved in the same directory as `nc_dir`.\n",
    "        incl_subdirs (bool, optional): Specifies whether to search for netCDF files in subdirectories as well. Default\n",
    "            is False.\n",
    "        concat_dim (str, optional): The name of the dimension along which the netCDF files will be concatenated.\n",
    "            Default is \"time\".\n",
    "        format (str, optional): The format of the netCDF file. Default is \".nc\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        merged_save_path (Path): The path where the merged netCDF file is saved.\n",
    "    \"\"\"\n",
    "    # specify whether searching subdirectories as well\n",
    "    filepaths = file_ops.return_list_filepaths(nc_dir, format, incl_subdirs)\n",
    "    # if only a single file present (no need to merge)\n",
    "    if len(filepaths) == 1:\n",
    "        print(f\"Single {format} file found in {str(nc_dir)}.\")\n",
    "        return xa.open_dataset(filepaths[0])\n",
    "\n",
    "    nc_dir = Path(nc_dir)\n",
    "\n",
    "    # if filename not specified\n",
    "    if not filename:\n",
    "        filename = f\"{nc_dir.parent.stem}_merged\"\n",
    "\n",
    "    # if path to save file not specified\n",
    "    if not merged_save_path:\n",
    "        merged_save_path = Path(nc_dir / file_ops.remove_suffix(filename)).with_suffix(\".nc\")\n",
    "\n",
    "    # check if already merged\n",
    "    if not merged_save_path.is_file():\n",
    "        print(f\"Merging {format} files into {merged_save_path}\")\n",
    "        merged_ds = process_xa_d(\n",
    "            xa.open_mfdataset(\n",
    "                filepaths, concat_dim=concat_dim, combine=\"nested\", drop_variables=['depth']\n",
    "            )\n",
    "        )\n",
    "        merged_ds.to_netcdf(merged_save_path)\n",
    "    else:\n",
    "        print(f\"{merged_save_path} already exists.\")\n",
    "    return merged_save_path\n",
    "\n",
    "\n",
    "def process_xa_d(\n",
    "    xa_d: xa.Dataset | xa.DataArray,\n",
    "    rename_mapping: dict = {\n",
    "        \"lat\": \"latitude\",\n",
    "        \"lon\": \"longitude\",\n",
    "        \"x\": \"longitude\",\n",
    "        \"y\": \"latitude\",\n",
    "    },\n",
    "    squeeze_coords: str | list[str] = None,\n",
    "    chunk_dict: dict = {\"latitude\": 100, \"longitude\": 100, \"time\": 100},\n",
    "    crs: str = \"EPSG:4326\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the input xarray Dataset or DataArray by standardizing coordinate names, squeezing dimensions,\n",
    "    chunking along specified dimensions, and sorting coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        xa_d (xa.Dataset or xa.DataArray): The xarray Dataset or DataArray to be processed.\n",
    "        rename_mapping (dict, optional): A dictionary specifying the mapping for coordinate renaming.\n",
    "            The keys are the existing coordinate names, and the values are the desired names.\n",
    "            Defaults to a mapping that standardizes common coordinate names.\n",
    "        squeeze_coords (str or list of str, optional): The coordinates to squeeze by removing size-1 dimensions.\n",
    "                                                      Defaults to ['band'].\n",
    "        chunk_dict (dict, optional): A dictionary specifying the chunk size for each dimension.\n",
    "                                     The keys are the dimension names, and the values are the desired chunk sizes.\n",
    "                                     Defaults to {'latitude': 100, 'longitude': 100, 'time': 100}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        xa.Dataset or xa.DataArray: The processed xarray Dataset or DataArray.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_xa_d = xa_d.copy()\n",
    "\n",
    "    for coord, new_coord in rename_mapping.items():\n",
    "        if new_coord not in temp_xa_d.coords and coord in temp_xa_d.coords:\n",
    "            temp_xa_d = temp_xa_d.rename({coord: new_coord})\n",
    "    # temp_xa_d = xa_d.rename(\n",
    "    #     {coord: rename_mapping.get(coord, coord) for coord in xa_d.coords}\n",
    "    # )\n",
    "    if \"band\" in temp_xa_d.dims:\n",
    "        temp_xa_d = temp_xa_d.squeeze(\"band\")\n",
    "    if squeeze_coords:\n",
    "        temp_xa_d = temp_xa_d.squeeze(squeeze_coords)\n",
    "\n",
    "    if \"time\" in temp_xa_d.dims:\n",
    "        temp_xa_d = temp_xa_d.transpose(\"time\", \"latitude\", \"longitude\", ...)\n",
    "    else:\n",
    "        temp_xa_d = temp_xa_d.transpose(\"latitude\", \"longitude\")\n",
    "\n",
    "    # add crs\n",
    "    temp_xa_d.rio.write_crs(crs, inplace=True)\n",
    "    chunked_xa_d = spatial_data.chunk_as_necessary(temp_xa_d, chunk_dict)\n",
    "    # sort coords by ascending values\n",
    "    return chunked_xa_d.sortby(list(temp_xa_d.dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " variable loop:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mlotst data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VARS_mlotst_DATES_1992-12-31_1993-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1993-12-31_1994-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1994-12-31_1995-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1995-12-31_1996-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1996-12-31_1997-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1997-12-31_1998-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1998-12-31_1999-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_1999-12-31_2000-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2000-12-31_2001-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2001-12-31_2002-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2002-12-31_2003-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2003-12-31_2004-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2004-12-31_2005-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2005-12-31_2006-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2006-12-31_2007-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2007-12-31_2008-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2008-12-31_2009-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2009-12-31_2010-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2010-12-31_2011-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2011-12-31_2012-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2012-12-31_2013-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2013-12-31_2014-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2014-12-31_2015-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2015-12-31_2016-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2016-12-31_2017-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2017-12-31_2018-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2018-12-31_2019-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n",
      "VARS_mlotst_DATES_2019-12-31_2020-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 already exists in /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " variable loop:   0%|          | 0/7 [00:18<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-23 12:50:54.577 [ INFO] Asynchronous mode set\n",
      "2023-06-23 12:50:54.578 [ INFO] Authenticating user otimmerman for service https://my.cmems-du.eu/motu-web/Motu\n",
      "2023-06-23 12:50:57.374 [ INFO] Requesting file to download (this can take a while)...\n",
      "2023-06-23 12:51:02.827 [ INFO] Authenticating user otimmerman for service https://my.cmems-du.eu/motu-web/Motu\n",
      "2023-06-23 12:51:11.065 [ERROR] 010-6 : The date range is invalid. Invalid date range: [2020-12-31 00:00:00,2021-12-31 00:00:00]. Valid range is: [1993-01-16 12:00:00,2020-12-16 12:00:00].\n",
      "\n",
      "Dictionary saved as json file at /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst/VARS_mlotst_DATES_2020-12-31_2021-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1.json\n",
      "VARS_mlotst_DATES_2020-12-31_2021-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1 written to /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/mlotst and metadata generated.\n",
      "Merging .nc files into /lustre_scratch/orlando-code/datasets/global_ocean_reanalysis/monthly_means/Great_Barrier_Reef_C/merged_vars/VARS_mlotst_DATES_1992-12-31_2021-12-31_LONS_148_154_LATS_-18_-24_DEPTHS_0-3_1.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# download monthly data. Can be adjusted to specify subset of variables, dates, and depths to download.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Values generated here are those reported in the accompanying paper.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m directories\u001b[39m.\u001b[39mget_monthly_cmems_dir()\n\u001b[0;32m----> 4\u001b[0m xa_cmems_monthly, cmems_monthly_path \u001b[39m=\u001b[39m download_reanalysis(download_dir\u001b[39m=\u001b[39;49m\u001b[39mdir\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     region \u001b[39m=\u001b[39;49m reef_areas\u001b[39m.\u001b[39;49mget_short_filename(area_name),\n\u001b[1;32m      6\u001b[0m     final_filename \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcmems_gopr_monthly_B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     lat_lims \u001b[39m=\u001b[39;49m reef_areas\u001b[39m.\u001b[39;49mget_lat_lon_limits(area_name)[\u001b[39m0\u001b[39;49m], lon_lims \u001b[39m=\u001b[39;49m reef_areas\u001b[39m.\u001b[39;49mget_lat_lon_limits(area_name)[\u001b[39m1\u001b[39;49m], \n\u001b[1;32m      8\u001b[0m     product_id \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcmems_mod_glo_phy_my_0.083_P1M-m\u001b[39;49m\u001b[39m\"\u001b[39;49m)   \n",
      "Cell \u001b[0;32mIn[6], line 259\u001b[0m, in \u001b[0;36mdownload_reanalysis\u001b[0;34m(download_dir, region, final_filename, variables, date_lims, depth_lims, lon_lims, lat_lims, product_type, service_id, product_id)\u001b[0m\n\u001b[1;32m    257\u001b[0m date_merged_name \u001b[39m=\u001b[39m generate_spatiotemporal_var_filename_from_dict(var_name_dict)\n\u001b[1;32m    258\u001b[0m \u001b[39m# merge files by time\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m merged_path \u001b[39m=\u001b[39m merge_nc_files_in_dir(merged_download_dir, date_merged_name)\n\u001b[1;32m    260\u001b[0m \u001b[39m# generate metadata\u001b[39;00m\n\u001b[1;32m    261\u001b[0m generate_metadata(\n\u001b[1;32m    262\u001b[0m     merged_download_dir,\n\u001b[1;32m    263\u001b[0m     date_merged_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     depth_lims,\n\u001b[1;32m    269\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 707\u001b[0m, in \u001b[0;36mmerge_nc_files_in_dir\u001b[0;34m(nc_dir, filename, merged_save_path, incl_subdirs, concat_dim, format)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m merged_save_path\u001b[39m.\u001b[39mis_file():\n\u001b[1;32m    705\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMerging \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m files into \u001b[39m\u001b[39m{\u001b[39;00mmerged_save_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    706\u001b[0m     merged_ds \u001b[39m=\u001b[39m process_xa_d(\n\u001b[0;32m--> 707\u001b[0m         xa\u001b[39m.\u001b[39;49mopen_mfdataset(\n\u001b[1;32m    708\u001b[0m             filepaths, concat_dim\u001b[39m=\u001b[39;49mconcat_dim, combine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnested\u001b[39;49m\u001b[39m\"\u001b[39;49m, drop_variables\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mdepth\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    709\u001b[0m         )\n\u001b[1;32m    710\u001b[0m     )\n\u001b[1;32m    711\u001b[0m     merged_ds\u001b[39m.\u001b[39mto_netcdf(merged_save_path)\n\u001b[1;32m    712\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/lustre_scratch/conda-envs/coralshift/lib/python3.10/site-packages/xarray/backends/api.py:1003\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mfspath(p) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p, os\u001b[39m.\u001b[39mPathLike) \u001b[39melse\u001b[39;00m p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m paths]\n\u001b[1;32m   1002\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m paths:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mno files to open\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1005\u001b[0m \u001b[39mif\u001b[39;00m combine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnested\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(concat_dim, (\u001b[39mstr\u001b[39m, DataArray)) \u001b[39mor\u001b[39;00m concat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "# download monthly data. Can be adjusted to specify subset of variables, dates, and depths to download.\n",
    "# Values generated here are those reported in the accompanying paper.\n",
    "dir = directories.get_monthly_cmems_dir()\n",
    "xa_cmems_monthly, cmems_monthly_path = download_reanalysis(download_dir=dir,\n",
    "    region = reef_areas.get_short_filename(area_name),\n",
    "    final_filename = \"cmems_gopr_monthly_B\",\n",
    "    lat_lims = reef_areas.get_lat_lon_limits(area_name)[0], lon_lims = reef_areas.get_lat_lon_limits(area_name)[1], \n",
    "    product_id = \"cmems_mod_glo_phy_my_0.083_P1M-m\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download daily data\n",
    "xa_cmems_daily, cmems_daily_path = climate_data.download_reanalysis(download_dir=directories.get_daily_cmems_dir(), \n",
    "    final_filename = \"cmems_gopr_daily.nc\",\n",
    "    lat_lims = reef_areas.get_lat_lon_limits(area_name)[0], lon_lims = reef_areas.get_lat_lon_limits(area_name)[1], \n",
    "    product_id = \"cmems_mod_glo_phy_my_0.083_P1D-m\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially pad the data\n",
    "\n",
    "TODO: add visual explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatially_buffer_timeseries(\n",
    "    xa_ds: xa.Dataset,\n",
    "    buffer_size: int = 3,\n",
    "    exclude_vars: list[str] = [\"spatial_ref\", \"coral_algae_1-12_degree\"],\n",
    ") -> xa.Dataset:\n",
    "    \"\"\"Applies a spatial buffer to each data variable in the xarray dataset.\n",
    "\n",
    "    Parameters\n",
    "        xa_ds (xarray.Dataset): Input xarray dataset.\n",
    "        buffer_size (int): Buffer size in grid cells.\n",
    "        exclude_vars (list[str]): List of variable names to exclude from buffering.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: Xarray dataset with buffered data variables.\n",
    "    \"\"\"\n",
    "    filtered_vars = [var for var in xa_ds.data_vars if var not in exclude_vars]\n",
    "\n",
    "    buffered_ds = xa.Dataset()\n",
    "    for data_var in tqdm(\n",
    "        filtered_vars, desc=f\"Buffering variables by {buffer_size} pixel(s)\"\n",
    "    ):\n",
    "        buffered = xa.apply_ufunc(\n",
    "            spatial_data.buffer_nans,\n",
    "            xa_ds[data_var],\n",
    "            input_core_dims=[[]],\n",
    "            output_core_dims=[[]],\n",
    "            kwargs={\"size\": buffer_size},\n",
    "            dask=\"parallelized\",\n",
    "        )\n",
    "        buffered_ds[data_var] = buffered\n",
    "\n",
    "    return buffered_ds\n",
    "\n",
    "\n",
    "def spatially_buffer_nc_file(nc_path: Path | str, buffer_size: int = 3):\n",
    "    # TODO: specify distance buffer\n",
    "    nc_path = Path(nc_path)\n",
    "    buffered_name = nc_path.stem + f\"_buffered_{buffer_size}_pixel\"\n",
    "    buffered_path = (nc_path.parent / buffered_name).with_suffix(\".nc\")\n",
    "\n",
    "    # if buffered file doesn't already exist\n",
    "    if not buffered_path.is_file():\n",
    "        nc_file = xa.open_dataset(nc_path)\n",
    "        buffered_ds = spatially_buffer_timeseries(\n",
    "            nc_file, buffer_size=buffer_size\n",
    "        )\n",
    "        buffered_ds.to_netcdf(buffered_path)\n",
    "    else:\n",
    "        buffered_ds = xa.open_dataset(buffered_path)\n",
    "        print(\n",
    "            f\"Area buffered by {buffer_size} pixel(s) already exists at {buffered_path}.\"\n",
    "        )\n",
    "\n",
    "    return buffered_ds, buffered_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xa_cmems_monthly_buffered, _ = spatial_data.spatially_buffer_nc_file(cmems_monthly_path, buffer_size=5)\n",
    "xa_cmems_daily_buffered, _ = spatially_buffer_nc_file(cmems_daily_path, buffer_size=5)\n",
    "# TODO: this taking forever (seemingly getting stuck after second variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_plots.plot_DEM(xa.open_dataset(directories.get_monthly_cmems_dir() / \"cmems_gopr_monthly_buffered_5_pixel\")[\"mlotst\"].isel(time=0), \"\")\n",
    "# spatial_plots.plot_DEM(buffered[\"mlotst\"].isel(time=0), \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_data.generate_era5_data(\n",
    "    lat_lims=reef_areas.get_lat_lon_limits(area_name)[0], lon_lims=reef_areas.get_lat_lon_limits(area_name)[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

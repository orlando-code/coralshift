{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "location = \"remote\"\n",
    "if location == \"remote\":\n",
    "    # TODO: hacky, shouldn't be necessary\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"lustre_scratch/coralshift/notebooks/convlstm.ipynb\"\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 08:21:31.389417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# importing entire keras library leads to quasi-errors: https://stackoverflow.com/questions/65493824/tensorflow-gpu-memory-allocation\n",
    "from tensorflow.keras import layers, models, losses, optimizers, callbacks\n",
    "\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n",
    "\n",
    "\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from scipy.interpolate import interp2d\n",
    "# from sklearn.utils import class_weight\n",
    "\n",
    "import xbatcher\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "# import rasterio\n",
    "# # from rasterio.plot import show\n",
    "# import rioxarray as rio\n",
    "\n",
    "# # from bs4 import BeautifulSoup\n",
    "# # import requests\n",
    "\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories\n",
    "from coralshift.plotting import spatial_plots, model_results\n",
    "from coralshift.dataloading import data_structure, climate_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "# import keras\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n",
      "[PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 08:21:36.360914: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 08:21:36.970597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29769 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        print(tf.config.experimental.get_visible_devices('GPU'))\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 24 08:49:24 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    46W / 163W |  31460MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    47W / 163W |  14424MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 40, 60\n",
    "seq_len = 10\n",
    "num_fs = 5\n",
    "target_frac = 0.1\n",
    "cells = 32\n",
    "\n",
    "# Creating the features array\n",
    "features = np.random.random((cells, seq_len, rows, cols, num_fs)).astype(np.float32)\n",
    "\n",
    "# Creating the label array\n",
    "labels = np.random.choice((2), size=(cells, 1, rows, cols, 1), p=[1-target_frac,target_frac])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels.flatten())\n",
    "\n",
    "# Verifying the shape of the arrays\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Label shape:\", labels.shape)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "\n",
    "# # Creating the features array in form (sequence_length, width, height, num_features)\n",
    "# features = np.random.random((num_samples, seq_len, rows, cols, num_fs)).astype(np.float32)\n",
    "\n",
    "# # # Creating the label array\n",
    "# # labels = np.repeat(np.expand_dims(\n",
    "# #     np.random.choice((2), size=(rows, cols), p=[1-target_frac,target_frac]).reshape((1,40,60,1)), axis=0),\n",
    "# #     num_samples, axis=0)\n",
    "\n",
    "# # batches_X = np.repeat(np.expand_dims(features,axis=0), 100, axis=0)\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels.flatten())\n",
    "\n",
    "# # Verifying the shape of the arrays\n",
    "# print(\"Features shape:\", features.shape)\n",
    "# print(\"Label shape:\", labels.shape)\n",
    "# print(\"Class weights:\", class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send numpy features/label array to tf.Data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "SHUFFLE_BUFFER_SIZE = 10\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(\"Number of batches:\", len(train_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace toy with real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs shape:  (187, 137, 4, 336)\n",
      "ys shape:  (187, 137)\n"
     ]
    }
   ],
   "source": [
    "# TODO: replace with all variables\n",
    "dir = directories.get_datasets_dir() / \"test\"\n",
    "Xs, ys = np.load(dir / \"Xs_conv.npy\"), np.load(dir / \"ys_conv.npy\")\n",
    "\n",
    "# Xs = np.expand_dims(Xs, axis=0)\n",
    "# ys = np.expand_dims(np.expand_dims(np.expand_dims(ys, axis=-1), axis=-1), axis=0)\n",
    "\n",
    "print(\"Xs shape: \", Xs.shape)\n",
    "print(\"ys shape: \", ys.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights:  [0.54231583 6.40795398]\n"
     ]
    }
   ],
   "source": [
    "# X_train = Xs[:1000, :, :20]\n",
    "# y_train = ys[:1000]\n",
    "X_train = Xs\n",
    "y_train = ys\n",
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.flatten())\n",
    "print(\"class weights: \", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(187, 137, 4, 336), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(187, 137, 1, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send numpy features/label array to tf.Data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 187, 137, 4, 336), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 187, 137, 1, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 12\n",
    "SHUFFLE_BUFFER_SIZE = 2\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(\"Number of batches:\", len(train_dataset))\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 187, 137, 4, 336])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features.shape[2:]\n",
    "inp = layers.Input(shape=(X_train.shape[1:]))\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stitch_up(X, rows, cols):\n",
    "    # nested concat split calls to\n",
    "    # stitch back together a image\n",
    "    # from a 5 x 3 grid of sub-images\n",
    "    return tf.concat(\n",
    "            tf.split(\n",
    "                tf.concat(\n",
    "                    tf.split(\n",
    "                        X, cols, axis=0),\n",
    "                    axis=2),\n",
    "                rows,\n",
    "                axis=0),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "\n",
    "def pad_to_box(X, rows, cols):\n",
    "    height = tf.shape(X)[0]\n",
    "    width = tf.shape(X)[1]\n",
    "    target_height = tf.cast(tf.math.ceil(height / rows) * rows, tf.int32)\n",
    "    target_width = tf.cast(tf.math.ceil(width / cols) * cols, tf.int32)\n",
    "\n",
    "    # Pad the image to the target height and width\n",
    "    return tf.image.resize_with_pad(X, 0, 0, target_height, target_width)\n",
    "\n",
    "def \n",
    "\n",
    "def cut_squares(X, rows, cols):\n",
    "    # nested split concat calls\n",
    "    # to break image into 5 x 3 grid\n",
    "    # of 448 * 448 sub-images\n",
    "    return tf.concat(\n",
    "        tf.split(\n",
    "            tf.concat(\n",
    "                tf.split(X, rows, axis=1),\n",
    "                axis=0\n",
    "            ),\n",
    "            cols,\n",
    "            axis=2),\n",
    "        axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = tf.convert_to_tensor(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 137, 4, 336)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(187, 190, 140, 336), dtype=float64, numpy=\n",
       "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]]])>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut_squares(ten, 5, 3)\n",
    "pad_to_box(Xs, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wm2jdwof) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fanciful-firebrand-24</strong> at: <a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/wm2jdwof' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm/runs/wm2jdwof</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230624_084558-wm2jdwof/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wm2jdwof). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdda3bd1e894912acda721eb34ad9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669322814171512, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lustre_scratch/orlando-code/coralshift/wandb/run-20230624_084802-3yd50tuo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/3yd50tuo' target=\"_blank\">colorful-salad-25</a></strong> to <a href='https://wandb.ai/orlando-code/new_conv_lstm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/orlando-code/new_conv_lstm' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/3yd50tuo' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm/runs/3yd50tuo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 08:48:26.544440: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\025TensorSliceDataset:29\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 187\n",
      "        }\n",
      "        dim {\n",
      "          size: 137\n",
      "        }\n",
      "        dim {\n",
      "          size: 4\n",
      "        }\n",
      "        dim {\n",
      "          size: 336\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 187\n",
      "        }\n",
      "        dim {\n",
      "          size: 137\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeabcaaad704c189e4dd496a5f02408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c450c797c0654d36900b10cae94579a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " train batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"Shape_1:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"Shape:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"Shape_1:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 08:48:33.246393: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/StatefulPartitionedCall/gradient_tape/conv_lst_model_2/conv_lstm2d_8/while/conv_lst_model_2/conv_lstm2d_8/while_grad/body/_620/input/_1595' -> 'StatefulPartitionedCall/gradient_tape/conv_lst_model_2/conv_lstm2d_8/while/conv_lst_model_2/conv_lstm2d_8/while_grad/body/_620/gradient_tape/conv_lst_model_2/conv_lstm2d_8/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/conv_lst_model_2/conv_lstm2d_6/while/conv_lst_model_2/conv_lstm2d_6/while_grad/body/_1010/input/_1830' -> 'StatefulPartitionedCall/gradient_tape/conv_lst_model_2/conv_lstm2d_6/while/conv_lst_model_2/conv_lstm2d_6/while_grad/body/_1010/gradient_tape/conv_lst_model_2/conv_lstm2d_6/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/conv_lst_model_2/conv_lstm2d_7/while/conv_lst_model_2/conv_lstm2d_7/while_grad/body/_815/input/_1714' -> 'StatefulPartitionedCall/gradient_tape/conv_lst_model_2/conv_lstm2d_7/while/conv_lst_model_2/conv_lstm2d_7/while_grad/body/_815/gradient_tape/conv_lst_model_2/conv_lstm2d_7/while/gradients/AddN', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/Relu' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/mul_3', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/clip_by_value_2' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/mul_5', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/mul_2' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/add_5', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/convolution_4' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_7/while/body/_244/conv_lst_model_2/conv_lstm2d_7/while/add', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/mul_2' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/add_5', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/clip_by_value_2' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/mul_5', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/convolution_4' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/add', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/Relu' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_8/while/body/_432/conv_lst_model_2/conv_lstm2d_8/while/mul_3', 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_6/while/body/_66/conv_lst_model_2/conv_lstm2d_6/while/Identity_4' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_6/while/next_iteration/_213', 'Func/StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_6/while/body/_66/input/_1235' -> 'StatefulPartitionedCall/conv_lst_model_2/conv_lstm2d_6/while/body/_66/conv_lst_model_2/conv_lstm2d_6/while/mul_2'}.\n",
      "2023-06-24 08:48:34.385227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-06-24 08:48:35.835026: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f2ea5b7c710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-24 08:48:35.835098: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-32GB-LS, Compute Capability 7.0\n",
      "2023-06-24 08:48:35.840696: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-24 08:48:35.965682: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdbaed031a84ac8a755fffbf7e24660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " test batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 08:48:39.677443: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'conv_lst_model_2/conv_lstm2d_8/while/body/_97/conv_lst_model_2/conv_lstm2d_8/while/Relu_1' -> 'conv_lst_model_2/conv_lstm2d_8/while/body/_97/conv_lst_model_2/conv_lstm2d_8/while/mul_5', 'conv_lst_model_2/conv_lstm2d_8/while/body/_97/conv_lst_model_2/conv_lstm2d_8/while/mul_2' -> 'conv_lst_model_2/conv_lstm2d_8/while/body/_97/conv_lst_model_2/conv_lstm2d_8/while/add_5', 'conv_lst_model_2/conv_lstm2d_8/while/body/_97/conv_lst_model_2/conv_lstm2d_8/while/convolution_7' -> 'conv_lst_model_2/conv_lstm2d_8/while/body/_97/conv_lst_model_2/conv_lstm2d_8/while/add_6', 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/mul_2' -> 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/add_5', 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/clip_by_value' -> 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/mul_3', 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/convolution_6' -> 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/add_4', 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/clip_by_value_2' -> 'conv_lst_model_2/conv_lstm2d_7/while/body/_49/conv_lst_model_2/conv_lstm2d_7/while/mul_5', 'conv_lst_model_2/conv_lstm2d_6/while/body/_1/conv_lst_model_2/conv_lstm2d_6/while/Identity_4' -> 'conv_lst_model_2/conv_lstm2d_6/while/next_iteration/_44', 'Func/conv_lst_model_2/conv_lstm2d_6/while/body/_1/input/_164' -> 'conv_lst_model_2/conv_lstm2d_6/while/body/_1/conv_lst_model_2/conv_lstm2d_6/while/mul_2'}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833a5eb116ae49919b582554f29f0fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " train batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3af01e731e4f6698eaa7a19f2181f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " test batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032885070a98499fb98e9d4d46f39dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " train batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f17c3f5c7424fd6a8aea6a68590b166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " test batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4e48dca1d744dfb5fc339f02240262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " train batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f58ef51d644db38b3fc3002b5b2351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " test batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a561876296f494ea31a1b4db156440c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-salad-25</strong> at: <a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/3yd50tuo' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm/runs/3yd50tuo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230624_084802-3yd50tuo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################\n",
    "# DEFINE CONVOLUTIONAL LSTM MODEL\n",
    "####################################\n",
    "\n",
    "\n",
    "class ConvLSTModel(tf.keras.Model):\n",
    "    \"\"\"Convolutional LSTM model for binary classification of spatiotemporal data\"\"\"\n",
    "\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, filters: list[int] = [32, 64, 128], kernel_sizes: list[int] = [3,3,3], \n",
    "        strides: list[int] = [1,1,1], num_layers: int = 3, padding: str = \"same\", activation: str = \"relu\"\n",
    "        # input_shape: tuple[int] = INPUT_SHAPE,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the ConvLSTM model (ConvLSTModel)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            filters (list[int]): List of filters for each ConvLSTM2D layer.\n",
    "            kernel_sizes (list[int]): List of kernel sizes for each ConvLSTM2D layer.\n",
    "            strides (list[int]): List of strides for each ConvLSTM2D layer.\n",
    "            num_layers (int): Number of ConvLSTM2D layers.\n",
    "            padding (str): Padding mode for ConvLSTM2D layers.\n",
    "            activation (str): Activation function to use.\n",
    "        \"\"\"\n",
    "\n",
    "        # initialise CoonvLSTModel model as subclass of tf.keras.Model\n",
    "        super(ConvLSTModel, self).__init__()\n",
    "\n",
    "        # check argument consistency\n",
    "        assert(len(filters) == num_layers)\n",
    "        assert(len(kernel_sizes) == len(filters))\n",
    "        assert(len(strides) == len(filters))\n",
    "\n",
    "        self.conv_lstm_0 = layers.ConvLSTM2D(filters=filters[0],\n",
    "                                            kernel_size=(kernel_sizes[0], kernel_sizes[0]),\n",
    "                                            strides=(strides[0], strides[0]),\n",
    "                                            padding=padding,\n",
    "                                            return_sequences=True,\n",
    "                                            activation=activation)\n",
    "\n",
    "\n",
    "        self.conv_lstm_1 = layers.ConvLSTM2D(filters=filters[1],\n",
    "                                            kernel_size=(kernel_sizes[1], kernel_sizes[1]),\n",
    "                                            strides=(strides[1], strides[1]),\n",
    "                                            padding=padding,\n",
    "                                            return_sequences=True,\n",
    "                                            activation=activation)\n",
    "\n",
    "\n",
    "        self.conv_lstm_2 = layers.ConvLSTM2D(filters=filters[2],\n",
    "                                            kernel_size=(kernel_sizes[2], kernel_sizes[2]),\n",
    "                                            strides=(strides[2], strides[2]),\n",
    "                                            padding=padding,\n",
    "                                            return_sequences=True,\n",
    "                                            activation=activation)\n",
    "\n",
    "        self.batch_norm_0 = layers.BatchNormalization()\n",
    "        self.batch_norm_1 = layers.BatchNormalization()\n",
    "        self.batch_norm_2 = layers.BatchNormalization()\n",
    "\n",
    "        # reducing parameter space to 1 to match label feature dimension\n",
    "        self.dense_layer_out = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # for now, threee convLSTM2D layers with batchnorm sandwiches\n",
    "        x = self.conv_lstm_0(inputs)\n",
    "        x = self.batch_norm_0(x, training=training)\n",
    "        x = self.conv_lstm_1(x)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = self.conv_lstm_2(x)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        x = self.dense_layer_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "###########################\n",
    "# INITIALISE PARAMETERS\n",
    "###########################\n",
    "\n",
    "num_features = 5\n",
    "seq_length = 100\n",
    "\n",
    "# configure model\n",
    "LEARNING_RATE = 1e-3    # TODO: adjustable (not particularly preessing though)\n",
    "INPUT_SHAPE = (None, seq_length, None, None, num_features)\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "EPOCHS = 4\n",
    "ACTIVATION = \"relu\"\n",
    "BATCH_SIZE_PER_REPLICA = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"new_conv_lstm\",\n",
    "    entity=\"orlando-code\",\n",
    "    settings=wandb.Settings(start_method=\"fork\")\n",
    "    )\n",
    "\n",
    "wandb.config = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"activation\": ACTIVATION,\n",
    "    \"batch_size_per_replica\": BATCH_SIZE_PER_REPLICA\n",
    "}\n",
    "\n",
    "\n",
    "# set up distributed processing\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "GLOBAL_BATCH_SIZE = (BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync)\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    conv_lstm = ConvLSTModel(activation=wandb.config[\"activation\"])\n",
    "    DIST_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    def compute_loss(\n",
    "        labels: tf.Tensor, predictions: tf.Tensor, class_weights: tuple[float] = None, distributed: bool = True):\n",
    "        \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "        incorporating class weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            labels (tf.Tensor): True binary labels, where 0 represents the negative class.\n",
    "            predictions (tf.Tensor): Predicted labels as probability values between 0 and 1.\n",
    "            class_weights (np.ndarray): Weights for each class. If None, no class weights will be applied.\n",
    "            distributed (bool): Flag indicating if the computation is distributed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float: Negative log likelihood loss computed using binary cross-entropy loss between 'labels' and \n",
    "                'predictions', incorporating class weights if provided.\n",
    "        \"\"\"\n",
    "        per_example_loss = negative_log_likelihood(labels, predictions, class_weights, distributed)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    train_accuracy = tf.keras.metrics.BinaryCrossentropy(name=\"train_accuracy\")\n",
    "    test_accuracy = tf.keras.metrics.BinaryCrossentropy(name=\"test_accuracy\")\n",
    "    # would use BinaryAccuracy if predicting ones/zeros\n",
    "\n",
    "# cast datasets to mirrored versions for distributed processing\n",
    "dist_train_dataset = mirrored_strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "\n",
    "#################################\n",
    "# DEFINE TRAIN/TEST FUNCTIONS\n",
    "#################################\n",
    "\n",
    "\n",
    "def negative_log_likelihood(\n",
    "    labels: np.ndarray, predictions: np.ndarray, class_weights: np.ndarray = None, distributed: bool=False) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if distributed:\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)  \n",
    "\n",
    "    print(tf.shape(labels))\n",
    "    print(tf.shape(predictions))\n",
    "\n",
    "    # TODO: this may be broken, throwing error associated with numpy/tensor incompatability\n",
    "    if class_weights is not None:\n",
    "        # sample_weights = tf.gather(class_weights, np.asarray(tf.concat(labels.values, axis=0), dtype=np.int32))\n",
    "        sample_weights = tf.gather(class_weights, tf.cast(labels, dtype=tf.int32))\n",
    "\n",
    "        # reshape to match size of y and y_pred\n",
    "        return bce(labels, predictions, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "    return bce(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor, \n",
    "    optimizer: tf.keras.optimizers.Optimizer = OPTIMIZER, learn_rate: float = LEARNING_RATE, \n",
    "    class_weights: tuple = None, distributed: bool=False):\n",
    "    \n",
    "    with tf.GradientTape() as lstm_tape:\n",
    "        predictions = conv_lstm(features, training=True)\n",
    "        # Calculate loss\n",
    "        loss = compute_loss(labels, predictions, class_weights, distributed)\n",
    "    \n",
    "    # get trainable variables\n",
    "    trainable_vars = conv_lstm.trainable_variables\n",
    "    # Calculate gradient          `\n",
    "    lstm_grads = lstm_tape.gradient(loss, trainable_vars)\n",
    "    # And then apply the gradient to change the weights\n",
    "    optimizer.apply_gradients(zip(lstm_grads, trainable_vars))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor):\n",
    "\n",
    "    predictions = conv_lstm(features, training=False)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(model: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor, \n",
    "    optimizer: tf.keras.optimizers.Optimizer = DIST_OPTIMIZER, learn_rate: float = LEARNING_RATE,\n",
    "    class_weights: tuple = None, distributed: bool = True):\n",
    "    \"\"\"Performs a distributed training step on the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (tf.keras.Model): The model to train.\n",
    "        features (tf.Tensor): Input features for training.\n",
    "        labels (tf.Tensor): True labels for training.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): Optimizer to use for training. Default is DIST_OPTIMIZER.\n",
    "        learn_rate (float): Learning rate for the optimizer. Default is LEARNING_RATE.\n",
    "        class_weights (tuple): Weights for each class. If None, no class weights will be applied. Default is None.\n",
    "        distributed (bool): Flag indicating if the computation is distributed. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tf.Tensor: Loss computed during the training step.\n",
    "    \"\"\"\n",
    "    per_replica_losses = mirrored_strategy.run(train_step, args=(\n",
    "        model, features, labels, optimizer, learn_rate, class_weights, distributed,))\n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor):\n",
    "    \"\"\"Performs a distributed test step on the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        conv_lstm (tf.keras.Model): The model to evaluate.\n",
    "        features (tf.Tensor): Input features for evaluation.\n",
    "        labels (tf.Tensor): True labels for evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    return mirrored_strategy.run(test_step, args=(conv_lstm, features, labels,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# RUN MODEL\n",
    "################\n",
    "\n",
    "for epoch in tqdm(range(wandb.config[\"epochs\"]), desc=\"epochs\", position=0):\n",
    "    # Just total loss\n",
    "    epoch_loss = 0\n",
    "    # Iterate through the train dataset\n",
    "    for features, labels in tqdm(dist_train_dataset, desc=\" train batches\", position=1):\n",
    "        loss = distributed_train_step(conv_lstm, features, labels, DIST_OPTIMIZER, wandb.config[\"learning_rate\"],\n",
    "            # class_weights\n",
    "            )\n",
    "        epoch_loss += loss\n",
    "        wandb.log({\"epoch loss\": epoch_loss, \"train accuracy\": train_accuracy.result()})\n",
    "\n",
    "    # Iterate through the test dataset\n",
    "    for features, labels in tqdm(dist_train_dataset, desc=\" test batches\", position=1):\n",
    "        distributed_test_step(conv_lstm, features, labels)\n",
    "        wandb.log({\"test accuracy\": test_accuracy.result()})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = layers.Input(shape=(features.shape[1:]))\n",
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4*10*60*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(features.shape[1:]))\n",
    "\n",
    "convlstm = ConvLSTModel()\n",
    "\n",
    "convlstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                       tf.keras.metrics.FalseNegatives()])\n",
    "# convlstm.build(INPUT_SHAPE)\n",
    "# convlstm.build(features.shape[:])\n",
    "\n",
    "convlstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(3,))\n",
    "outputs = tf.keras.layers.Dense(2)(inputs)\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.metrics_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and weights\n",
    "\n",
    "[Source](https://towardsdatascience.com/creating-a-trashy-model-from-scratch-with-tensorflow-2-an-example-with-an-anomaly-detection-27f0d1d7bd00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.save_weights(filepath='encoder_model.tf')\n",
    "\n",
    "# # Saving the configuration\n",
    "# import json\n",
    "# ## Encoder\n",
    "# encoder_dict = {'fillters': [32, 64, 128, 196, 256, 1024],\n",
    "#                 'kernel_sizes': [3, 3, 3, 3, 3, 3],\n",
    "#                 'strides': [2, 2, 2, 2, 2, 2],\n",
    "#                 'num_of_layers': 6}\n",
    "# with open('encoder_config.json', 'w') as fp:\n",
    "#     json.dump(encoder_dict, fp)\n",
    "\n",
    "# ## Decoder\n",
    "# decoder_dict = {'fillters': [1024, 256, 196, 128, 64, 32, 1], \n",
    "#                 'kernel_sizes': [3, 3, 3, 3, 3, 3, 1],\n",
    "#                 'strides': [2, 2, 2, 2, 2, 2, 1],\n",
    "#                 'num_of_layers': 7}\n",
    "# with open('decoder_config.json', 'w') as fp:\n",
    "#     json.dump(decoder_dict, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

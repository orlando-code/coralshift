{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "location = \"remote\"\n",
    "if location == \"remote\":\n",
    "    # TODO: hacky, shouldn't be necessary\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"lustre_scratch/coralshift/notebooks/convlstm.ipynb\"\n",
    "    os.chdir(\"/lustre_scratch/orlando-code/coralshift/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 10:23:49.118930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xa\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# importing entire keras library leads to quasi-errors: https://stackoverflow.com/questions/65493824/tensorflow-gpu-memory-allocation\n",
    "from tensorflow.keras import layers, models, losses, optimizers, callbacks\n",
    "\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n",
    "\n",
    "\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from scipy.interpolate import interp2d\n",
    "# from sklearn.utils import class_weight\n",
    "\n",
    "import xbatcher\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "# import rasterio\n",
    "# # from rasterio.plot import show\n",
    "# import rioxarray as rio\n",
    "\n",
    "# # from bs4 import BeautifulSoup\n",
    "# # import requests\n",
    "\n",
    "\n",
    "#issues with numpy deprecation in pytorch_env\n",
    "from coralshift.processing import spatial_data\n",
    "from coralshift.utils import file_ops, directories\n",
    "from coralshift.plotting import spatial_plots, model_results\n",
    "from coralshift.dataloading import data_structure, climate_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "# import keras\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (32, 10, 40, 60, 5)\n",
      "Label shape: (32, 1, 40, 60, 1)\n",
      "Class weights: [0.55223194 5.28634361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 10:24:04.768261: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 10:24:05.871687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31017 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2023-06-17 10:24:05.873957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31017 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:8a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "rows, cols = 40, 60\n",
    "seq_len = 10\n",
    "num_fs = 5\n",
    "target_frac = 0.1\n",
    "num_samples = 32\n",
    "\n",
    "# Creating the features array in form (sequence_length, width, height, num_features)\n",
    "features = np.random.random((num_samples, seq_len, rows, cols, num_fs)).astype(np.float32)\n",
    "\n",
    "# Creating the label array\n",
    "labels = np.repeat(np.expand_dims(\n",
    "    np.random.choice((2), size=(rows, cols), p=[1-target_frac,target_frac]).reshape((1,40,60,1)), axis=0),\n",
    "    num_samples, axis=0)\n",
    "\n",
    "batches_X = np.repeat(np.expand_dims(features,axis=0), 100, axis=0)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels.flatten())\n",
    "\n",
    "# Verifying the shape of the arrays\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Label shape:\", labels.shape)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "# send numpy features/label array to tf.Data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE_BUFFER_SIZE = 10\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(\"Number of batches:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if class_weights is not None:\n",
    "        sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "        return bce(y, y_pred, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "    return bce(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.shape[2:]\n",
    "inp = layers.Input(shape=(None, *features.shape[1:]))\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construct the input layer with no definite frame size.\n",
    "# inp = layers.Input(shape=(features.shape[1:]))\n",
    "# # inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
    "\n",
    "# # TODO: add config file, potentially with smaller learning rate\n",
    "# # https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons\n",
    "# # mirrored_strategy leads to nan loss values\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# with mirrored_strategy.scope():\n",
    "\n",
    "#     # We will construct 3 `ConvLSTM2D` layers with batch normalization\n",
    "#     # \n",
    "#     x = layers.ConvLSTM2D(\n",
    "#         filters=64,\n",
    "#         kernel_size=(5, 5),\n",
    "#         padding=\"same\",\n",
    "#         return_sequences=True,\n",
    "#         activation=\"relu\",\n",
    "#     )(inp)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ConvLSTM2D(\n",
    "#         filters=64,\n",
    "#         kernel_size=(3, 3),\n",
    "#         padding=\"same\",\n",
    "#         return_sequences=True,\n",
    "#         activation=\"relu\",\n",
    "#     )(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ConvLSTM2D(\n",
    "#         filters=64,\n",
    "#         kernel_size=(1, 1),\n",
    "#         padding=\"same\",\n",
    "#         return_sequences=True,\n",
    "#         activation=\"relu\",\n",
    "#     )(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ConvLSTM2D(\n",
    "#         filters=64,\n",
    "#         kernel_size=(1, 1),\n",
    "#         padding=\"same\",\n",
    "#         return_sequences=True,\n",
    "#         activation=\"relu\",\n",
    "#     )(x)\n",
    "\n",
    "#     # Reshape the output tensor to match the target shape (num_samples, 1, rows, cols, 1)\n",
    "#     # x = layers.Reshape((1, *x.shape[2:]))(x)\n",
    "#     # x= layers.TimeDistributed(layers.Dense(1))(x)\n",
    "#     x= layers.Dense(1)(x)\n",
    "\n",
    "#     # # Add a final Conv3D layer for the spatiotemporal outputs\n",
    "#     # x = layers.Conv3D(\n",
    "#     #     filters=8, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "#     # )(x)\n",
    "\n",
    "#     # Next, we will build the complete model and compile it.\n",
    "#     model = models.Model(inputs=inp,outputs=x)\n",
    "\n",
    "#     # TODO: include weighting of classes: https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "#     model.compile(\n",
    "#         loss=losses.binary_crossentropy,\n",
    "#         optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "#     )\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(features))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reshape((1,40,60,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_X = np.repeat(np.expand_dims(features,axis=1), 100, axis=0)\n",
    "batches_y = np.repeat(np.expand_dims(labels,axis=1), 100, axis=0)\n",
    "\n",
    "print(\"batches_X:\", batches_X.shape)\n",
    "print(\"batches_y:\", batches_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_batches_X = np.tile(batches_X, (1, batches_y.shape[1], 1, 1, 1))\n",
    "duplicated_batches_X.shape\n",
    "duplicated_batches_y = np.tile(batches_y, (1, batches_X.shape[1], 1, 1, 1))\n",
    "duplicated_batches_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define some callbacks to improve training. Patience measured in number of epochs.\n",
    "# early_stopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# # Define modifiable training hyperparameters.\n",
    "# epochs = 4\n",
    "# batch_size = 32\n",
    "\n",
    "# # Start a run, tracking hyperparameters\n",
    "# # wandb.init(\n",
    "# #     # set the wandb project where this run will be logged\n",
    "# #     project=\"coralshift_test\",\n",
    "# # )\n",
    "\n",
    "\n",
    "# # with tf.device(\"/GPU:0\"):\n",
    "# # Fit the model to the training data.\n",
    "# model.fit(\n",
    "#     # x_train,\n",
    "#     # y_train,\n",
    "#     batches_X,\n",
    "#     batches_y,\n",
    "#     # class_weight=class_weights,\n",
    "#     # batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     # validation_data=(x_val, y_val),\n",
    "#     validation_data=(batches_X, batches_y),\n",
    "#     callbacks=[early_stopping, reduce_lr,\n",
    "#         # WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\"),\n",
    "#         # WandbCallback(training_data=(padded_spatial_Xs,padded_spatial_ys), log_weights=True, log_gradients=True)\n",
    "# ])\n",
    "\n",
    "# # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = features\n",
    "# y_train = labels\n",
    "\n",
    "# # wandb.init(\n",
    "# #     project=\"coralshift\",\n",
    "# #     entity=\"orlando-code\",\n",
    "# #     settings=wandb.Settings(start_method=\"fork\")\n",
    "# #     # config={    }\n",
    "# #     )\n",
    "\n",
    "# # initialize optimiser: will need hyperparameter scan for learning rate and others\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "# optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "# # X = ds_man.get_dataset(\"monthly_climate_1_12_X_np\")\n",
    "# # y = ds_man.get_dataset(\"monthly_climate_1_12_y_np\")\n",
    "# # # check that untrained model runs (should output array of non-nan values)\n",
    "# # # why values change?\n",
    "# # # g_model(X[:32])\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "# #     X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "# #     sub_X, sub_y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# # Define Convolution LSTM Model\n",
    "# class ConvLSTModel(tf.keras.Model):\n",
    "#     # initialise class instance to define layers of the model\n",
    "#     def __init__(self, filters: list[int] = [32, 64, 128], kernel_sizes: list[int] = [3,3,3], strides = list[int] = [1,1,1],\n",
    "#         input_shape: tuple[int] = INPUT_SHAPE, num_layers: int = 3):\n",
    "\n",
    "#         # initialise CoonvLSTModel model as subclass of tf.keras.Model\n",
    "#         super(ConvLSTModel, self).__init__()\n",
    "\n",
    "#         # check argument consistency\n",
    "#         assert(len(filters) == num_layers)\n",
    "#         assert(len(kernel_sizes) == len(filters))\n",
    "#         assert(len(strides) == len(filters))\n",
    "\n",
    "#         # TODO: put outside in wandb config\n",
    "#         activation = \"relu\"\n",
    "\n",
    "#         self.conv_lstm_0 = layers.ConvLSTM2D(filters=filters[0],\n",
    "#                                             kernel_size=(kernel_sizes[0], kernel_sizes[0]),\n",
    "#                                             strides=(strides[0], strides[0])\n",
    "#                                             padding=padding,\n",
    "#                                             return_sequences=True,\n",
    "#                                             activation=activation)\n",
    "\n",
    "\n",
    "#         self.conv_lstm_1 = layers.ConvLSTM2D(filters=filters[1],\n",
    "#                                             kernel_size=(kernel_sizes[1], kernel_sizes[1]),\n",
    "#                                             strides=(strides[1], strides[1])\n",
    "#                                             padding=padding,\n",
    "#                                             return_sequences=True,\n",
    "#                                             activation=activation)\n",
    "\n",
    "\n",
    "#         self.conv_lstm_2 = layers.ConvLSTM2D(filters=filters[2],\n",
    "#                                             kernel_size=(kernel_sizes[2], kernel_sizes[2]),\n",
    "#                                             strides=(strides[2], strides[2])\n",
    "#                                             padding=padding,\n",
    "#                                             return_sequences=True,\n",
    "#                                             activation=activation)\n",
    "\n",
    "#         self.batch_norm_0 = layers.BatchNormalization()\n",
    "#         self.batch_norm_1 = layers.BatchNormalization()\n",
    "#         self.batch_norm_2 = layers.BatchNormalization()\n",
    "\n",
    "#         # reducing parameter space to 1 to match label\n",
    "#         self.dense_layer_out = layers.Dense(1)\n",
    "\n",
    "\n",
    "#         # mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "#     def call(self, inputs, training=False)\n",
    "#         # for now, threee convLSTM2D layers with batchnorm sandwiches\n",
    "#         x = self.conv_lstm_0(inputs)\n",
    "#         x = self.batch_norm_0(x, training=training)\n",
    "#         x = self.conv_lstm_1(x)\n",
    "#         x = self.batch_norm_1(x, training=training)\n",
    "#         x = self.conv_lstm_2(x)\n",
    "#         x = self.batch_norm_2(x, training=training)\n",
    "#         x = self.dense_layer_out(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "#         # Reshape the output tensor to match the target shape (num_samples, 1, rows, cols, 1)\n",
    "#         # x = layers.Reshape((1, *x.shape[2:]))(x)\n",
    "#         x= layers.TimeDistributed(layers.Dense(1))(x)\n",
    "\n",
    "#         # # Add a final Conv3D layer for the spatiotemporal outputs\n",
    "#         # x = layers.Conv3D(\n",
    "#         #     filters=8, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "#         # )(x)\n",
    "\n",
    "#         # Next, we will build the complete model and compile it.\n",
    "#         model = models.Model(inputs=inp,outputs=x)\n",
    "\n",
    "#         # TODO: include weighting of classes: https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "#         model.compile(\n",
    "#             loss=losses.binary_crossentropy,\n",
    "#             optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "#         )\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "#         \"\"\"Sets up a GRU model architecture with multiple layers and dense layers for mapping the outputs of the GRU \n",
    "#         layers to a desired output shape\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         rnn_units (list[int]): list containing the number of neurons to use in each layer\n",
    "#         num_layers (int): number of layers in GRU model\n",
    "#         \"\"\"\n",
    "#         super(gru_model, self).__init__()   # initialise GRU model as subclass of tf.keras.Model\n",
    "#         # store values for later use\n",
    "#         self.num_layers = num_layers    # number of layers in GRU model\n",
    "#         self.rnn_units = rnn_units\n",
    "#         # self.dff = dff\n",
    "#         # define model layers: creating new `tf.keras.layers.GRU` layer for each iteration\n",
    "#         self.grus = [tf.keras.layers.GRU(rnn_units[i],  # number (integer) of rnn units/neurons to use in each model layer\n",
    "#                                    return_sequences=True,   # return full sequence of outputs for each timestep\n",
    "#                                    return_state=True) for i in range(num_layers)] # return last hidden state of RNN at end of sequence\n",
    "        \n",
    "#         # dense layers are linear mappings of RNN layer outputs to desired output shape\n",
    "#         # self.w1 = tf.keras.layers.Dense(dff) # 10 units\n",
    "#         self.w1 = tf.keras.layers.Dense(10) # 10 units\n",
    "\n",
    "#         self.w2 = tf.keras.layers.Dense(1)  # 1 unit (dimension 1 required before final sigmoid function)\n",
    "#         # self.A = tf.keras.layers.Dense(30)\n",
    "#         # self.B = tf.keras.layers.Dense(dff)\n",
    "\n",
    "\n",
    "\n",
    "#     def call(self, inputs: np.ndarray, training: bool=False):\n",
    "#         \"\"\"Processes an input sequence of data through several layers of GRU cells, followed by a couple of\n",
    "#         fully-connected dense layers, and outputs the probability of an event happening.\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         inputs (np.ndarray): input tensor of shape (batch_size, seq_length, features)\n",
    "#             batch_size - defines the size of the sample drawn from datapoints\n",
    "#             seq_length - number of timesteps in sequence\n",
    "#             features - number of features associated with each datapoint\n",
    "#         training (bool, defaults to False): True if model is in training, False if in inference mode\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         target: probability of an event occuring, with shape (batch_size, 1)\n",
    "#         \"\"\"\n",
    "#         # input shape: (batch_size, seq_length, features)\n",
    "       \n",
    "#         assert self.num_layers == len(self.rnn_units)\n",
    "\n",
    "#         # check that input tensor has correct shape\n",
    "#         if (len(inputs.shape) != 3):\n",
    "#             print(f\"Incorrect shape of input tensor. Expected 3D array. Recieved {len(inputs.shape)}D array.\")\n",
    "\n",
    "#         # print('input dim ({}, {}, {})'.format(inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
    "#         # whole_seq, static_input = inputs\n",
    "#         whole_seq = inputs\n",
    "\n",
    "\n",
    "#         # iteratively passes input tensor to GRU layers, overwriting preceding sequence 'whole_seq'\n",
    "#         for layer_num in range(self.num_layers):\n",
    "#             whole_seq, final_s = self.grus[layer_num](whole_seq, training=training)\n",
    "\n",
    "#         # adding extra layers\n",
    "#         # static = self.B(tf.nn.gelu(self.A(static_input)))\n",
    "#         # target = self.w1(final_s)  + static # final hidden state of last layer used as input to fully connected dense layers...\n",
    "#         target = self.w1(final_s)   # final hidden state of last layer used as input to fully connected dense layers...\n",
    "\n",
    "#         target = tf.nn.relu(target) #Â via ReLU activation function\n",
    "#         target = self.w2(target)    # final hidden layer must have dimension 1 \n",
    "        \n",
    "#         # obtain a probability value between 0 and 1\n",
    "#         target = tf.nn.sigmoid(target)\n",
    "        \n",
    "#         return target\n",
    "\n",
    "\n",
    "# # initialise GRU model with 500 hidden layers, one GRU unit per layer \n",
    "# g_model = gru_model([100], 1) # N.B. [x] is number of hidden layers in GRU network\n",
    "\n",
    "\n",
    "# def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "#     \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "#     incorporating class weights.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "#     y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "#     class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "#     incorporating class weights if provided\n",
    "#     \"\"\"\n",
    "#     bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "#     if class_weights is not None:\n",
    "#         sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "#         # reshape to match size of y and y_pred\n",
    "#         return bce(y, y_pred, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "#     return bce(y, y_pred)\n",
    "\n",
    "\n",
    "# def training_batches(X: np.ndarray, y: np.ndarray, batch_num: int, batch_size: int=32):\n",
    "#     start_idx = batch_num * batch_size\n",
    "#     end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "#     X_batch = X[start_idx:end_idx]\n",
    "#     y_batch = y[start_idx:end_idx]\n",
    "    \n",
    "#     return X_batch, y_batch\n",
    "\n",
    "# # https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "# # should aim to delete the following to speed up training: but can't figure out a way to make wandb reporting work\n",
    "# # without it\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# def build_graph():\n",
    "    \n",
    "#     # compile function as graph using tf's autograph feature: leads to faster execution times, at expense of limitations\n",
    "#     # to Python objects/certain control flow structures (somewhat relaxed by experimental_relax_shapes)\n",
    "#     @tf.function(experimental_relax_shapes=True)\n",
    "#     def train_step(gru: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, X: tf.Tensor, y: tf.Tensor, \n",
    "#                training: bool = True, class_weights=class_weights) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "#         \"\"\"Train model using input `X` and target data `y` by computing gradients of the loss (via \n",
    "#         negative_log_likelihood)\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "#         y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred'\n",
    "#         \"\"\"\n",
    "#         if training:\n",
    "#             # num_samples = X.shape[0]\n",
    "#             # num_batches = num_samples // batch_size\n",
    "#             # num_batches = batch_num\n",
    "#             # total_epoch_loss = 0.0\n",
    "#             # for batch_num in tqdm(range(num_batches), desc=\"batches\", position=0, leave=True):\n",
    "#             # for batch_num, batch in tqdm(enumerate(ds), desc=\"batches\", position=0, leave=True):\n",
    "#             total_batch_loss = 0\n",
    "\n",
    "#             y_pred = gru(X, training)\n",
    "#             xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "\n",
    "#                 # X_batch, y_batch = training_batches(X, y, batch_num=batch_num, batch_size=batch_size)\n",
    "\n",
    "#             with tf.GradientTape(persistent=True) as tape:\n",
    "#                 y_pred = gru(X, training) \n",
    "#                 xent = negative_log_likelihood(y, y_pred, class_weights)\n",
    "#                 # y_pred = gru(X, training) # TO DELETE\n",
    "#                 # xent = negative_log_likelihood(y, y_pred)\n",
    "            \n",
    "#             gradients = tape.gradient(xent, gru.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(gradients, gru.trainable_variables))\n",
    "#             # print(\"xent\", xent.numpy())\n",
    "#             # print(\"total_epoch_loss\", total_epoch_loss)\n",
    "#             total_batch_loss += xent\n",
    "#                 # learning rate?\n",
    "#                 # wandb.log({\"batch\": batch_num, \"loss\": xent, \"total_epoch_loss\": total_epoch_loss})\n",
    "\n",
    "#             average_loss = total_batch_loss / num_batches\n",
    "#             # return predicted output values and total loss value\n",
    "#             return y_pred, xent, total_batch_loss\n",
    "\n",
    "#     # set default float type\n",
    "#     tf.keras.backend.set_floatx('float32')\n",
    "#     # TODO: this isn't assigned... What should it return otherwise?\n",
    "#     return train_step\n",
    "\n",
    "\n",
    "# with tf.device(\"/GPU:0\"):\n",
    "#     num_epochs = 2\n",
    "#     # will update so that subsamples are fed in from which batches are taken: will require recomputation\n",
    "#     # of class_weight for each subsample\n",
    "\n",
    "#     tr_step = build_graph()\n",
    "\n",
    "#     for epoch in tqdm(range(num_epochs), desc= \" epochs\", position=1, leave=True):\n",
    "#         total_epoch_loss = 0.0\n",
    "#         for X_batch, y_batch in tqdm(train_dataset, position=0, desc=\" training on batches\", leave=True):\n",
    "#             y_pred, xent, batch_loss = tr_step(\n",
    "#                 g_model, optimizer, X_batch, y_batch, class_weights=class_weights, training=True)\n",
    "\n",
    "#             total_epoch_loss += batch_loss\n",
    "\n",
    "#         average_loss = total_epoch_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "# # wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define Convolution LSTM Model\n",
    "# class ConvLSTModel(tf.keras.Model):\n",
    "#     # initialise class instance to define layers of the model\n",
    "#     def __init__(self, filters: list[int] = [32, 64, 128], kernel_sizes: list[int] = [3,3,3], \n",
    "#         strides: list[int] = [1,1,1], num_layers: int = 3, padding: str = \"same\",\n",
    "#         # input_shape: tuple[int] = INPUT_SHAPE,\n",
    "#         ):\n",
    "\n",
    "#         # initialise CoonvLSTModel model as subclass of tf.keras.Model\n",
    "#         super(ConvLSTModel, self).__init__()\n",
    "\n",
    "#         # check argument consistency\n",
    "#         assert(len(filters) == num_layers)\n",
    "#         assert(len(kernel_sizes) == len(filters))\n",
    "#         assert(len(strides) == len(filters))\n",
    "\n",
    "#         # TODO: put outside in wandb config\n",
    "#         activation = \"relu\"\n",
    "\n",
    "#         self.conv_lstm_0 = layers.ConvLSTM2D(filters=filters[0],\n",
    "#                                             kernel_size=(kernel_sizes[0], kernel_sizes[0]),\n",
    "#                                             strides=(strides[0], strides[0]),\n",
    "#                                             padding=padding,\n",
    "#                                             return_sequences=True,\n",
    "#                                             activation=activation)\n",
    "\n",
    "\n",
    "#         self.conv_lstm_1 = layers.ConvLSTM2D(filters=filters[1],\n",
    "#                                             kernel_size=(kernel_sizes[1], kernel_sizes[1]),\n",
    "#                                             strides=(strides[1], strides[1]),\n",
    "#                                             padding=padding,\n",
    "#                                             return_sequences=True,\n",
    "#                                             activation=activation)\n",
    "\n",
    "\n",
    "#         self.conv_lstm_2 = layers.ConvLSTM2D(filters=filters[2],\n",
    "#                                             kernel_size=(kernel_sizes[2], kernel_sizes[2]),\n",
    "#                                             strides=(strides[2], strides[2]),\n",
    "#                                             padding=padding,\n",
    "#                                             return_sequences=True,\n",
    "#                                             activation=activation)\n",
    "\n",
    "#         self.batch_norm_0 = layers.BatchNormalization()\n",
    "#         self.batch_norm_1 = layers.BatchNormalization()\n",
    "#         self.batch_norm_2 = layers.BatchNormalization()\n",
    "\n",
    "#         # reducing parameter space to 1 to match label\n",
    "#         self.dense_layer_out = layers.Dense(5)\n",
    "\n",
    "\n",
    "#     def call(self, inputs, training=False):\n",
    "#         # print(inputs.shape)\n",
    "#         # for now, threee convLSTM2D layers with batchnorm sandwiches\n",
    "#         x = self.conv_lstm_0(inputs)\n",
    "#         # print(\"conv0\", x.shape)\n",
    "#         x = self.batch_norm_0(x, training=training)\n",
    "#         x = self.conv_lstm_1(x)\n",
    "#         # print(\"conv1\", x.shape)\n",
    "#         x = self.batch_norm_1(x, training=training)\n",
    "#         x = self.conv_lstm_2(x)\n",
    "#         # print(\"conv2\", x.shape)\n",
    "#         x = self.batch_norm_2(x, training=training)\n",
    "#         x = self.dense_layer_out(x)\n",
    "#         # print(\"dense\", x.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def build_graph():\n",
    "#     @tf.function\n",
    "#     def train_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor, \n",
    "#         optimizer: tf.keras.optimizers.Optimizer = OPTIMIZER, class_weights: tuple = None):\n",
    "#         with tf.GradientTape() as lstm_tape:\n",
    "\n",
    "#             prediction = conv_lstm(features, training=True)\n",
    "#             # Calculate loss\n",
    "#             loss = negative_log_likelihood(labels, prediction, class_weights)\n",
    "#             # get trainable variables\n",
    "#             trainable_vars = conv_lstm.trainable_variables\n",
    "\n",
    "#         # Calculate gradient          `\n",
    "#         lstm_grads = lstm_tape.gradient(loss, trainable_vars)\n",
    "#         # And then apply the gradient to change the weights\n",
    "#         optimizer.apply_gradients(zip(lstm_grads, trainable_vars))\n",
    "\n",
    "#         return loss\n",
    "    \n",
    "#     return train_step\n",
    "\n",
    "\n",
    "# def negative_log_likelihood(y: np.ndarray, y_pred: np.ndarray, class_weights: np.ndarray = None) -> float:\n",
    "#     \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "#     incorporating class weights.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "#     y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "#     class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "#     incorporating class weights if provided\n",
    "#     \"\"\"\n",
    "#     bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "#     if class_weights is not None:\n",
    "#         sample_weights = tf.gather(class_weights, np.asarray(y,dtype=np.int32))\n",
    "#         # reshape to match size of y and y_pred\n",
    "#         return bce(y, y_pred, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "#     return bce(y, y_pred)\n",
    "\n",
    "\n",
    "# conv_lstm = ConvLSTModel()\n",
    "\n",
    "# tr_step = build_graph()\n",
    "\n",
    "\n",
    "# # strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# # with strategy.scope():\n",
    "\n",
    "# for epoch in tqdm(range(EPOCHS), desc=\"epochs\", position=0):\n",
    "#     # Just total loss\n",
    "#     loss_tot = 0\n",
    "#     # Iterate through all the examples\n",
    "#     for features, labels in tqdm(train_dataset, desc=\" batches\", position=1):\n",
    "#         # Break if the number of computed batches exceeds the\n",
    "#         # total number of the examples\n",
    "\n",
    "#         # HERE WE PERFORM ONE TRAINING STEP\n",
    "#         loss = tr_step(conv_lstm, features, labels)\n",
    "#         loss_tot += loss\n",
    "\n",
    "#     # Print the total loss every epoch\n",
    "#     # print(f'\\nLoss for epoch {epoch+1} is {loss_tot}')\n",
    "\n",
    "\n",
    "# # # Create an instance of the model\n",
    "# # model = ConvLSTModel()\n",
    "\n",
    "# # # Print the model summary\n",
    "# # model.build(shape=(None, 10, 40, 60, 5))  # Provide an example input shape\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 1, 40, 60, 1), dtype=float64, numpy=\n",
       "array([[[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]]])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_weights\n",
    "tf.gather(class_weights, np.asarray(tf.concat(labels.values, axis=0), dtype=np.int32))\n",
    "# tf.concat(labels.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [1],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [1],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [1],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [1],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [1],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [1],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "\n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]]]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(tf.concat(labels.values, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 1, 40, 60, 1), dtype=float64, numpy=\n",
       "array([[[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.28634361],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]],\n",
       "\n",
       "         [[0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          ...,\n",
       "          [0.55223194],\n",
       "          [0.55223194],\n",
       "          [0.55223194]]]]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(class_weights, np.asarray(tf.concat(labels.values, axis=0), dtype=np.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kp5nrswy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-wave-20</strong> at: <a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/kp5nrswy' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm/runs/kp5nrswy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230617_120420-kp5nrswy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kp5nrswy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49473cc90704edb94e87231191cd822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666856117080897, max=1.0)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lustre_scratch/orlando-code/coralshift/wandb/run-20230617_120523-8qcj18ic</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/8qcj18ic' target=\"_blank\">easy-deluge-21</a></strong> to <a href='https://wandb.ai/orlando-code/new_conv_lstm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/orlando-code/new_conv_lstm' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/8qcj18ic' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm/runs/8qcj18ic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 12:05:47.618610: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 10\n",
      "        }\n",
      "        dim {\n",
      "          size: 40\n",
      "        }\n",
      "        dim {\n",
      "          size: 60\n",
      "        }\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "        dim {\n",
      "          size: 40\n",
      "        }\n",
      "        dim {\n",
      "          size: 60\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"Shape_1:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"replica_1/Shape:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"replica_1/Shape_1:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"Shape:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"Shape_1:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"replica_1/Shape:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"replica_1/Shape_1:0\", shape=(5,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 12:05:58.295843: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/StatefulPartitionedCall/gradient_tape/conv_lst_model_20/conv_lstm2d_62/while/conv_lst_model_20/conv_lstm2d_62/while_grad/body/_684/input/_2783' -> 'StatefulPartitionedCall/gradient_tape/conv_lst_model_20/conv_lstm2d_62/while/conv_lst_model_20/conv_lstm2d_62/while_grad/body/_684/gradient_tape/conv_lst_model_20/conv_lstm2d_62/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_62/while/replica_1/conv_lst_model_20/conv_lstm2d_62/while_grad/body/_1808/input/_3491' -> 'StatefulPartitionedCall/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_62/while/replica_1/conv_lst_model_20/conv_lstm2d_62/while_grad/body/_1808/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_62/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/conv_lst_model_20/conv_lstm2d_61/while/conv_lst_model_20/conv_lstm2d_61/while_grad/body/_879/input/_2902' -> 'StatefulPartitionedCall/gradient_tape/conv_lst_model_20/conv_lstm2d_61/while/conv_lst_model_20/conv_lstm2d_61/while_grad/body/_879/gradient_tape/conv_lst_model_20/conv_lstm2d_61/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/conv_lst_model_20/conv_lstm2d_60/while/conv_lst_model_20/conv_lstm2d_60/while_grad/body/_1074/input/_3018' -> 'StatefulPartitionedCall/gradient_tape/conv_lst_model_20/conv_lstm2d_60/while/conv_lst_model_20/conv_lstm2d_60/while_grad/body/_1074/gradient_tape/conv_lst_model_20/conv_lstm2d_60/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_61/while/replica_1/conv_lst_model_20/conv_lstm2d_61/while_grad/body/_2003/input/_3610' -> 'StatefulPartitionedCall/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_61/while/replica_1/conv_lst_model_20/conv_lstm2d_61/while_grad/body/_2003/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_61/while/gradients/AddN', 'Func/StatefulPartitionedCall/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_60/while/replica_1/conv_lst_model_20/conv_lstm2d_60/while_grad/body/_2198/input/_3726' -> 'StatefulPartitionedCall/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_60/while/replica_1/conv_lst_model_20/conv_lstm2d_60/while_grad/body/_2198/gradient_tape/replica_1/conv_lst_model_20/conv_lstm2d_60/while/gradients/AddN'}.\n",
      " train batches: 4it [00:13,  3.47s/it]\n",
      " test batches: 4it [00:02,  1.63it/s]\n",
      " train batches: 4it [00:00,  6.26it/s]:49, 16.35s/it]\n",
      " test batches: 4it [00:00, 14.52it/s]\n",
      " train batches: 4it [00:00,  6.39it/s]:14,  7.28s/it]\n",
      " test batches: 4it [00:00, 14.52it/s]\n",
      " train batches: 4it [00:00,  6.30it/s]:04,  4.37s/it]\n",
      " test batches: 4it [00:00, 13.88it/s]\n",
      "epochs: 100%|ââââââââââ| 4/4 [00:19<00:00,  4.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch loss</td><td>ââââââââââââââââ</td></tr><tr><td>test accuracy</td><td>ââââââââââââââââ</td></tr><tr><td>train accuracy</td><td>ââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch loss</td><td>124629.51562</td></tr><tr><td>test accuracy</td><td>1.39123</td></tr><tr><td>train accuracy</td><td>2.6163</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-deluge-21</strong> at: <a href='https://wandb.ai/orlando-code/new_conv_lstm/runs/8qcj18ic' target=\"_blank\">https://wandb.ai/orlando-code/new_conv_lstm/runs/8qcj18ic</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230617_120523-8qcj18ic/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################\n",
    "# DEFINE CONVOLUTIONAL LSTM MODEL\n",
    "####################################\n",
    "\n",
    "\n",
    "class ConvLSTModel(tf.keras.Model):\n",
    "    \"\"\"Convolutional LSTM model for binary classification of spatiotemporal data\"\"\"\n",
    "\n",
    "    # initialise class instance to define layers of the model\n",
    "    def __init__(self, filters: list[int] = [32, 64, 128], kernel_sizes: list[int] = [3,3,3], \n",
    "        strides: list[int] = [1,1,1], num_layers: int = 3, padding: str = \"same\", activation: str = \"relu\"\n",
    "        # input_shape: tuple[int] = INPUT_SHAPE,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the ConvLSTM model (ConvLSTModel)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            filters (list[int]): List of filters for each ConvLSTM2D layer.\n",
    "            kernel_sizes (list[int]): List of kernel sizes for each ConvLSTM2D layer.\n",
    "            strides (list[int]): List of strides for each ConvLSTM2D layer.\n",
    "            num_layers (int): Number of ConvLSTM2D layers.\n",
    "            padding (str): Padding mode for ConvLSTM2D layers.\n",
    "            activation (str): Activation function to use.\n",
    "        \"\"\"\n",
    "\n",
    "        # initialise CoonvLSTModel model as subclass of tf.keras.Model\n",
    "        super(ConvLSTModel, self).__init__()\n",
    "\n",
    "        # check argument consistency\n",
    "        assert(len(filters) == num_layers)\n",
    "        assert(len(kernel_sizes) == len(filters))\n",
    "        assert(len(strides) == len(filters))\n",
    "\n",
    "        self.conv_lstm_0 = layers.ConvLSTM2D(filters=filters[0],\n",
    "                                            kernel_size=(kernel_sizes[0], kernel_sizes[0]),\n",
    "                                            strides=(strides[0], strides[0]),\n",
    "                                            padding=padding,\n",
    "                                            return_sequences=True,\n",
    "                                            activation=activation)\n",
    "\n",
    "\n",
    "        self.conv_lstm_1 = layers.ConvLSTM2D(filters=filters[1],\n",
    "                                            kernel_size=(kernel_sizes[1], kernel_sizes[1]),\n",
    "                                            strides=(strides[1], strides[1]),\n",
    "                                            padding=padding,\n",
    "                                            return_sequences=True,\n",
    "                                            activation=activation)\n",
    "\n",
    "\n",
    "        self.conv_lstm_2 = layers.ConvLSTM2D(filters=filters[2],\n",
    "                                            kernel_size=(kernel_sizes[2], kernel_sizes[2]),\n",
    "                                            strides=(strides[2], strides[2]),\n",
    "                                            padding=padding,\n",
    "                                            return_sequences=True,\n",
    "                                            activation=activation)\n",
    "\n",
    "        self.batch_norm_0 = layers.BatchNormalization()\n",
    "        self.batch_norm_1 = layers.BatchNormalization()\n",
    "        self.batch_norm_2 = layers.BatchNormalization()\n",
    "\n",
    "        # reducing parameter space to 1 to match label feature dimension\n",
    "        self.dense_layer_out = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # for now, threee convLSTM2D layers with batchnorm sandwiches\n",
    "        x = self.conv_lstm_0(inputs)\n",
    "        x = self.batch_norm_0(x, training=training)\n",
    "        x = self.conv_lstm_1(x)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = self.conv_lstm_2(x)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        x = self.dense_layer_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "###########################\n",
    "# INITIALISE PARAMETERS\n",
    "###########################\n",
    "\n",
    "num_features = 5\n",
    "seq_length = 100\n",
    "\n",
    "# configure model\n",
    "LEARNING_RATE = 1e-3    # TODO: adjustable (not particularly preessing though)\n",
    "INPUT_SHAPE = (None, seq_length, None, None, num_features)\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "EPOCHS = 4\n",
    "ACTIVATION = \"relu\"\n",
    "BATCH_SIZE_PER_REPLICA = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"new_conv_lstm\",\n",
    "    entity=\"orlando-code\",\n",
    "    settings=wandb.Settings(start_method=\"fork\")\n",
    "    )\n",
    "\n",
    "wandb.config = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"activation\": ACTIVATION,\n",
    "    \"batch_size_per_replica\": BATCH_SIZE_PER_REPLICA\n",
    "}\n",
    "\n",
    "\n",
    "# set up distributed processing\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "GLOBAL_BATCH_SIZE = (BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync)\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    conv_lstm = ConvLSTModel(activation=wandb.config[\"activation\"])\n",
    "    DIST_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    def compute_loss(\n",
    "        labels: tf.Tensor, predictions: tf.Tensor, class_weights: tuple[float] = None, distributed: bool = True):\n",
    "        \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "        incorporating class weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            labels (tf.Tensor): True binary labels, where 0 represents the negative class.\n",
    "            predictions (tf.Tensor): Predicted labels as probability values between 0 and 1.\n",
    "            class_weights (np.ndarray): Weights for each class. If None, no class weights will be applied.\n",
    "            distributed (bool): Flag indicating if the computation is distributed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float: Negative log likelihood loss computed using binary cross-entropy loss between 'labels' and \n",
    "                'predictions', incorporating class weights if provided.\n",
    "        \"\"\"\n",
    "        per_example_loss = negative_log_likelihood(labels, predictions, class_weights, distributed)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    train_accuracy = tf.keras.metrics.BinaryCrossentropy(name=\"train_accuracy\")\n",
    "    test_accuracy = tf.keras.metrics.BinaryCrossentropy(name=\"test_accuracy\")\n",
    "    # would use BinaryAccuracy if predicting ones/zeros\n",
    "\n",
    "# cast datasets to mirrored versions for distributed processing\n",
    "dist_train_dataset = mirrored_strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "\n",
    "#################################\n",
    "# DEFINE TRAIN/TEST FUNCTIONS\n",
    "#################################\n",
    "\n",
    "\n",
    "def negative_log_likelihood(\n",
    "    labels: np.ndarray, predictions: np.ndarray, class_weights: np.ndarray = None, distributed: bool=False) -> float:\n",
    "    \"\"\"Compute binary cross-entropy loss between ground-truth binary labels and predicted probabilities,\n",
    "    incorporating class weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y (np.ndarray): true binary labels, where 0 represents the negative class\n",
    "    y_pred (np.ndarray): predicted labels (as probability value between 0 and 1)\n",
    "    class_weights (np.ndarray): weights for each class. If None, no class weights will be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: negative log likelihood loss computed using binary cross-entropy loss between 'y' and 'y_pred',\n",
    "    incorporating class weights if provided\n",
    "    \"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()  \n",
    "\n",
    "    if distributed:\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)  \n",
    "\n",
    "    print(tf.shape(labels))\n",
    "    print(tf.shape(predictions))\n",
    "\n",
    "    # TODO: this may be broken, throwing error associated with numpy/tensor incompatability\n",
    "    if class_weights is not None:\n",
    "        # sample_weights = tf.gather(class_weights, np.asarray(tf.concat(labels.values, axis=0), dtype=np.int32))\n",
    "        sample_weights = tf.gather(class_weights, tf.cast(labels, dtype=tf.int32))\n",
    "\n",
    "        # reshape to match size of y and y_pred\n",
    "        return bce(labels, predictions, sample_weight=tf.reshape(sample_weights, (-1, 1)))\n",
    "\n",
    "    return bce(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor, \n",
    "    optimizer: tf.keras.optimizers.Optimizer = OPTIMIZER, learn_rate: float = LEARNING_RATE, \n",
    "    class_weights: tuple = None, distributed: bool=False):\n",
    "    \n",
    "    with tf.GradientTape() as lstm_tape:\n",
    "        predictions = conv_lstm(features, training=True)\n",
    "        # Calculate loss\n",
    "        loss = compute_loss(labels, predictions, class_weights, distributed)\n",
    "    \n",
    "    # get trainable variables\n",
    "    trainable_vars = conv_lstm.trainable_variables\n",
    "    # Calculate gradient          `\n",
    "    lstm_grads = lstm_tape.gradient(loss, trainable_vars)\n",
    "    # And then apply the gradient to change the weights\n",
    "    optimizer.apply_gradients(zip(lstm_grads, trainable_vars))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor):\n",
    "\n",
    "    predictions = conv_lstm(features, training=False)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(model: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor, \n",
    "    optimizer: tf.keras.optimizers.Optimizer = DIST_OPTIMIZER, learn_rate: float = LEARNING_RATE,\n",
    "    class_weights: tuple = None, distributed: bool = True):\n",
    "    \"\"\"Performs a distributed training step on the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (tf.keras.Model): The model to train.\n",
    "        features (tf.Tensor): Input features for training.\n",
    "        labels (tf.Tensor): True labels for training.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): Optimizer to use for training. Default is DIST_OPTIMIZER.\n",
    "        learn_rate (float): Learning rate for the optimizer. Default is LEARNING_RATE.\n",
    "        class_weights (tuple): Weights for each class. If None, no class weights will be applied. Default is None.\n",
    "        distributed (bool): Flag indicating if the computation is distributed. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tf.Tensor: Loss computed during the training step.\n",
    "    \"\"\"\n",
    "    per_replica_losses = mirrored_strategy.run(train_step, args=(\n",
    "        model, features, labels, optimizer, learn_rate, class_weights, distributed,))\n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(conv_lstm: tf.keras.Model, features: tf.Tensor, labels: tf.Tensor):\n",
    "    \"\"\"Performs a distributed test step on the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        conv_lstm (tf.keras.Model): The model to evaluate.\n",
    "        features (tf.Tensor): Input features for evaluation.\n",
    "        labels (tf.Tensor): True labels for evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    return mirrored_strategy.run(test_step, args=(conv_lstm, features, labels,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# RUN MODEL\n",
    "################\n",
    "\n",
    "for epoch in tqdm(range(wandb.config[\"epochs\"]), desc=\"epochs\", position=0):\n",
    "    # Just total loss\n",
    "    epoch_loss = 0\n",
    "    # Iterate through the train dataset\n",
    "    for features, labels in tqdm(dist_train_dataset, desc=\" train batches\", position=1):\n",
    "        loss = distributed_train_step(conv_lstm, features, labels, DIST_OPTIMIZER, wandb.config[\"learning_rate\"],\n",
    "            # class_weights\n",
    "            )\n",
    "        epoch_loss += loss\n",
    "        wandb.log({\"epoch loss\": epoch_loss, \"train accuracy\": train_accuracy.result()})\n",
    "\n",
    "    # Iterate through the test dataset\n",
    "    for features, labels in tqdm(dist_train_dataset, desc=\" test batches\", position=1):\n",
    "        distributed_test_step(conv_lstm, features, labels)\n",
    "        wandb.log({\"test accuracy\": test_accuracy.result()})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = layers.Input(shape=(features.shape[1:]))\n",
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*10*60*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(features.shape[1:]))\n",
    "\n",
    "convlstm = ConvLSTModel()\n",
    "\n",
    "convlstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                       tf.keras.metrics.FalseNegatives()])\n",
    "# convlstm.build(INPUT_SHAPE)\n",
    "# convlstm.build(features.shape[:])\n",
    "\n",
    "convlstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(3,))\n",
    "outputs = tf.keras.layers.Dense(2)(inputs)\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.metrics_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and weights\n",
    "\n",
    "[Source](https://towardsdatascience.com/creating-a-trashy-model-from-scratch-with-tensorflow-2-an-example-with-an-anomaly-detection-27f0d1d7bd00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.save_weights(filepath='encoder_model.tf')\n",
    "\n",
    "# # Saving the configuration\n",
    "# import json\n",
    "# ## Encoder\n",
    "# encoder_dict = {'fillters': [32, 64, 128, 196, 256, 1024],\n",
    "#                 'kernel_sizes': [3, 3, 3, 3, 3, 3],\n",
    "#                 'strides': [2, 2, 2, 2, 2, 2],\n",
    "#                 'num_of_layers': 6}\n",
    "# with open('encoder_config.json', 'w') as fp:\n",
    "#     json.dump(encoder_dict, fp)\n",
    "\n",
    "# ## Decoder\n",
    "# decoder_dict = {'fillters': [1024, 256, 196, 128, 64, 32, 1], \n",
    "#                 'kernel_sizes': [3, 3, 3, 3, 3, 3, 1],\n",
    "#                 'strides': [2, 2, 2, 2, 2, 2, 1],\n",
    "#                 'num_of_layers': 7}\n",
    "# with open('decoder_config.json', 'w') as fp:\n",
    "#     json.dump(decoder_dict, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CORALSHIFT",
   "language": "python",
   "name": "coralshift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
